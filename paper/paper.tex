% vim: set spell spelllang=en tw=100 et sw=4 sts=4 :

\documentclass[runningheads]{llncs}

\usepackage{complexity}
\usepackage{amsmath}
% \usepackage[ruled,vlined]{algorithm2e}
\usepackage{tikz}
\usepackage{cleveref}

% \usepackage{showframe}

\newcommand{\neighbourhood}{\operatorname{N}}
\newcommand{\vertexset}{\operatorname{V}}

\newcommand{\siplabel}[1]{\label{line:sip:#1}}
\newcommand{\siplineref}[1]{line~\ref{line:sip:#1}}
\newcommand{\siplinerangeref}[2]{lines~\ref{line:sip:#1} to~\ref{line:sip:#2}}

\newcommand{\citet}[2]{#1\cite{#2}}

% cref style
\crefname{figure}{Figure}{Figures}
\Crefname{figure}{Figure}{Figures}
\crefname{section}{Section}{Sections}
\Crefname{section}{Section}{Sections}
\crefname{proposition}{Proposition}{Propositions}
\Crefname{proposition}{Proposition}{Propositions}
\crefname{corollary}{Corollary}{Corollaries}
\Crefname{corollary}{Corollary}{Corollaries}

% \crefname{algocf}{Algorithm}{Algorithms}
% \Crefname{algocf}{Algorithm}{Algorithms}

\usetikzlibrary{decorations, decorations.pathreplacing, decorations.pathmorphing,
    calc, backgrounds, positioning, tikzmark, patterns, fit}

\definecolor{highlight}{rgb}{0.529412, 0.74902, 0.466667}

\begin{document}

\title{Beating Simple Backtracking Search in Subgraph Algorithms\thanks{This work was
supported by the Engineering and Physical Sciences Research Council (grant numbers EP/026842/1 and
EP/M508056/1). This work used the Cirrus UK National Tier-2 HPC Service at EPCC
(http://www.cirrus.ac.uk) funded by the University of Edinburgh and EPSRC (EP/P020267/1).}}

\author{Ciaran McCreesh\orcidID{0000-0002-6106-4871}\and
Patrick Prosser\orcidID{0000-0003-4460-6912} \and
James Trimble\orcidID{0000-0001-7282-8745}}

\authorrunning{C. McCreesh et al.}

\institute{University of Glasgow, Glasgow, Scotland\\
\email{ciaran.mccreesh@glasgow.ac.uk}}

\maketitle

\begin{abstract}
    The current state of the art in subgraph isomorphism solving involves using degree as a
    value-ordering heuristics to direct backtracking search. Such a search makes a heavy commitment
    to the first branching choice, which is often incorrect. We evaluate discrepancy-based searches
    as an alternative, but find them too expensive to be effective in this situation, even on
    satisfiable instances. We then demonstrate that a random value-ordering heuristic with restarts
    beats conventional search with the best-known tailored heuristic. This motivates the
    introduction of a novel slightly-random heuristic with similar behaviour to the tailored
    heuristic: when combined with restarts and nogood recording, this new search strategy is over ??
    times more effective on satisfiable instances, and does not affect performance on
    unsatisfiable instances. Such a search can also be parallelised in a way which is much
    simpler than conventional work-stealing: we obtain further speedups of ??  on ??  cores, and
    ?? over ?? distributed-memory hosts. Finally, we show that this search strategy also improves
    two maximum common induced subgraph algorithms.
\end{abstract}

\section{Introduction}

The subgraph isomorphism problem is to decide whether a copy of a small ``pattern'' graph occurs
inside a larger ``target'' graph. Although the problem is \NP-complete, by combining design techniques from
artificial intelligence with careful algorithm engineering, modern subgraph isomorphism solvers can
often produce exact solutions quickly even on graphs with thousands of vertices. The current single
strongest subgraph isomorphism solver uses ``highest degree first'' as a value-ordering heuristic to
direct a constraint programming style search
\cite{DBLP:conf/cp/McCreeshP15,DBLP:conf/lion/KotthoffMS16,DBLP:journals/jair/McCreeshPST18}. This
heuristic is much better than branching randomly, but is still far from perfect. We therefore
investigate different techniques for offsetting mistakes in value-ordering decisions. We first try
discrepancy searches, which turn out to be much too expensive to be a practical solution. We then
look at using random value-ordering, restarts, and nogood recording, which together give better
aggregate performance at the expense of much worse behaviour on many instances. Next, we introduce a
new way of adapting the existing value-ordering heuristic to introduce just a small amount of
randomness.  This lets us make a state-of-the-art subgraph algorithm perform much better
on a large number of satisfiable instances, whilst performing worse only rarely on satisfiable
instances, and never on unsatisfiable instances.

?? Parallel

Finally, we show that this strategy is also effective in an optimisation setting, producing benefits
in two maximum common induced subgraph algorithms. This combination of techniques gives us, for the
first time, a practical alternative to backtracking search for constraint programming style
algorithms where we have a strong value-ordering heuristic and where we care both about satisfiable
and unsatisfiable instances. ?? Parallel

\subsection{Definitions}

% Let $G$ be a graph. We denote its set of vertices by $\vertexset(G)$, and write $v \sim_G w$ to mean
% that vertices $v$ and $w$ are adjacent; we allow a vertex to be adjacent to itself (a \emph{loop}). The
% \emph{neighbourhood} of a vertex, $\neighbourhood_G(v)$, is the set of vertices adjacent to $v$, and
% the \emph{degree} of $v$, $\deg_G(v)$, is the cardinality of its neighbourhood. When the graph is clear from the context, we omit
% the $G$ subscripts. As per \citet{Hoffmann et al.\ }{DBLP:conf/aaai/HoffmannMR17}, $G^{n,\ell}$ denotes the graph with
% vertex set $\vertexset(G)$, and with edges between vertices $v$ and $w$ if there are at least $n$
% simple paths of length $\ell$ between vertices $v$ and $w$ in $G$.

The \emph{non-induced subgraph isomorphism problem} is to find an injective mapping from the
vertices of a \emph{pattern} graph $\mathcal{P}$ to a \emph{target} graph $\mathcal{T}$, such that
adjacent vertices in $\mathcal{P}$ are mapped to adjacent vertices in $\mathcal{T}$ (including that
vertices with loops in $\mathcal{P}$ may only be mapped to vertices with loops in $\mathcal{T}$).
The \emph{induced} problem additionally requires that non-adjacent vertices are mapped to
non-adjacent vertices.

?? Applications

% We write $v \mapsto v'$ to mean that pattern vertex $v$ is mapped to target $v'$ under
% either kind of mapping. Finally, we define a function $\FuncSty{compatible}(v, w)$ to mean that
% mapping $v \mapsto w$ is feasible based upon both the loop rule, and the neighbourhood degree
% sequence relation of \citet{Zampelli et al.\ }{DBLP:journals/constraints/ZampelliDS10}.

\subsection{The Glasgow Subgraph Solver}

This paper looks at improving the Glasgow Subgraph
Solver\footnote{\url{https://github.com/ciaranm/glasgow-subgraph-solver/}}, which can solve both the
non-induced and the induced subgraph isomorphism problems.  The solver is very closely based upon
the $k{\downarrow}$ algorithm of \citet{Hoffmann et al.\ }{DBLP:conf/aaai/HoffmannMR17} with $k =
0$, and we refer the reader to that paper for full technical details; that algorithm, in turn, is a
simplification and re-engineering of an older Glasgow algorithm
\cite{DBLP:conf/cp/McCreeshP15,DBLP:conf/lion/KotthoffMS16}.  Essentially, the solver is essentially
a dedicated forward-checking constraint programming implementation specifically for subgraph
problems. It works with a model having a variable per pattern graph vertex, with domains ranging
over the target graph vertices, and performs a backtracking search to map each pattern vertex to a
target vertex whilst propagating adjacency and injectivity constraints (together with further
implied constraints based upon degrees and paths).  However, it uses specialised bit-parallel data
structures and algorithms, and propagates constraints in a fixed order rather than using a queue.

% We now briefly outline the non-shaded parts of \cref{algorithm:sip}, which is how we will solve the
% subgraph isomorphism problem. This algorithm is very closely based upon the $k{\downarrow}$
% algorithm of \citet{Hoffmann et al.\ }{DBLP:conf/aaai/HoffmannMR17} with $k = 0$, and we refer the reader to that paper
% for full technical details; that algorithm, in turn, is a simplification of the Glasgow2 algorithm
% \cite{DBLP:conf/cp/McCreeshP15,DBLP:conf/lion/KotthoffMS16}. We show the non-induced problem; the
% induced problem may be solved by including the loop-complements of $\mathcal{P}$ and $\mathcal{T}$
% in $L$ on \siplineref{supplemental}.
% 
% \Cref{algorithm:sip} works by recursively building up a valid mapping of pattern vertices to target
% vertices. For each pattern vertex $v$, we have a domain $D_v$ containing a set of feasible target
% vertices. Each domain is initialised at the top of search (\siplinerangeref{initstart}{initend}),
% using the invariants described by \citet{Zampelli et al.\ }{DBLP:journals/constraints/ZampelliDS10} and
% \citet{Hoffmann et al.\ }{DBLP:conf/aaai/HoffmannMR17} to restrict its initial values.
% 
% The $\FuncSty{search}$ procedure recursively selects an unassigned domain $D$
% (\siplineref{variableordering}), and tries assigning
% it each of its remaining values in turn (\siplineref{valueordering}). The effects of this assignment
% are propagated (\siplineref{dopropagate}). If an
% inconsistency is detected, we continue and try another value; otherwise, we recurse
% (\siplineref{recurse}) until a solution is found. Finally, if we run out of values, we backtrack
% (\siplineref{backtrack}).
% 
% Our $\FuncSty{propagate}$ routine is also close to that of \citet{Hoffmann et al.\ }{DBLP:conf/aaai/HoffmannMR17},
% differing only in that we carry out the filtering for injectivity
% (\siplinerangeref{alldiffstart}{alldiffend}) after every assignment rather than once at the end of
% the unit propagation loop. (We make this change because the all-different propagator used is not
% idempotent, and we wish to avoid an odd interaction when we introduce nogood filtering.)

% \begin{algorithm}[p]\DontPrintSemicolon\small\SetInd{0.1em}{1em}
%     \begin{tikzpicture}[remember picture,overlay]
%         \coordinate (restart1sc) at ($(pic cs:restart1s) + (0, 0.15)$);
%         \coordinate (restart1ec) at ($(pic cs:restart1e) + (0, 0.00)$);
%         \node [fill=highlight, rounded corners, fit=(restart1sc) (restart1ec)] { };
%         \coordinate (restart2sc) at ($(pic cs:restart2s) + (0, 0.15)$);
%         \coordinate (restart2ec) at ($(pic cs:restart2e) + (0, 0.00)$);
%         \node [fill=highlight, rounded corners, fit=(restart2sc) (restart2ec)] { };
%         \coordinate (restart9sc) at ($(pic cs:restart9s) + (0, 0.15)$);
%         \coordinate (restart9ec) at ($(pic cs:restart9e) + (-0.15, 0.00)$);
%         \node [fill=highlight, rounded corners, fit=(restart9sc) (restart9ec)] { };
%     \end{tikzpicture}
% 
%     \nl $\FuncSty{subgraphIsomorphism}$ (Graph $\mathcal{P}$, Graph $\mathcal{T}$) $\rightarrow$ Bool \;
% \nl \Begin{
%     \nl \lIf{$\left|\vertexset(\mathcal{P})\right| >
%     \left|\vertexset(\mathcal{T})\right|$}{$\KwSty{return}~\KwSty{false}$\siplabel{enough}}
%     \nl Discard isolated vertices in $\mathcal{P}$\siplabel{isolated} \;
%     \nl \KwSty{global} $L \gets \big[ (\mathcal{P},\ \mathcal{T}), \siplabel{supplemental}
%         (\mathcal{P}^{1,2}, \mathcal{T}^{1,2}),
%         (\mathcal{P}^{2,2},\ \mathcal{T}^{2,2}),$ \\
%         $\hspace*{3em}(\mathcal{P}^{3,2},\ \mathcal{T}^{3,2}),
%         (\mathcal{P}^{4,2},\ \mathcal{T}^{4,2}) \big]$ \;
%     \nl \ForEach{\siplabel{initstart}$v \in \vertexset(\mathcal{P})$}{
%         \nl $D_v \gets \vertexset(\mathcal{T})$\;
%         \nl \ForEach{$(P,\,T) \in L$}{\nl$D_v \gets \{ w \in D_v : \FuncSty{compatible}(v, w)\}$\siplabel{initend}
%         }
%     }
%     \nl \While{$\KwSty{true}$\siplabel{restartsloop}}{
%         \nl \lIf{$\KwSty{not}~\FuncSty{propagate}(D)$}{$\KwSty{return}~\KwSty{false}$}
%         \nl \tikzmark{restart1s}$\KwSty{global}~(R, R') \gets (0, \textnormal{the next Luby sequence value})$\tikzmark{restart1e} \;
%         \nl $S \gets \FuncSty{search}(\{ E \in D : \left|E\right| > 1 \}, \top)$ \;
%         \nl \tikzmark{restart9s}\lIf{$S \ne \KwSty{restart}$}{\tikzmark{restart9e}$\KwSty{return}~S$}
%         \nl \tikzmark{restart2s}trigger nogoods containing only one clause\tikzmark{restart2e} \;
%     }
% }
% \caption{The main subgraph isomorphism algorithm. The shaded code is added for restarting. See
%     \cref{algorithm:sip:search,algorithm:sip:propagate} for the search and propagation routines.}
% \label{algorithm:sip}
% \end{algorithm}
% 
% \begin{algorithm}[p]\DontPrintSemicolon\small\SetInd{0.1em}{1em}
%     \begin{tikzpicture}[remember picture,overlay]
%         \coordinate (restart3sc) at ($(pic cs:restart3s) + (0, 0.15)$);
%         \coordinate (restart3ec) at ($(pic cs:restart3e) + (0, 0.00)$);
%         \node [fill=highlight, rounded corners, fit=(restart3sc) (restart3ec)] { };
%         \coordinate (restart4sc) at ($(pic cs:restart4s) + (0, 0.15)$);
%         \coordinate (restart4ec) at ($(pic cs:restart4e) + (0, 0.00)$);
%         \node [fill=highlight, rounded corners, fit=(restart4sc) (restart4ec)] { };
%         \coordinate (restart5sc) at ($(pic cs:restart5s) + (0, 0.15)$);
%         \coordinate (restart5ec) at ($(pic cs:restart5e) - (0, 0.10)$);
%         \coordinate (restart5rc) at ($(pic cs:restart5r) + (0, 0.00)$);
%         \node [fill=highlight, rounded corners, fit=(restart5sc) (restart5ec) (restart5rc)] { };
%         \coordinate (restart6sc) at ($(pic cs:restart6s) + (0, 0.15)$);
%         \coordinate (restart6ec) at ($(pic cs:restart6e) + (0, 0.00)$);
%         \coordinate (restart6rc) at ($(pic cs:restart6r) + (0.8, 0.00)$);
%         \node [fill=highlight, rounded corners, fit=(restart6sc) (restart6ec) (restart6rc)] { };
%         \coordinate (restart8sc) at ($(pic cs:restart8s) + (0.05, 0.15)$);
%         \coordinate (restart8ec) at ($(pic cs:restart8e) + (0, 0.00)$);
%         \node [fill=highlight, rounded corners, fit=(restart8sc) (restart8ec)] { };
%     \end{tikzpicture}
%     \nl $\FuncSty{search}$ (Domains $D$, Trail $B$) $\rightarrow$ Bool
%     \tikzmark{restart8s}\KwSty{or} \KwSty{restart}\tikzmark{restart8e} \;
% \nl \Begin{
%     \nl \lIf{$D = \emptyset$}{$\KwSty{return}~\KwSty{true}$}
%     \nl $D_v \gets \textnormal{a domain from}~D~\textnormal{chosen by variable heuristic}$\siplabel{variableordering} \;
%     \nl \tikzmark{restart3s}$C \gets \emptyset$\tikzmark{restart3e} \;
%     \nl \ForEach {$v' \in D_v~\textnormal{ordered by value heuristic}$\siplabel{valueordering}}{
%         \nl \tikzmark{restart4s}$C \gets C \cup \{ v' \}$\tikzmark{restart4e} \;
%         \nl $D' \gets \FuncSty{clone}(D)$ \;
%         \nl $D'_v \gets \{ v' \} $ \;
%         \nl \If{\FuncSty{propagate}(D')\siplabel{dopropagate}}{
%             \nl $S \gets \FuncSty{search}(\{ E \in D'\,{:}\,\left|E\right|{>}\,1 \}, B \wedge (v\,{\mapsto}\,v'))$\siplabel{recurse}\;
%             \nl \lIf{$S = \KwSty{true}$}{$\KwSty{return}~\KwSty{true}$}
%             \nl \tikzmark{restart5s}\ElseIf{$S = \KwSty{restart}$}{
%                 \nl \ForEach{$c \in C$}{\nl$\textnormal{post nogood}~B \wedge (v \mapsto c) \Rightarrow \bot$\tikzmark{restart5r}}
%                 \nl $\KwSty{return}~\KwSty{restart}$\tikzmark{restart5e} \;
%             }
%         }
%     }
%     \nl \tikzmark{restart6s}\If{$(R \gets R + 1) = R'$\tikzmark{restart6r}}{
%         \nl post nogood $B$ \;
%         \nl $\KwSty{return}~\KwSty{restart}\tikzmark{restart6e}$\siplabel{countrestarts}}
%     \nl $\KwSty{return}~\KwSty{false}$\siplabel{backtrack}
% }
% \caption{Search routine for \cref{algorithm:sip}.}
% \label{algorithm:sip:search}
% \end{algorithm}
% 
% \begin{algorithm}[tb]\DontPrintSemicolon\small\SetInd{0.1em}{1em}
%     \begin{tikzpicture}[remember picture,overlay]
%         \coordinate (restart7sc) at ($(pic cs:restart7s) + (0, 0.15)$);
%         \coordinate (restart7ec) at ($(pic cs:restart7e) + (0, 0.00)$);
%         \node [fill=highlight, rounded corners, fit=(restart7sc) (restart7ec)] { };
%     \end{tikzpicture}
% \nl $\FuncSty{propagate}$ (Domains $D$) $\rightarrow$ Bool \;
% \nl \Begin{
%     \nl \While{$D_v \gets $\textnormal{a unit domain from}~$D$}{
%         \nl $v' \gets \textnormal{the single value in}~D_v$ \;
%         \nl \tikzmark{restart7s}trigger nogoods with watch $(v \mapsto v')$\tikzmark{restart7e}\siplabel{2wl} \;
%         \nl \ForEach{$D_w \in D \setminus \{ D_v$ \}}{
%             \nl $D_w \gets D_w \setminus \{ v' \}$ \siplabel{removev} \;
%             \nl \ForEach{$(P,\,T) \in L~\KwSty{where}~v \sim_P w$}{\nl$D_w \gets D_w \cap \neighbourhood_T(v')$}
%             \nl \lIf{$D_w = \emptyset$}{$\KwSty{return}~\KwSty{false}$}
%         }
% \nl $(H,\,A,\,n) \gets (\emptyset,\,\emptyset,\,0)$\siplabel{alldiffstart} \;
% \nl \ForEach{$D_v \in D$ \textnormal{in ascending cardinality order\siplabel{eachdomain}}}{
%     \nl $(D_v, A,\,n) \gets (D_v \gets D_v \setminus H\siplabel{elimhall}, A \cup D_v,\,n + 1)$ \siplabel{acc} \;
%     \nl \lIf{$D_v = \emptyset~\vee~|A| < n$}{$\KwSty{return}~\KwSty{false}$\siplabel{failhall}}
%     \nl \lIf{$|A| = n$}{$(H,\,A,\,n) \gets (H \cup A,\,\emptyset,\,0)$\siplabel{hall}}\siplabel{alldiffend}
% }
%     }
% \nl $\KwSty{return}~\KwSty{true}$ \;
% }
% \caption{Propagation routine for \cref{algorithm:sip}.}
% \label{algorithm:sip:propagate}
% \end{algorithm}

% For the variable-ordering heuristic on \siplineref{variableordering} of
% \cref{algorithm:sip}, we use smallest domain first, tiebreaking on highest degree
% \cite{DBLP:journals/ai/HaralickE80,DBLP:journals/jair/McCreeshPST18}. Variable ordering is not the
% focus of this paper---as far as possible, we use this same dynamic ordering throughout the paper, so
% as not to affect the performance of the algorithm on unsatisfiable instances.  For value ordering,
% which occurs on \siplineref{valueordering} of \cref{algorithm:sip}, we will initially select
% vertices from highest degree to lowest degree; value ordering is the major theme of the remainder of
% this paper. Note that the variable-ordering heuristic affects both unsatisfiable and satisfiable
% instances, whilst the value-ordering heuristic is relevant only for satisfiable instances.

\subsection{Experimental Setup}

Our experiments are performed on systems with dual Intel Xeon E5-2697A v4 CPUs and 512GBytes RAM,
running Ubuntu 17.04, using GCC 7.2.0 as the compiler. We use a deterministic pseudo-random number
generator for reproducibility.  We use the dataset introduced by \citet{Kotthoff et al.\
}{DBLP:conf/lion/KotthoffMS16} for evaluation. This dataset brings together a range of
randomly-generated and application instance families from earlier papers:

\begin{description}
    \item[BVG(r), M4D(r), and Rand] are families of randomly generated graphs using different models (bounded
        degree, regular mesh, and uniform), where each pattern is a permuted random connected subgraph
        of the target (and so each instance is satisfiable) \cite{DBLP:journals/pami/CordellaFSV04}.
        These benchmark instances are widely used, but have unusual properties and so broad
        conclusions should not be drawn based solely upon behaviour on these instances
        \cite{DBLP:journals/jair/McCreeshPST18}.
    \item[SF] contains randomly generated scale-free graphs using a similar method
        \cite{DBLP:journals/constraints/ZampelliDS10}.
   \item[LV] consists of various kinds of graph gathered by Larrosa and Valiente
       \cite{DBLP:journals/mscs/LarrosaV02} from the Stanford Graph Database. We include both the
        50 small graphs, and the 50 larger graphs.
    \item[Phase] contains hard crafted instances that lie near the satisfiable / unsatisfiable phase
        transition \cite{DBLP:journals/jair/McCreeshPST18}.
    \item[PR] contains graphs generated from segmented images, corresponding to a computer vision
        problem \cite{DBLP:journals/pr/SolnonDHJ15}.
    \item[Images and Meshes] contain graphs representing 2D segmented images and 3D object models
        respectively, again representing a computer vision problem
        \cite{DBLP:journals/cviu/DamiandSHJS11}.
\end{description}

\noindent
Other studies use a random selection of 200 of each of the instances from the ``meshes'' and
``images'' families because some earlier solvers find many of these instances extremely hard. We
would like to have a larger number of satisfiable instances in our test set, and so we include all
pattern / target pairs. This gives a total of 14,621 instances (rather than the original 5,725). At
least \input{gen-n-sat.tex}\unskip\ of these instances are known to be satisfiable for the
non-induced problem, and at least \input{gen-n-unsat.tex}\unskip\ are unsatisfiable.

\section{Improving Sequential Search}

\begin{figure}[tb]
    \includegraphics*{gen-graph-others.pdf}
    \hfill
    \includegraphics*{gen-graph-others-induced.pdf}

    \bigskip

    \includegraphics*{gen-graph-others-zoom.pdf}
    \hfill
    \includegraphics*{gen-graph-others-induced-zoom.pdf}

    \caption{On the top row, the cumulative number of instances solved over time, comparing our algorithm
    (both in its basic form, and with the improvements introduced in the remainder of the paper) to
    other solvers, for the non-induced and induced problems. On the bottom row, the same,
    considering only satisfiable instances.}
    \label{figure:others}
\end{figure}

We begin by verifying that the Glasgow Subgraph Solver is the state of the art. In the first and
third plots \cref{figure:others} we show the cumulative number of instances solved over time for the
non-induced and induced problems respectively. We compare the Glasgow Subgraph Solver (both
unmodified, and with the modifications described in the remainder of this paper), the PathLAD
variation of the LAD algorithm \cite{DBLP:journals/ai/Solnon10,DBLP:conf/lion/KotthoffMS16}, VF2
\cite{DBLP:journals/pami/CordellaFSV04}, RI \cite{DBLP:journals/bmcbi/BonniciGPSF13}, and VF3
\cite{DBLP:conf/gbrpr/CarlettiFSV17} (which only supports the induced problem), in each case using
the original implementation provided by the algorithm's authors. For each algorithm, the $y$ value
gives the cumulative number of instances which (individually) can be solved in no more than $x$
milliseconds.  The vertical distance between two lines therefore shows how many more instances can
be solved by one solver than another, if every instance is run separately with the chosen $x$
timeout. The horizontal distance shows how many times longer the per-instance timeout would need to
be to allow the rightmost algorithm to succeed on $y$ out of the 14,621 instances (bearing in mind
that the two sets of $y$ instances could be different).  The plots show that our starting point
comfortably beats PathLAD, VF2, VF3 and RI, except for very low choices of timeout.

The dataset includes many instances which are extremely easy for a good solver, and so it can be
hard to see the differences between the stronger solvers at higher runtimes. This paper focusses
upon improving the performance on the remaining hard satisfiable instances, and so in the second and
fourth plots in \cref{figure:others} (and in subsequent cumulative plots) we show only satisfiable
instances, and use a reduced range on both axes.  For the remainder of this paper, we show only
the non-induced problem, which tends to be harder; results with the induced variant are similar.

\subsection{Discrepancy Searches}

A \emph{discrepancy} is where search goes against a value-ordering heuristic.  Discrepancy searches
\cite{DBLP:conf/ijcai/HarveyG95,DBLP:conf/aaai/Korf96,DBLP:conf/ijcai/Walsh97,DBLP:conf/cpaior/KarouiHLN07}
are alternatives to backtracking search that initially search disallowing all discrepancies, and
then retry search allowing an increasing number of discrepancies at each iteration until either a
solution is found or unsatisfiability is proven. These schemes assume that value-ordering heuristics
are usually reliable, and that most solutions can be found with only a small number of
discrepancies; in such cases, the heavy commitment to early branching choices made by conventional
backtracking search can be extremely costly.

\begin{figure}[tb]
    \includegraphics*{gen-graph-dds.pdf}
    \hfill
    \includegraphics*{gen-graph-scatter-dds.pdf}

    \caption{Comparing depth-bounded discrepancy search to backtracking search, both with the degree
    heuristic.}\label{figure:scatter-dds}
\end{figure}

?? Also do LDS?

\Cref{figure:scatter-dds} shows the effects of adding Walsh's \cite{DBLP:conf/ijcai/Walsh97}
depth-bounded discrepancy search (DDS) to the solver (results with other discrepancy search variants
are similar).  Each point represents the solving time for one instance---to avoid noise for easier
instances, we measure only time spent during search, and exclude time spent in preprocessing and
initialisation.  Points below the $x-y$ diagonal are speedups, whilst points on the top and right
axes represent instances which timed out after one thousand seconds with one algorithm, but not the
other. For satisfiable instances, the different point styles show the different families, whilst all
unsatisfiable instances are shown as dark dots.  The points well below the diagonal line and along
the right-hand axis on the scatter plot show that DDS can sometimes be extremely beneficial on
satisfiable instances.  However, on both unsatisfiable and most satisfiable instances, the overheads
can be extremely large, and DDS is much worse in aggregate and is not a viable approach (even when
only considering satisfiable instances).  These large overheads are to be expected: discrepancy
searches are aimed primarily at getting better feasible solutions in optimisation problems which are
too large for a proof of optimality to be a realistic prospect, and they are not well-suited for
unsatisfiable decision problems. Despite this, the extremely large gains on some satisfiable
instances confirm our suspicions that we should find an alternative to heavy-commitment backtracking
search.

\subsection{Value-Ordering Heuristics}

?? Lead in

The left plot of \cref{figure:value-ordering-heuristics} shows the cumulative number of satisfiable
instances solved over time using different value-ordering heuristics.

\begin{description}
\item[Degree] We select vertices from highest degree to lowest degree
\cite{DBLP:conf/ijcai/McCreeshPT16}.

\item[Random] We select vertices uniformly at random.

\item[Anti] We select vertices from lowest degree to highest degree. This heuristic is used as
a sanity check: as expected, degree beats random, and random beats anti.

\item[Biased] We branch by selecting a vertex $v'$ from the chosen domain $D_v$ with
probability \[ p(v') = \frac{2^{\deg(v')}}{\sum_{w \in D_v}{2^{\deg(w)}}} \text{.} \]\end{description}

\noindent
The biased heuristic is new. It continues to prefer vertices of high degree, but will give the same
chance of being selected next to two vertices of equal degree.  It also introduces an element of
randomness, which will be needed to make restarts have an effect.  The performance of the degree and
biased heuristics are very similar: the biased heuristic has slightly worse performance below a 400s
timeout, and effectively equal performance above it. This shows that we can introduce an element of
randomness into the degree value-ordering heuristic without adversely affecting its performance
\emph{in aggregate} (and more generally, this scheme could be used for any value-ordering
heuristic which puts choices in some kind of rank with ties.)

\begin{figure}[tb]
    \includegraphics*{gen-graph-value-ordering-heuristics.pdf}
    \hfill
    \includegraphics*{gen-graph-scatter-heuristics.pdf}

    \caption{On the left, the cumulative number of satisfiable instances solved over time, using
    four different value-ordering heuristics. On the right, an instance by instance comparison of
    the degree and biased heuristics on all instances. Points on the outer axes are timeouts, and
    point style shows instance family.}
    \label{figure:value-ordering-heuristics}
\end{figure}

The right-hand plot of \cref{figure:value-ordering-heuristics} gives a detailed comparison of the
degree and biased heuristics. It shows that despite the aggregate
performance being similar, on a case by case basis, the two heuristics can make a large difference
to the performance of individual satisfiable instances. This suggests that although degree is a good
heuristic, we should perhaps not commit heavily to a vertex of highest degree, but also consider
vertices of the same or similar degree.

?? Intuition

\subsection{Adding Restarts and Nogood Recording}

Another alternative to plain backtracking search is provided by \emph{restarts}. The general idea is
to perform a certain amount of search, and then if no solution has been found (and unsatisfiability
has not been proven), to abandon search and restart from the beginning. Such an approach
can only be beneficial if something changes after restarting---in a constraint programming setting,
this is usually the variable-ordering heuristic
\cite{DBLP:journals/jsat/LecoutreSTV07,DBLP:conf/cp/GayHLS15,DBLP:conf/aaai/LeeSZ16,DBLP:conf/cp/GlorianBLLM17}.
In this paper, we instead rely upon randomness in a special new \emph{value}-ordering heuristic, and continue
to use smallest domain first with static tiebreaking for variable-ordering.\footnote{It may be
possible to further improve the solver by also introducing randomness or some form of
learning into its variable-ordering heuristic. However, simultaneously introducing a second change
would considerably complicate the empirical analysis. Additionally, the solver's current
hand-crafted variable-ordering heuristics already beat adaptive heuristics like impact or
activity-based search.} Using restarts on value-ordering heuristics is uncommon (although
\citet{Razgon et al.\ }{DBLP:conf/flairs/RazgonOP07} look at learning value-ordering heuristics from
restarts, \citet{Chu et al.\ }{DBLP:conf/cp/ChuSS09} use a similar scheme in the context of parallel
search, and an early approach by \citet{Gomes et al.\ }{DBLP:conf/aaai/GomesSK98} does so in an
optimisation context).

% The shaded code shows how to introduce restarts to \cref{algorithm:sip}. The recursive
% $\FuncSty{search}$ procedure is modified to return either true (if the instance is satisfiable),
% false (unsatisfiable), or a third value if some threshold was reached without the solution being
% known.  The main function then calls $\FuncSty{search}$ in a loop (\siplineref{restartsloop}), until
% a solution is found.

Preliminary experiments directed us to use the Luby scheme \cite{DBLP:journals/ipl/LubySZ93} to
determine when to restart.
%We count the number of backtracks
%(that is, when we reach the end of the main $\KwSty{foreach}$ loop on
%\siplineref{countrestarts})
%to decide when to restart, rather than counting recursive calls.
Following convention, we multiply each item in the Luby sequence by a constant---we used the SMAC
automatic parameter tuner \cite{DBLP:conf/lion/HutterHL11} to select the value 660.
% The remainder of the changes are to introduce nogoods.  The $B$ argument to $\FuncSty{search}$ tracks the branching choices
% made so far, in the form of a CNF clause with each literal being a guessed $(v \mapsto v')$
% assignment.
To avoid exploring portions of the search
space that we have already visited, every time we restart, we add new constraints to the problem
which eliminate already-explored subtrees---such a constraint is called a nogood. We generate
simple decision nogoods. That is, upon backtracking due to a decision to restart, we post a nogood
of the form $(v \mapsto v') \wedge (w \mapsto w') \wedge (x \mapsto x') \Rightarrow \bot$ for every
branch to the left of the current (incomplete) branch in the search tree, and when we first make a
decision to restart before backtracking, we post a similar nogood eliminating the entire subtree
explored.

Finally, we use the two watched literals technique \cite{DBLP:conf/dac/MoskewiczMZZM01} to
propagate stored nogoods.
%(\siplineref{2wl}).
This has two benefits: the propagation complexity does not particularly depend upon the number of
stored nogoods, and it does not require any work upon backtracking.  Other more sophisticated nogood
generation and propagation schemes exist \cite{DBLP:conf/aaai/LeeSZ16,DBLP:conf/cp/GlorianBLLM17},
but these would not be helpful here since our solver does not maintain arc consistency or use a
propagation queue.

\subsection{The Effects of Restarts and Nogood Recording}

\begin{figure}[tb]
    \includegraphics*{gen-graph-restarts.pdf}
    \hfill
    \includegraphics*{gen-graph-scatter-by-family.pdf}

    \caption{On the left, the number of satisfiable instances solved over time, with and without
    restarts and nogood recording. To the right, a comparison between the original algorithm and the
    new algorithm with all features enabled.}
    \label{figure:old-vs-new}
\end{figure}

In \cref{figure:old-vs-new} we show the effects of adding restarts and nogood recording to the
algorithm. With restarts and nogood recording, the random value-ordering heuristic comfortably beats
the degree strategy with simple backtracking search. In other words, although having a good
value-ordering heuristic is beneficial, introducing randomness into the search is better, if it is
done alongside a mechanism to avoid heavy commitment to any particular random choice.
\Cref{figure:old-vs-new} also shows that the biased heuristic together with restarts is better
still---that is, if we are introducing restarts, then it is better to add a small amount of
randomness to a tailored heuristic than it is to throw away the heuristic altogether.  Indeed, the
original algorithm can solve 1,983 satisfiable instances by 993.0 seconds, whilst the biased and
random restarting algorithms require only 9.2 seconds and 33.8 seconds respectively to solve the
same number.

In the more detailed view in the right-hand plot of \cref{figure:old-vs-new}, comparing the basic
algorithm with the degree heuristic to the degree-biased algorithm with restarts, all of the
unsatisfiable instances are very close to the $x-y$ diagonal, showing that their performance is
nearly unchanged. On the other hand, there are large numbers of satisfiable instances well below the
diagonal line, indicating large speedups.  Better yet, there are only a handful of satisfiable
instances that are more than a factor of ten times worse.  In other words, as well as improving
performance, we have made up most of the consistency we lost by introducing randomness.
This is a promising result: for the first time, we have introduced a viable alternative to
backtracking search in a setting where we both have a strong \emph{value}-ordering heuristic,
\emph{and} care about both satisfiable and unsatisfiable instances.

\begin{figure}[tb]
    \includegraphics*{gen-graph-scatter-random-goods.pdf}
    \hfill
    \includegraphics*{gen-graph-scatter-random.pdf}
    \caption{On the left, not using nogood recording introduces slowdowns, particularly on
    unsatisfiable instances. On the right, using a random value-ordering gives much worse
    performance on many satisfiable instances.}
    \label{figure:features}
\end{figure}

As we might expect, these properties do not hold if any of the combination of changes are disabled.
In the left-hand plot of \cref{figure:features}, we see large slowdowns on unsatisfiable instances
when using a random value ordering and restarts but without nogood recording, and on the right-hand
plot we see many more satisfiable instances above the $x-y$ diagonal when using the random
value-ordering heuristic as opposed to the degree-biased heuristic.

\section{Parallel Search}

?? No sharing, sharing with Luby restarts, sharing with constant restarts, sharing with triggered
restarts

?? Reproducibility, constant and triggered

?? How many more nodes on unsat instances

?? Scalability to MPI

\section{Maximum Common Subgraph Algorithms}

\begin{figure}[tb]
    \centering
    \includegraphics*{gen-graph-mcs.pdf}

    \medskip

    \includegraphics*{gen-graph-scatter-kdown.pdf}
    \hfill
    \includegraphics*{gen-graph-scatter-mcsplit.pdf}

    \caption{Above, the cumulative number of maximum common induced subgraph instances solved by
    k${\downarrow}$ and McSplit${\downarrow}$ over time, with and without restarts and nogood
    recording. Below, comparing k${\downarrow}$ (left) and McSplit${\downarrow}$ (right) on an instance by instance
    basis.}\label{figure:mcs}
\end{figure}

Having looked at subgraph isomorphism in detail, we now briefly discuss the maximum common induced
subgraph problem, to see whether our new approach to search has more general applicability.  Two
recent algorithms for this problem also make use of backtracking search with degree as a
value-ordering heuristic. The k${\downarrow}$ algorithm \cite{DBLP:conf/aaai/HoffmannMR17} attempts
to solve the problem by first trying to solve the induced subgraph isomorphism problem, and then if
that fails, retries allowing a single unmatched vertex (and thus using weaker invariants), and so
on. Due to its similarity to the Glasgow Subgraph Solver, we can introduce the same bias and restart
strategy.

Meanwhile, the McSplit${\downarrow}$ algorithm
\cite{DBLP:conf/ijcai/McCreeshPT17} uses a constraint programming style search, but with special
propagators and backtrackable data structures that exploit special properties of the problem. The
unconventional domain store used by McSplit${\downarrow}$ precludes the use of arbitrary unit
propagation, and so when introducing restarts, we cannot propagate using nogoods.  Instead, we can
only detect when we are inside an already-visited branch.  We must therefore use the one watched
literal scheme instead, and we also introduce a basic subsumption scheme to prune redundant clauses.

Performance results from these two modified algorithms, using the same families of instances as in
the previous section, are shown in \cref{figure:mcs}. Although we have
moved from a decision problem to an optimisation problem, the same changes remain clearly
beneficial. For the $k{\downarrow}$ algorithm, the change has a minimal effect on many instances
(typically, where the $k = 0$ subproblem is unsatisfiable and hard, and the $k = 1$ subproblem is
satisfiable and easy), but gives large benefits on many more instances than it penalises: it is over
an order of magnitude better on nearly a hundred instances, whilst being an order of magnitude worse
on only seven.

With McSplit${\downarrow}$, the inability to use two watched literals means that in many cases we
introduce a small slowdown. However, the overall pattern is the same: when introducing restarts and
a biased value ordering heuristic, it is much more common to see a large speedup than a large
slowdown.

\section{Conclusion and Future Work}

We have shown that it is both possible and beneficial to introduce a small amount of randomness into
backtracking search algorithms for hard subgraph problems, without compromising existing search
order heuristics, and without paying a penalty for unsatisfiable instances. In principle it is
well-known that introducing some form of randomness \cite{DBLP:conf/aaai/GomesSK98} or reduced
commitment \cite{DBLP:conf/ijcai/HarveyG95} into backtracking search can be beneficial, but until
now it has not been clear how to do so efficiently in practice, particularly when we already have
strong search ordering heuristics and when are expecting to encounter large numbers of unsatisfiable
instances. Our results show that the key to success is a combination of aggressive restarts, nogood
recording, and biasing an existing value-ordering heuristic with just a pinch of randomness.

?? Parallel

?? Apply to CP in general

\section*{Acknowledgements}

The authors would like to thank Ruth Hoffmann, Christophe Lecoutre, Christine Solnon, and Craig
Reilly for their comments.

\bibliographystyle{splncs04}
\bibliography{dblp}

\end{document}

