% vim: set spell spelllang=en tw=100 et sw=4 sts=4 :

\documentclass[runningheads]{llncs}

\usepackage{complexity}
\usepackage{amsmath}
% \usepackage[ruled,vlined]{algorithm2e}
\usepackage{tikz}
\usepackage{cleveref}

% \usepackage{showframe}

\newcommand{\neighbourhood}{\operatorname{N}}
\newcommand{\vertexset}{\operatorname{V}}

\newcommand{\siplabel}[1]{\label{line:sip:#1}}
\newcommand{\siplineref}[1]{line~\ref{line:sip:#1}}
\newcommand{\siplinerangeref}[2]{lines~\ref{line:sip:#1} to~\ref{line:sip:#2}}

\newcommand{\citet}[2]{#1\cite{#2}}

% cref style
\crefname{figure}{Figure}{Figures}
\Crefname{figure}{Figure}{Figures}
\crefname{section}{Section}{Sections}
\Crefname{section}{Section}{Sections}
\crefname{proposition}{Proposition}{Propositions}
\Crefname{proposition}{Proposition}{Propositions}
\crefname{corollary}{Corollary}{Corollaries}
\Crefname{corollary}{Corollary}{Corollaries}

% \crefname{algocf}{Algorithm}{Algorithms}
% \Crefname{algocf}{Algorithm}{Algorithms}

\usetikzlibrary{decorations, decorations.pathreplacing, decorations.pathmorphing,
    calc, backgrounds, positioning, tikzmark, patterns, fit}

\definecolor{highlight}{rgb}{0.529412, 0.74902, 0.466667}

\begin{document}

\title{Sequential and Parallel Solution-Biased Search for Subgraph Algorithms\thanks{This work was
supported by the Engineering and Physical Sciences Research Council (grant numbers EP/P026842/1,
EP/M508056/1, and EP/N007565). This work used the Cirrus UK National Tier-2 HPC Service at EPCC
(http://www.cirrus.ac.uk) funded by the University of Edinburgh and EPSRC (EP/P020267/1).}}

\author{Blair Archibald\inst{1}\orcidID{0000-0003-3699-6658}\and
Fraser Dunlop\inst{2}\orcidID{0000-0002-4485-4871}\and
Ruth Hoffmann\inst{2}\orcidID{0000-0002-1011-5894}\and
Ciaran McCreesh\inst{1}\orcidID{0000-0002-6106-4871}\and
Patrick Prosser\inst{1}\orcidID{0000-0003-4460-6912} \and
James Trimble\inst{1}\orcidID{0000-0001-7282-8745}}

\authorrunning{B. Archibald et al.}

\institute{University of Glasgow, Glasgow, Scotland \and University of St Andrews, St Andrews,
Scotland \\ \email{ciaran.mccreesh@glasgow.ac.uk}}

\maketitle

\begin{abstract}
    The current state of the art in subgraph isomorphism solving involves using degree as a
    value-ordering heuristic to direct backtracking search. Such a search makes a heavy commitment
    to the first branching choice, which is often incorrect. To mitigate this, we introduce and
    evaluate a new approach, which we call ``solution-biased search''. By combining a
    slightly-random value-ordering heuristic, rapid restarts, and nogood recording, we design an
    algorithm which instead uses degree to direct the proportion of search effort spent in different
    subproblems. This increases performance by two orders of magnitude on satisfiable instances,
    whilst not affecting performance on unsatisfiable instances. This algorithm can also be
    parallelised in a very simple but effective way: across both satisfiable and unsatisfiable
    instances, we get a further speedup of over thirty from thirty-six cores, or ninety-five from
    ten distributed-memory hosts. Finally, we show that solution-biased search is also suitable for
    optimisation problems, by using it to improve two maximum common induced subgraph algorithms.
\end{abstract}

\section{Introduction}

The subgraph isomorphism problem is to decide whether a copy of a small ``pattern'' graph occurs
inside a larger ``target'' graph. The problem is broadly applicable, arising in areas
including bioinformatics \cite{DBLP:journals/bmcbi/BonniciGPSF13}, chemistry \cite{o:Regin95}, computer vision
\cite{DBLP:journals/cviu/DamiandSHJS11,DBLP:journals/pr/SolnonDHJ15}, law enforcement
\cite{DBLP:journals/cacm/CoffmanGM04}, model checking \cite{DBLP:journals/tcs/SevegnaniC15},
malware detection \cite{DBLP:conf/dimva/BruschiMM06}, compilers
\cite{DBLP:conf/cgo/MurrayF12,DBLP:conf/cp/BlindellLCS15}, pattern recognition
\cite{DBLP:journals/ijprai/FoggiaPV14}, program similarity
comparison \cite{o:DallaPredaV17}, the design of mechanical locks
\cite{DBLP:journals/siamrev/VomelLBF17}, and graph databases
\cite{DBLP:journals/jair/McCreeshPST18}.

Although the problem is \NP-complete, by combining design techniques from artificial intelligence
with careful algorithm engineering, modern subgraph isomorphism solvers can often produce exact
solutions quickly even on graphs with thousands of vertices. The current single strongest subgraph
isomorphism solver uses ``highest degree first'' as a value-ordering heuristic to direct a
constraint programming style search
\cite{DBLP:conf/cp/McCreeshP15,DBLP:conf/lion/KotthoffMS16,DBLP:journals/jair/McCreeshPST18}. This
heuristic is much better than branching randomly, but is still far from perfect. To offset mistakes
made by this heuristic, this paper proposes a new perspective on value-ordering: rather than
defining a search order, we use degree to direct what \emph{proportion} of the search effort should
be spent in each subproblem. By combining rapid restarts and nogood recording, and introducing a
small amount of randomness into the value-ordering heuristic, we make a state-of-the-art subgraph
algorithm perform two orders of magnitude better on a large number of satisfiable instances, whilst
performing worse only rarely on satisfiable instances, and never on unsatisfiable instances.  This
strategy is also effective in an optimisation setting, producing benefits in two maximum common
induced subgraph algorithms.

This new form of search can also be parallelised, with a \emph{much} simpler implementation than
conventional work-stealing. By running many threads with different random seeds but the same restart
schedule, and sharing nogoods only following restarts, we can achieve speedups of thirty-one from a
thirty-six core machine, or ninety-five by using ten such machines with message passing.

\subsection{Background}

% Let $G$ be a graph. We denote its set of vertices by $\vertexset(G)$, and write $v \sim_G w$ to mean
% that vertices $v$ and $w$ are adjacent; we allow a vertex to be adjacent to itself (a \emph{loop}). The
% \emph{neighbourhood} of a vertex, $\neighbourhood_G(v)$, is the set of vertices adjacent to $v$, and
% the \emph{degree} of $v$, $\deg_G(v)$, is the cardinality of its neighbourhood. When the graph is clear from the context, we omit
% the $G$ subscripts. As per \citet{Hoffmann et al.\ }{DBLP:conf/aaai/HoffmannMR17}, $G^{n,\ell}$ denotes the graph with
% vertex set $\vertexset(G)$, and with edges between vertices $v$ and $w$ if there are at least $n$
% simple paths of length $\ell$ between vertices $v$ and $w$ in $G$.

The \emph{non-induced subgraph isomorphism problem} is to find an injective mapping from the
vertices of a \emph{pattern} graph $\mathcal{P}$ to the vertices of a \emph{target} graph
$\mathcal{T}$, such that adjacent vertices in $\mathcal{P}$ are mapped to adjacent vertices in
$\mathcal{T}$ (including that vertices with loops in $\mathcal{P}$ may only be mapped to vertices
with loops in $\mathcal{T}$).  The \emph{induced} problem additionally requires that non-adjacent
vertices are mapped to non-adjacent vertices. The \emph{degree} of a vertex is the number of other
vertices to which it is adjacent.

% We write $v \mapsto v'$ to mean that pattern vertex $v$ is mapped to target $v'$ under
% either kind of mapping. Finally, we define a function $\FuncSty{compatible}(v, w)$ to mean that
% mapping $v \mapsto w$ is feasible based upon both the loop rule, and the neighbourhood degree
% sequence relation of \citet{Zampelli et al.\ }{DBLP:journals/constraints/ZampelliDS10}.

This paper looks at improving the Glasgow Subgraph
Solver\footnote{\url{https://github.com/ciaranm/glasgow-subgraph-solver/}}, which can solve both the
non-induced and the induced subgraph isomorphism problems.  The solver is very closely based upon
the $k{\downarrow}$ algorithm of \citet{Hoffmann et al.\ }{DBLP:conf/aaai/HoffmannMR17} with $k =
0$, and we refer the reader to that paper for full technical details; that algorithm, in turn, is a
simplification and re-engineering of an older Glasgow algorithm
\cite{DBLP:conf/cp/McCreeshP15,DBLP:conf/lion/KotthoffMS16}.  Essentially, the solver is
a dedicated forward-checking constraint programming implementation specifically for subgraph
problems. It works with a model having a variable per pattern graph vertex, with domains ranging
over the target graph vertices, and performs a backtracking search to map each pattern vertex to a
target vertex whilst propagating adjacency and injectivity constraints (together with further
implied constraints based upon degrees and paths).  However, it uses specialised bit-parallel data
structures and algorithms, and propagates constraints in a fixed order rather than using a queue.

% We now briefly outline the non-shaded parts of \cref{algorithm:sip}, which is how we will solve the
% subgraph isomorphism problem. This algorithm is very closely based upon the $k{\downarrow}$
% algorithm of \citet{Hoffmann et al.\ }{DBLP:conf/aaai/HoffmannMR17} with $k = 0$, and we refer the reader to that paper
% for full technical details; that algorithm, in turn, is a simplification of the Glasgow2 algorithm
% \cite{DBLP:conf/cp/McCreeshP15,DBLP:conf/lion/KotthoffMS16}. We show the non-induced problem; the
% induced problem may be solved by including the loop-complements of $\mathcal{P}$ and $\mathcal{T}$
% in $L$ on \siplineref{supplemental}.
%
% \Cref{algorithm:sip} works by recursively building up a valid mapping of pattern vertices to target
% vertices. For each pattern vertex $v$, we have a domain $D_v$ containing a set of feasible target
% vertices. Each domain is initialised at the top of search (\siplinerangeref{initstart}{initend}),
% using the invariants described by \citet{Zampelli et al.\ }{DBLP:journals/constraints/ZampelliDS10} and
% \citet{Hoffmann et al.\ }{DBLP:conf/aaai/HoffmannMR17} to restrict its initial values.
%
% The $\FuncSty{search}$ procedure recursively selects an unassigned domain $D$
% (\siplineref{variableordering}), and tries assigning
% it each of its remaining values in turn (\siplineref{valueordering}). The effects of this assignment
% are propagated (\siplineref{dopropagate}). If an
% inconsistency is detected, we continue and try another value; otherwise, we recurse
% (\siplineref{recurse}) until a solution is found. Finally, if we run out of values, we backtrack
% (\siplineref{backtrack}).
%
% Our $\FuncSty{propagate}$ routine is also close to that of \citet{Hoffmann et al.\ }{DBLP:conf/aaai/HoffmannMR17},
% differing only in that we carry out the filtering for injectivity
% (\siplinerangeref{alldiffstart}{alldiffend}) after every assignment rather than once at the end of
% the unit propagation loop. (We make this change because the all-different propagator used is not
% idempotent, and we wish to avoid an odd interaction when we introduce nogood filtering.)

% \begin{algorithm}[p]\DontPrintSemicolon\small\SetInd{0.1em}{1em}
%     \begin{tikzpicture}[remember picture,overlay]
%         \coordinate (restart1sc) at ($(pic cs:restart1s) + (0, 0.15)$);
%         \coordinate (restart1ec) at ($(pic cs:restart1e) + (0, 0.00)$);
%         \node [fill=highlight, rounded corners, fit=(restart1sc) (restart1ec)] { };
%         \coordinate (restart2sc) at ($(pic cs:restart2s) + (0, 0.15)$);
%         \coordinate (restart2ec) at ($(pic cs:restart2e) + (0, 0.00)$);
%         \node [fill=highlight, rounded corners, fit=(restart2sc) (restart2ec)] { };
%         \coordinate (restart9sc) at ($(pic cs:restart9s) + (0, 0.15)$);
%         \coordinate (restart9ec) at ($(pic cs:restart9e) + (-0.15, 0.00)$);
%         \node [fill=highlight, rounded corners, fit=(restart9sc) (restart9ec)] { };
%     \end{tikzpicture}
%
%     \nl $\FuncSty{subgraphIsomorphism}$ (Graph $\mathcal{P}$, Graph $\mathcal{T}$) $\rightarrow$ Bool \;
% \nl \Begin{
%     \nl \lIf{$\left|\vertexset(\mathcal{P})\right| >
%     \left|\vertexset(\mathcal{T})\right|$}{$\KwSty{return}~\KwSty{false}$\siplabel{enough}}
%     \nl Discard isolated vertices in $\mathcal{P}$\siplabel{isolated} \;
%     \nl \KwSty{global} $L \gets \big[ (\mathcal{P},\ \mathcal{T}), \siplabel{supplemental}
%         (\mathcal{P}^{1,2}, \mathcal{T}^{1,2}),
%         (\mathcal{P}^{2,2},\ \mathcal{T}^{2,2}),$ \\
%         $\hspace*{3em}(\mathcal{P}^{3,2},\ \mathcal{T}^{3,2}),
%         (\mathcal{P}^{4,2},\ \mathcal{T}^{4,2}) \big]$ \;
%     \nl \ForEach{\siplabel{initstart}$v \in \vertexset(\mathcal{P})$}{
%         \nl $D_v \gets \vertexset(\mathcal{T})$\;
%         \nl \ForEach{$(P,\,T) \in L$}{\nl$D_v \gets \{ w \in D_v : \FuncSty{compatible}(v, w)\}$\siplabel{initend}
%         }
%     }
%     \nl \While{$\KwSty{true}$\siplabel{restartsloop}}{
%         \nl \lIf{$\KwSty{not}~\FuncSty{propagate}(D)$}{$\KwSty{return}~\KwSty{false}$}
%         \nl \tikzmark{restart1s}$\KwSty{global}~(R, R') \gets (0, \textnormal{the next Luby sequence value})$\tikzmark{restart1e} \;
%         \nl $S \gets \FuncSty{search}(\{ E \in D : \left|E\right| > 1 \}, \top)$ \;
%         \nl \tikzmark{restart9s}\lIf{$S \ne \KwSty{restart}$}{\tikzmark{restart9e}$\KwSty{return}~S$}
%         \nl \tikzmark{restart2s}trigger nogoods containing only one clause\tikzmark{restart2e} \;
%     }
% }
% \caption{The main subgraph isomorphism algorithm. The shaded code is added for restarting. See
%     \cref{algorithm:sip:search,algorithm:sip:propagate} for the search and propagation routines.}
% \label{algorithm:sip}
% \end{algorithm}
%
% \begin{algorithm}[p]\DontPrintSemicolon\small\SetInd{0.1em}{1em}
%     \begin{tikzpicture}[remember picture,overlay]
%         \coordinate (restart3sc) at ($(pic cs:restart3s) + (0, 0.15)$);
%         \coordinate (restart3ec) at ($(pic cs:restart3e) + (0, 0.00)$);
%         \node [fill=highlight, rounded corners, fit=(restart3sc) (restart3ec)] { };
%         \coordinate (restart4sc) at ($(pic cs:restart4s) + (0, 0.15)$);
%         \coordinate (restart4ec) at ($(pic cs:restart4e) + (0, 0.00)$);
%         \node [fill=highlight, rounded corners, fit=(restart4sc) (restart4ec)] { };
%         \coordinate (restart5sc) at ($(pic cs:restart5s) + (0, 0.15)$);
%         \coordinate (restart5ec) at ($(pic cs:restart5e) - (0, 0.10)$);
%         \coordinate (restart5rc) at ($(pic cs:restart5r) + (0, 0.00)$);
%         \node [fill=highlight, rounded corners, fit=(restart5sc) (restart5ec) (restart5rc)] { };
%         \coordinate (restart6sc) at ($(pic cs:restart6s) + (0, 0.15)$);
%         \coordinate (restart6ec) at ($(pic cs:restart6e) + (0, 0.00)$);
%         \coordinate (restart6rc) at ($(pic cs:restart6r) + (0.8, 0.00)$);
%         \node [fill=highlight, rounded corners, fit=(restart6sc) (restart6ec) (restart6rc)] { };
%         \coordinate (restart8sc) at ($(pic cs:restart8s) + (0.05, 0.15)$);
%         \coordinate (restart8ec) at ($(pic cs:restart8e) + (0, 0.00)$);
%         \node [fill=highlight, rounded corners, fit=(restart8sc) (restart8ec)] { };
%     \end{tikzpicture}
%     \nl $\FuncSty{search}$ (Domains $D$, Trail $B$) $\rightarrow$ Bool
%     \tikzmark{restart8s}\KwSty{or} \KwSty{restart}\tikzmark{restart8e} \;
% \nl \Begin{
%     \nl \lIf{$D = \emptyset$}{$\KwSty{return}~\KwSty{true}$}
%     \nl $D_v \gets \textnormal{a domain from}~D~\textnormal{chosen by variable heuristic}$\siplabel{variableordering} \;
%     \nl \tikzmark{restart3s}$C \gets \emptyset$\tikzmark{restart3e} \;
%     \nl \ForEach {$v' \in D_v~\textnormal{ordered by value heuristic}$\siplabel{valueordering}}{
%         \nl \tikzmark{restart4s}$C \gets C \cup \{ v' \}$\tikzmark{restart4e} \;
%         \nl $D' \gets \FuncSty{clone}(D)$ \;
%         \nl $D'_v \gets \{ v' \} $ \;
%         \nl \If{\FuncSty{propagate}(D')\siplabel{dopropagate}}{
%             \nl $S \gets \FuncSty{search}(\{ E \in D'\,{:}\,\left|E\right|{>}\,1 \}, B \wedge (v\,{\mapsto}\,v'))$\siplabel{recurse}\;
%             \nl \lIf{$S = \KwSty{true}$}{$\KwSty{return}~\KwSty{true}$}
%             \nl \tikzmark{restart5s}\ElseIf{$S = \KwSty{restart}$}{
%                 \nl \ForEach{$c \in C$}{\nl$\textnormal{post nogood}~B \wedge (v \mapsto c) \Rightarrow \bot$\tikzmark{restart5r}}
%                 \nl $\KwSty{return}~\KwSty{restart}$\tikzmark{restart5e} \;
%             }
%         }
%     }
%     \nl \tikzmark{restart6s}\If{$(R \gets R + 1) = R'$\tikzmark{restart6r}}{
%         \nl post nogood $B$ \;
%         \nl $\KwSty{return}~\KwSty{restart}\tikzmark{restart6e}$\siplabel{countrestarts}}
%     \nl $\KwSty{return}~\KwSty{false}$\siplabel{backtrack}
% }
% \caption{Search routine for \cref{algorithm:sip}.}
% \label{algorithm:sip:search}
% \end{algorithm}
%
% \begin{algorithm}[tb]\DontPrintSemicolon\small\SetInd{0.1em}{1em}
%     \begin{tikzpicture}[remember picture,overlay]
%         \coordinate (restart7sc) at ($(pic cs:restart7s) + (0, 0.15)$);
%         \coordinate (restart7ec) at ($(pic cs:restart7e) + (0, 0.00)$);
%         \node [fill=highlight, rounded corners, fit=(restart7sc) (restart7ec)] { };
%     \end{tikzpicture}
% \nl $\FuncSty{propagate}$ (Domains $D$) $\rightarrow$ Bool \;
% \nl \Begin{
%     \nl \While{$D_v \gets $\textnormal{a unit domain from}~$D$}{
%         \nl $v' \gets \textnormal{the single value in}~D_v$ \;
%         \nl \tikzmark{restart7s}trigger nogoods with watch $(v \mapsto v')$\tikzmark{restart7e}\siplabel{2wl} \;
%         \nl \ForEach{$D_w \in D \setminus \{ D_v$ \}}{
%             \nl $D_w \gets D_w \setminus \{ v' \}$ \siplabel{removev} \;
%             \nl \ForEach{$(P,\,T) \in L~\KwSty{where}~v \sim_P w$}{\nl$D_w \gets D_w \cap \neighbourhood_T(v')$}
%             \nl \lIf{$D_w = \emptyset$}{$\KwSty{return}~\KwSty{false}$}
%         }
% \nl $(H,\,A,\,n) \gets (\emptyset,\,\emptyset,\,0)$\siplabel{alldiffstart} \;
% \nl \ForEach{$D_v \in D$ \textnormal{in ascending cardinality order\siplabel{eachdomain}}}{
%     \nl $(D_v, A,\,n) \gets (D_v \gets D_v \setminus H\siplabel{elimhall}, A \cup D_v,\,n + 1)$ \siplabel{acc} \;
%     \nl \lIf{$D_v = \emptyset~\vee~|A| < n$}{$\KwSty{return}~\KwSty{false}$\siplabel{failhall}}
%     \nl \lIf{$|A| = n$}{$(H,\,A,\,n) \gets (H \cup A,\,\emptyset,\,0)$\siplabel{hall}}\siplabel{alldiffend}
% }
%     }
% \nl $\KwSty{return}~\KwSty{true}$ \;
% }
% \caption{Propagation routine for \cref{algorithm:sip}.}
% \label{algorithm:sip:propagate}
% \end{algorithm}

% For the variable-ordering heuristic on \siplineref{variableordering} of
% \cref{algorithm:sip}, we use smallest domain first, tiebreaking on highest degree
% \cite{DBLP:journals/ai/HaralickE80,DBLP:journals/jair/McCreeshPST18}. Variable ordering is not the
% focus of this paper---as far as possible, we use this same dynamic ordering throughout the paper, so
% as not to affect the performance of the algorithm on unsatisfiable instances.  For value ordering,
% which occurs on \siplineref{valueordering} of \cref{algorithm:sip}, we will initially select
% vertices from highest degree to lowest degree; value ordering is the major theme of the remainder of
% this paper. Note that the variable-ordering heuristic affects both unsatisfiable and satisfiable
% instances, whilst the value-ordering heuristic is relevant only for satisfiable instances.

\subsection{Experimental Setup}

Our experiments are performed on the EPCC Cirrus HPC facility, on systems with dual Intel Xeon
E5-2695 v4 CPUs and 256GBytes RAM, running Centos 7.3.1611. We use GCC 7.2.0 as the compiler. For
parallelism, we use C++ native threads, and for distributed parallelism we also use the SGI MPT
implementation of MPI. All timing measurements are steady-clock, and we use a deterministic
pseudo-random number generator for reproducibility.

We use the dataset introduced by \citet{Kotthoff et al.\ }{DBLP:conf/lion/KotthoffMS16} for
evaluation. This dataset brings together a range of randomly-generated and application instance
families from earlier papers:

\begin{description}
    \item[BVG(r), M4D(r), and Rand] are families of randomly generated graphs using different models (bounded
        degree, regular mesh, and uniform), where each pattern is a permuted random connected subgraph
        of the target (and so each instance is satisfiable) \cite{DBLP:journals/pami/CordellaFSV04}.
        These benchmark instances are widely used, but have unusual properties and so broad
        conclusions should not be drawn based solely upon behaviour of these instances
        \cite{DBLP:journals/jair/McCreeshPST18}.
    \item[SF] contains randomly generated scale-free graphs using a similar method
        \cite{DBLP:journals/constraints/ZampelliDS10}.
   \item[LV] consists of various kinds of graph gathered by Larrosa and Valiente
       \cite{DBLP:journals/mscs/LarrosaV02} from the Stanford Graph Database. We include both the
        50 small graphs, and the 50 large graphs.
    \item[Phase] contains hand crafted instances that lie near the satisfiable / unsatisfiable phase
        transition \cite{DBLP:journals/jair/McCreeshPST18}.
    \item[PR] contains graphs generated from segmented images, corresponding to a computer vision
        problem \cite{DBLP:journals/pr/SolnonDHJ15}.
    \item[Images and Meshes] contain graphs representing 2D segmented images and 3D object models,
        again representing a computer vision problem \cite{DBLP:journals/cviu/DamiandSHJS11}.
\end{description}

\noindent
Other studies use a random selection of 200 of each of the instances from the ``meshes'' and
``images'' families because some earlier solvers find many of these instances extremely hard. We
would like to have a larger number of satisfiable instances in our test set, and so we include all
pattern / target pairs. This gives a total of 14,621 instances (rather than the original 5,725). At
least \input{gen-n-sat.tex}\unskip\ of these instances are known to be satisfiable for the
non-induced problem, and at least \input{gen-n-unsat.tex}\unskip\ are unsatisfiable.

\section{Improving Sequential Search}

\begin{figure}[tb]
    \includegraphics{gen-graph-others.pdf}
    \hfill
    \includegraphics{gen-graph-others-induced.pdf}

    \bigskip

    \includegraphics{gen-graph-others-zoom.pdf}
    \hfill
    \includegraphics{gen-graph-others-induced-zoom.pdf}

    \caption{On the top row, the cumulative number of instances solved over time, comparing the
    Glasgow Subgraph Solver (both in its basic form, and with the improvements introduced in the
    remainder of the paper) to other solvers, for the non-induced and induced problems. On the
    bottom row, the same, considering only satisfiable instances.}
    \label{figure:others}
\end{figure}

We begin with a set of baseline performance measurements. In the top
two plots of \cref{figure:others} we show the cumulative number of instances solved over time for the
non-induced and induced problems respectively. We compare the Glasgow Subgraph Solver using
depth-first search (DFS) and with the modifications described in the remainder of this paper
(solution-biased search, SBS), the PathLAD variation of the LAD algorithm
\cite{DBLP:journals/ai/Solnon10,DBLP:conf/lion/KotthoffMS16}, VF2
\cite{DBLP:journals/pami/CordellaFSV04}, RI \cite{DBLP:journals/bmcbi/BonniciGPSF13}, and VF3
\cite{DBLP:conf/gbrpr/CarlettiFSV17} (which only supports the induced problem), in each case using
the original implementation provided by the algorithm's authors.  The plots show that our starting
point comfortably beats PathLAD, VF2, VF3 and RI, except for very low choices of timeout. For each
algorithm, the $y$ value gives the cumulative number of instances which (individually) can be solved
in no more than $x$ milliseconds.  The vertical distance between two lines therefore shows how many
more instances can be solved by one solver than another, if every instance is run separately with
the chosen $x$ timeout. The horizontal distance shows how many times longer the per-instance timeout
would need to be to allow the rightmost algorithm to succeed on $y$ out of the 14,621 instances
(bearing in mind that the two sets of $y$ instances could be different), and gives a measure called
\emph{aggregate speedup} \cite{DBLP:conf/cpaior/HoffmannMNPRS018}.

The dataset includes many instances which are extremely easy for a good solver, and so it can be
hard to see the differences between the stronger solvers at higher runtimes. This paper focusses
upon improving the performance on the remaining hard satisfiable instances, and so in the bottom two
plots in \cref{figure:others} (and in subsequent cumulative plots for sequential algorithms) we show
only satisfiable instances, and use a reduced range on both axes.  For the remainder of this paper,
we show only the non-induced problem, which tends to be harder; results with the induced variant are
similar.

\subsection{Discrepancy Searches}

A \emph{discrepancy} is where search goes against the advice of a value-ordering heuristic.
Discrepancy searches
\cite{DBLP:conf/ijcai/HarveyG95,DBLP:conf/aaai/Korf96,DBLP:conf/ijcai/Walsh97,DBLP:conf/cpaior/KarouiHLN07}
are alternatives to backtracking search that initially search disallowing all discrepancies, and
then retry search allowing an increasing number of discrepancies at each iteration until either a
solution is found or unsatisfiability is proven. These schemes assume that value-ordering heuristics
are usually reliable, and that most solutions can be found with only a small number of
discrepancies. In such cases, the heavy commitment to early branching choices made by
backtracking search can be extremely costly.

\begin{figure}[tb]
    \includegraphics{gen-graph-dds.pdf}
    \hfill
    \includegraphics{gen-graph-scatter-dds.pdf}

    \caption{Comparing depth-bounded discrepancy search (DDS) to depth-first backtracking search
    (DFS), both using degree as the value-ordering heuristic.}\label{figure:scatter-dds}
\end{figure}

\Cref{figure:scatter-dds} shows the effects of adding Walsh's \cite{DBLP:conf/ijcai/Walsh97}
depth-bounded discrepancy search (DDS) to the solver (results with other discrepancy search variants
are similar). On the scatter plot, each point represents the solving time for one instance---to
avoid noise for easier instances, we measure only time spent during search, and exclude time spent
in preprocessing and initialisation.  Points below the $x-y$ diagonal are speedups, whilst points on
the top and right axes represent instances which timed out after one thousand seconds with one
algorithm, but not the other. For satisfiable instances, the different point styles show the
different families, whilst all unsatisfiable instances are shown as dark dots.  The points well
below the diagonal line and along the right-hand axis on the scatter plot show that DDS can
sometimes be extremely beneficial on satisfiable instances.  However, on both unsatisfiable and most
satisfiable instances, the overheads can be extremely large, and DDS is much worse in aggregate and
is not a viable approach (even when only considering satisfiable instances).  These large overheads
are to be expected: discrepancy searches are aimed primarily at getting better feasible solutions in
optimisation problems which are too large for a proof of optimality to be a realistic prospect, and
they are not well-suited for unsatisfiable decision problems. Despite this, the extremely large
gains on some satisfiable instances confirm our suspicions that we should find an alternative to
heavy-commitment backtracking search.

\subsection{Value-Ordering Heuristics}

\begin{figure}[tb]
    \includegraphics{gen-graph-value-ordering-heuristics.pdf}
    \hfill
    \includegraphics{gen-graph-scatter-heuristics.pdf}

    \caption{On the left, the cumulative number of satisfiable instances solved over time, using
    four different value-ordering heuristics. On the right, an instance by instance comparison of
    the degree and biased heuristics on all instances. Points on the outer axes are timeouts, and
    point style shows instance family.}
    \label{figure:value-ordering-heuristics}
\end{figure}

Traditionally, value-ordering heuristics are designed to drive search towards the most promising
region of the search space \cite{DBLP:conf/ecai/Geelen92}, or the most constrained
\cite{DBLP:conf/aaai/GentMPW96}, or the region with the highest solution density
\cite{DBLP:journals/jair/PesantQZ12}. In subgraph isomorphism, this is done by selecting vertices
from highest degree to lowest \cite{DBLP:journals/jair/McCreeshPST18}. The left-hand plot of
\cref{figure:value-ordering-heuristics} demonstrates that this is indeed a good choice: the
\textbf{Degree} heuristic's line shows much better performance on satisfiable instances than the
\textbf{Random} (branch randomly) or \textbf{Anti} (branch from lowest degree to highest) heuristic
lines. Meanwhile, on unsatisfiable instances, the value-ordering heuristic has no effect on
performance, because a complete search must be performed.

But what happens if our value-ordering heuristic has to choose between mapping a pattern vertex to
one of, for example, three target vertices of degree ten, two vertices of degree nine, or five of
degree two? When driving conventional backtracking search, the degree heuristic would pick one of
the vertices of degree ten, and we would commit all of our search effort to the exponentially large
search tree underneath it, not considering any other choice until this tree has been fully explored
and eliminated.  We will show that this is not a wise choice, and that instead, we should
\emph{commit equal search effort} to each of the three subproblems found by mapping to vertices of
degree ten.  And similarly, should we be certain that a vertex of degree ten is so much better than a
vertex of degree nine that we should commit no effort to degree nine vertices until all the degree
ten subproblems have been explored? Or might it be better to commit, say, twice as much effort to
each degree ten subproblems as to each degree nine subproblems, and only a very small amount of
effort to the degree two subproblems?  To test this hypothesis, we will now introduce a new
alternative to backtracking search, which we call \emph{solution-biased search}.  This search is
made up of three components: a new slightly-random value-ordering heuristic, rapid restarts, and
nogood recording. The aim is to perform a complete search, but spending proportionally more time in
parts of the search tree that are preferred by the value-ordering heuristic.

\subsection{Biased Value-Ordering}

We first define a new \textbf{Biased} value-ordering heuristic, as follows. When branching, we
select a vertex $v'$ from the chosen domain $D_v$ with probability \[ p(v') =
\frac{2^{\deg(v')}}{\sum_{w \in D_v}{2^{\deg(w)}}} \text{.} \] This heuristic is now equally likely
to pick between vertices of equal degree, is twice as likely to pick a vertex of degree $d$ as one
of degree $d - 1$, and is over a thousand times more likely to pick a vertex of degree $d$ than
degree $d - 10$.

\Cref{figure:value-ordering-heuristics} confirms that this heuristic, when used with backtracking
search, will solve close to the same number of instances as the degree heuristic would for any given
choice of timeout. In other words, we can introduce an element of randomness into the degree
value-ordering heuristic without adversely affecting its performance \emph{in aggregate}.  The
right-hand plot gives a detailed comparison. It shows that despite the aggregate performance being
similar, on a case by case basis, the two heuristics can make a large difference to the performance
for individual satisfiable instances. This justifies our belief that although degree is a good
heuristic, we should perhaps not commit heavily to a single vertex of highest degree, but also consider
vertices of the same or similar degree.

\subsection{Restarting Search and Nogood Recording}

Having introduced a new value-ordering heuristic, we must now also move away from depth-first
backtracking search. We do this by using \emph{restarts} and \emph{nogood recording}. The general
idea is to perform a certain amount of search, and then if no solution has been found (and
unsatisfiability has not been proven), to abandon search and restart from the beginning. Such an
approach can only be beneficial if something changes after restarting---in a constraint programming
setting, this is usually the variable-ordering heuristic
\cite{DBLP:journals/jsat/LecoutreSTV07,DBLP:conf/cp/GayHLS15,DBLP:conf/aaai/LeeSZ16,DBLP:conf/cp/GlorianBLLM17}.
In this paper, we instead rely upon randomness in our new \emph{value}-ordering heuristic, and
continue to use smallest domain first with static tiebreaking for variable-ordering.\footnote{It may
be possible to further improve the solver by also introducing randomness or some form of learning
into its variable-ordering heuristic. However, simultaneously introducing a second change would
considerably complicate the empirical analysis. Additionally, the solver's current hand-crafted
variable-ordering heuristics already beat adaptive heuristics like impact or activity-based search.}
Using restarts on value-ordering heuristics is uncommon (although \citet{Razgon et al.\
}{DBLP:conf/flairs/RazgonOP07} look at learning value-ordering heuristics from restarts, \citet{Chu
et al.\ }{DBLP:conf/cp/ChuSS09} use a similar scheme in the context of parallel search, and an early
approach by \citet{Gomes et al.\ }{DBLP:conf/aaai/GomesSK98} does so in an optimisation context).

% The shaded code shows how to introduce restarts to \cref{algorithm:sip}. The recursive
% $\FuncSty{search}$ procedure is modified to return either true (if the instance is satisfiable),
% false (unsatisfiable), or a third value if some threshold was reached without the solution being
% known.  The main function then calls $\FuncSty{search}$ in a loop (\siplineref{restartsloop}), until
% a solution is found.

Preliminary experiments directed us to use the Luby scheme \cite{DBLP:journals/ipl/LubySZ93} to
determine when to restart.
%We count the number of backtracks
%(that is, when we reach the end of the main $\KwSty{foreach}$ loop on
%\siplineref{countrestarts})
%to decide when to restart, rather than counting recursive calls.
Following convention, we multiply each item in the Luby sequence by a constant---we used the SMAC
automatic parameter tuner \cite{DBLP:conf/lion/HutterHL11} to select the value 660.
% The remainder of the changes are to introduce nogoods.  The $B$ argument to $\FuncSty{search}$ tracks the branching choices
% made so far, in the form of a CNF clause with each literal being a guessed $(v \mapsto v')$
% assignment.

To avoid exploring portions of the search
space that we have already visited, every time we restart, we add new constraints to the problem
which eliminate already-explored subtrees---such a constraint is called a \emph{nogood}. We generate
simple decision nogoods. That is, upon backtracking due to a decision to restart, we post a nogood
of the form $(v \mapsto v') \wedge (w \mapsto w') \wedge (x \mapsto x') \Rightarrow \bot$ for every
branch to the left of the current (incomplete) branch in the search tree, and when we first make a
decision to restart before backtracking, we post a similar nogood eliminating the entire subtree
explored. We use the two watched literals technique \cite{DBLP:conf/dac/MoskewiczMZZM01} to
propagate stored nogoods.
%(\siplineref{2wl}).
This has two benefits: the propagation complexity does not particularly depend upon the number of
stored nogoods, and it does not require any work upon backtracking.  Other more sophisticated nogood
generation and propagation schemes exist \cite{DBLP:conf/aaai/LeeSZ16,DBLP:conf/cp/GlorianBLLM17},
but these are not helpful in this setting (our solver does not maintain arc consistency or use a
propagation queue).

\subsection{Solution-Biased Search in Practice}

\begin{figure}[tb]
    \includegraphics{gen-graph-restarts.pdf}
    \hfill
    \includegraphics{gen-graph-scatter-by-family.pdf}

    \caption{On the left, the number of satisfiable instances solved over time, comparing
    solution-biased search (SBS), random search with restarts (RSR), and the three value-ordering
    heuristics with conventional depth-first search. To the right, a comparison between the original
    algorithm and solution-biased search.}
    \label{figure:old-vs-new}
\end{figure}

In \cref{figure:old-vs-new} we show the effects of adding restarts and nogood recording to the
algorithm. With restarts and nogood recording (random search with restarts, RSR), the random
value-ordering heuristic comfortably beats the degree strategy with depth-first search. In other
words, although having a good value-ordering heuristic is beneficial, introducing randomness into
the search is better, if it is done alongside a mechanism to avoid heavy commitment to any
particular random choice. However, the biased heuristic together
with restarts (solution-biased search, SBS) is better still---that is, if we are introducing
restarts, then it is better to add a small amount of randomness to a tailored heuristic than it is
to throw away the heuristic altogether.  Indeed, the original algorithm can solve 1983 satisfiable
instances by 909 seconds, whilst the biased and random restarting algorithms require only 12 seconds
and 35 seconds respectively to solve the same number.

In the more detailed view in the right-hand plot of \cref{figure:old-vs-new}, comparing the original
algorithm to solution-biased search, all of the unsatisfiable instances are very close to the $x-y$
diagonal, showing that their performance is nearly unchanged. On the other hand, there are large
numbers of satisfiable instances well below the diagonal line, indicating large speedups.  Better
yet, there are only a handful of satisfiable instances that are more than a factor of ten times
worse.  In other words, as well as improving performance, we have made up most of the consistency we
lost by introducing randomness.

\begin{figure}[tb]
    \includegraphics{gen-graph-scatter-biased-goods.pdf}
    \hfill
    \includegraphics{gen-graph-scatter-random.pdf}
    \caption{On the left, not using nogood recording introduces slowdowns, particularly on
    unsatisfiable instances. On the right, using a random value-ordering gives much worse
    performance on many satisfiable instances.}
    \label{figure:features}
\end{figure}

As we might expect, these properties do not hold if any of the combination of changes are disabled.
In the left-hand plot of \cref{figure:features}, we see large slowdowns on unsatisfiable instances
when disabling nogood recording, and on the right-hand plot we see many more satisfiable instances
above the $x-y$ diagonal when using the random value-ordering heuristic as opposed to the
degree-biased heuristic.

\subsection{Solution-Biased Search in Theory}

Although we have shown that it provides good results, we have yet to justify where the biased
formula comes from, or indeed why we call this approach ``solution-biased''.  Our goal is to use
biased randomness in a value-ordering heuristic to spend time in subproblems proportional to an
estimate of their solution density \cite{DBLP:journals/jair/PesantQZ12}. Such an approach is better
than committing entirely to the area of maximum solution density because estimators only give a
probability---although we may estimate that one subtree has twice the solution density of another,
in reality the ``better'' subtree may contain no solutions at all.

To estimate solution density, we need an estimate of how big different subproblems are likely to be,
and of how many solutions each subproblem is likely to contain. Of course, obtaining exact (or even
approximate) values for these figures is at least as hard as solving the problem in its entirety,
but we may obtain usable approximations.  For pairs of Erd\H{o}s-R\'enyi
random graphs with large solution counts (i.e.\ chosen from within the ``easy satisfiable'' region
\cite{DBLP:journals/jair/McCreeshPST18}), we can observe a linear relationship between subproblem size
and number of solutions. Thus, for graphs from this distribution, we need only an estimator of
subproblem size. Measurements also suggest that, for pairs of Erd\H{o}s-R\'enyi graphs, subproblems
under a target vertex of degree $d$ tend to contain a small constant times more search nodes than
those under a target vertex of degree $d - 1$.  This empirical analysis suggests that an estimator
that is exponential in $d$ will give our method the desired behaviour, at least for
Erd\H{o}s-R\'enyi graphs. We expect it may be possible to derive better estimators for particular
input classes, although over the full range of problem instances, we have verified that exponential
estimators substantially outperform polynomial and factorial weightings.

\section{Parallel Search}

Exploiting multiple cores to speed up constraint programming solvers remains an active area of
research, with no universally perfect solution being available. Four of the more common approaches
are based upon decompositions
\cite{DBLP:journals/corr/abs-1008-4328,DBLP:journals/jair/MalapertRR16}, work stealing
\cite{DBLP:conf/cp/MichelSH07,DBLP:conf/cp/ChuSS09,DBLP:conf/cp/McCreeshP15,DBLP:conf/cpaior/HoffmannMNPRS018},
parallel discrepancy searches \cite{DBLP:conf/cp/MoisanGQ13,DBLP:conf/cpaior/MoisanQG14}, and
algorithm portfolios \cite{DBLP:conf/lion/LindauerHH15}. Decomposition approaches are unsuitable for
decision problems, or problems where we have good value-ordering heuristics, because the
decomposition interferes strongly with the shape of the search tree
\cite{DBLP:journals/jair/MalapertRR16}. Work stealing, traditionally, also interferes with
value-ordering \cite{DBLP:journals/topc/McCreeshP15}, although specially designed exceptions exist
\cite{DBLP:conf/cp/ChuSS09,DBLP:conf/cp/McCreeshP15,DBLP:conf/cpaior/HoffmannMNPRS018}. However,
these have very complicated implementations. Parallel discrepancy searches are aware of
value-ordering heuristics, but have other limitations: they struggles on search trees with heavy
filtering, and rely upon inner search tree nodes being orders of magnitude less expensive to process
than leaf nodes. Portfolios, meanwhile, typically rely upon running multiple models or heuristics
simultaneously, and selecting whichever finishes first, whereas here we have a known good model and
set of heuristics.

\subsection{Shared Memory Parallelism}

\begin{figure}[tb]
    \includegraphics{gen-graph-parallel.pdf}

    \medskip

    \includegraphics{gen-graph-scatter-par.pdf}
    \hfill
    \includegraphics{gen-graph-scatter-dist.pdf}

    \caption{Above, the cumulative number of instances solved over time, comparing the sequential
    algorithm to results using 32 threads on a single machine, and using five, ten or twenty
    distributed memory hosts. Below, instance by instance comparisons.}\label{figure:parallel}
\end{figure}

Solution-biased search allows for a much simpler parallel implementation. We create a
number of threads, and give each thread its own random seed; otherwise each thread performs the same
sequential search. Threads synchronise on restarts, a simple barrier causing each thread to wait for
every other thread to also restart.  Nogoods from all threads are then gathered and combined before
search resumes, now with a larger set of nogoods than in a sequential run.  Finally, whenever any
single thread terminates, either due to having found a solution or proved unsatisfiability, then
every other thread may immediately terminate.

This technique requires only limited changes to the top level search driver, and none whatsoever to
the main recursive search algorithm. Notably, it does not require any locking or communication
during the recursive search, aside from a single atomic boolean flag to assist early termination.  A
number of factors combine to make this approach feasible:

\begin{itemize}
    \item Each thread will be run with the same restart schedule, and so will spend approximately
        the same amount of time between restarts. Because the only synchronisation between threads
        is at a restart, we expect threads to be busy doing search.  (This is in contrast
        to an alternative method for paralellising restarting search \cite{DBLP:conf/aaai/CireKS14},
        which packs together successive sequence values to produce a balanced workload.) This
        approach therefore avoids the irregular task size issues which usually arise in parallel
        combinatorial search.
    \item Sequentially, on non-trivial instances the algorithm will restart often (many
        tens of thousands, for instances that reach the thousand second timeout).
    \item Because the search trees we explore are exponentially large, the randomness in the
        value-ordering heuristic is sufficient to ensure that most of the time, threads are
        exploring different parts of the search tree.
    \item The gathering of nogoods to describe the work done so far provides an alternative to
        requiring either a specific mechanism to allocate work, or expensive synchronisation between threads.
    \item If sometimes threads do happen to explore part of
        the same subproblems, this is not a problem: if the instance is satisfiable, either thread
        might find a solution first, and if the instance is unsatisfiable, we merely introduce some
        redundancy into the proof.\footnote{For solving a counting or enumeration problem,
        matters become slightly more complicated, but not devastatingly so.} The combination of
        rapid restarts and nogood recording is enough to ensure that this is only a small overhead.
\end{itemize}

\Cref{figure:parallel} shows how this scheme performs in practice. Sequentially, we can solve 14,357
instances within the thousand second timeout, with the last instance being solved at 939.0 seconds.
Using thirty-six threads on machines with two eighteen core processors, we can solve 14,357
instances with a timeout of only 74.2 seconds, giving an aggregate speedup
\cite{DBLP:conf/cpaior/HoffmannMNPRS018} of 12.7.

Closer inspection of the results reveals that with this many threads, a considerable proportion of
the overall search time is spent with threads waiting at the barrier for synchronisation. This is
because the time taken to carry out search until 660 backtracks are encountered is only
\emph{roughly} a constant (in practice it usually varies by around a factor of two). Furthermore,
the Luby sequence includes occasional large multipliers, and if unsatisfiability is proved during
towards the end of one of these runs, each thread will end up duplicating a large amount of work.

Because we are using nogood recording, an alternative approach is possible. Rather than using the
Luby sequence for restarts, we could restart after either a constant number of backtracks, or simply
after a certain time interval has passed---this bounds the maximum possible idle time that threads
could spend at a barrier. \Cref{figure:parallel} also shows the effects of restarting every 100ms.
Sequentially, this approach is slightly better than using the Luby sequence, being able to solve
14,370 instances, with the last at 996.4 seconds. With thirty-six threads, solving this many
instances takes 31.8 seconds, giving an aggregate speedup of 31.4.

It is important to emphasise that there is no expectation of a linear speedup from this approach
\cite{DBLP:conf/aaai/LiW84,DBLP:journals/tc/LiW86,DBLP:conf/irregular/BruinKT95},
particularly for satisfiable instances. Due to the biased random nature of the value-ordering heuristic,
one of our extra threads may ``get lucky'' and find a solution much quicker than we would
sequentially, leading to a superlinear speedup. Conversely, our extra threads may contribute nothing to finding a
solution---or worse, due to new nogoods altering the choice made by the random branching heuristic,
we could even get an absolute slowdown. We can see both of these effects in the bottom left plot of
\cref{figure:parallel}: although we achieve roughly a linear speedup on unsatisfiable instances,
satisfiable instances show much greater variability. However, this approach does at least mirror our intuition of
allocating search effort in proportion to where the value-ordering heuristic believes it will be
most fruitful, and so we should not be too surprised that we see roughly a linear speedup on average
on harder instances.

Additionally, for easy instances, most of the algorithm's execution time is spent in a preprocessing
phase. We have not parallelised this, which is why our results are poor below the one second mark.

\subsection{Distributed Memory Parallelism}

To further test the scalability of this technique, we also used MPI to implement a distributed-memory
parallelism layer on top of the threaded layer. In contrast to the huge difficulties of
implementing work-stealing in a distributed memory setting, this required only the addition of two
MPI calls: an ``all gather'' operation to communicate nogoods, and a ``gather'' to collect and
combine the results of each host. Termination, meanwhile, was handled by posting an empty nogood
(and so termination can only occur on a restart).

\Cref{figure:parallel} also shows the results of these experiments, using five, ten, and twenty
hosts.  Because each host has two CPU sockets, the five host results use ten MPI ranks, each with
eighteen threads, and the ten and twenty host results use twenty and forty MPI ranks respectively.
The supercomputing service we use is not designed for huge numbers of very short problems, and so we
ran only instances whose sequential runtime was at least one second; for the sake of plotting
results, we treat skipped instances as taking one second. Due to the job launcher used, it is also
not possible to accurately measure ``total'' runtime including startup costs, and so instead we
report the runtime of the rank zero process---this figure is somewhat optimistic for very easy
instances. With these caveats in mind, when seeing how long a timeout is needed to solve any 14,370
instances, we get aggregate speedups of 57.1 from five hosts and 95.5 from ten hosts over a
sequential baseline; using twenty hosts is slightly slower, due to increased overheads. However,
when seeing how long a timeout is needed to solve 14,424 instances, distributed parallelism gives us
further aggregate speedups of 2.2, 3.5, and 7.9 from five, ten, and twenty hosts, over the single
machine parallel baseline.

\section{Maximum Common Subgraph Algorithms}

\begin{figure}[tb]
    \includegraphics{gen-graph-mcs.pdf}

    \medskip

    \includegraphics{gen-graph-scatter-kdown.pdf}
    \hfill
    \includegraphics{gen-graph-scatter-mcsplit.pdf}

    \caption{Above, the cumulative number of maximum common induced subgraph instances solved by
    k${\downarrow}$ and McSplit${\downarrow}$ over time, with the two forms of search. Below,
    comparing k${\downarrow}$ (left) and McSplit${\downarrow}$ (right) on an instance by instance
    basis.}\label{figure:mcs}
\end{figure}

Having looked at subgraph isomorphism in detail, we now briefly discuss the maximum common induced
subgraph problem, to see whether our new approach to search has more general applicability.  Two
recent algorithms for this problem also make use of backtracking search with degree as a
value-ordering heuristic. The k${\downarrow}$ algorithm \cite{DBLP:conf/aaai/HoffmannMR17} attempts
to solve the problem by first trying to solve the induced subgraph isomorphism problem, and then if
that fails, retries allowing a single unmatched vertex (and thus using weaker invariants), and so
on. Due to its similarity to the Glasgow Subgraph Solver, we can introduce the same bias and restart
strategy.

Meanwhile, the McSplit${\downarrow}$ algorithm
\cite{DBLP:conf/ijcai/McCreeshPT17} uses a constraint programming style search, but with special
propagators and backtrackable data structures that exploit special properties of the problem. The
unconventional domain store used by McSplit${\downarrow}$ precludes the use of arbitrary unit
propagation, and so when introducing restarts, we cannot propagate using nogoods.  Instead, we can
only detect when we are inside an already-visited branch.  We must therefore use the one watched
literal scheme instead, and we also introduce a basic subsumption scheme to prune redundant clauses.

Performance results from these two modified algorithms, using the same families of instances as in
the previous section, are shown in \cref{figure:mcs}. Although we have
moved from a decision problem to an optimisation problem, the same changes remain clearly
beneficial. For the $k{\downarrow}$ algorithm, the change has a minimal effect on many instances
(typically, where the $k = 0$ subproblem is unsatisfiable and hard, and the $k = 1$ subproblem is
satisfiable and easy), but gives large benefits on many more instances than it penalises: it is over
an order of magnitude better on over three hundred instances, whilst being an order of magnitude worse
on only seven.

With McSplit${\downarrow}$, the inability to use two watched literals means that in many cases we
introduce a small slowdown. However, the overall pattern is the same: when introducing restarts and
a biased value ordering heuristic, it is much more common to see a large speedup than a large
slowdown.

\section{Conclusion and Future Work}

The conventional view of value-ordering heuristics is that they define a search order. We have
proposed an alternative perspective, where value-orderings define a weighting specifying how much
search effort should be put into different subproblems, based upon a rough estimate of solution
densities. We have also shown how to turn this perspective into an algorithm, by combining a biased
random value-ordering heuristic with rapid restarts and nogood recording. This combination of
techniques gives us, for the first time, a practical alternative to backtracking search where we
have a strong \emph{value}-ordering heuristic, and where we care both about satisfiable \emph{and}
unsatisfiable instances.

A further benefit is the ease with which such a search can be parallelised. By having each thread
carry out the same search with a different random seed, and sharing nogoods only on restarts, we
remove the need for intrusive changes to the core search algorithm, eliminate the irregularity
problem, and still respect the advice of the value-ordering heuristic.

We believe that these technique are broadly applicable, beyond subgraph algorithms, and we intend to
implement them in a full constraint programming solver. We are also interested in making better use
of statistical knowledge (either \emph{a priori} or learned during search) to further refine the
biased randomisation process. And finally, we are trying hard to work out whether our new
perspective also has some relevance to \emph{variable} ordering heuristics.

\section*{Acknowledgements}

The authors would like to thank \"{O}zg\"{u}r Akg\"{u}n, Chris Jefferson, Sonja Kraiczy, Christophe
Lecoutre, Christine Solnon, and Craig Reilly for their comments.

\bibliographystyle{splncs04}
\bibliography{dblp}

\end{document}
