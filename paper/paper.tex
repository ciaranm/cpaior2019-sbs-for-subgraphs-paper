% vim: set spell spelllang=en tw=100 et sw=4 sts=4 :

\def\year{2019}\relax
%File: formatting-instruction.tex
\documentclass[letterpaper]{article} %DO NOT CHANGE THIS
\usepackage{aaai19}  %Required
\usepackage{times}  %Required
\usepackage{helvet}  %Required
\usepackage{courier}  %Required
\usepackage{url}  %Required
\usepackage{graphicx}  %Required
\frenchspacing  %Required
\setlength{\pdfpagewidth}{8.5in}  %Required
\setlength{\pdfpageheight}{11in}  %Required
%PDF Info Is Required:
  \pdfinfo{
/Title (Value Ordering, Discrepancies, and Restarts for Subgraph Algorithms)
/Author (Anonymous and Anonymous and Anonymous)}
\setcounter{secnumdepth}{0}  

\usepackage{complexity}
\usepackage{amsmath}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{tikz}
\usepackage{cleveref}

% \usepackage{showframe}

\newcommand{\neighbourhood}{\operatorname{N}}
\newcommand{\vertexset}{\operatorname{V}}

\newcommand{\siplabel}[1]{\label{line:sip:#1}}
\newcommand{\siplineref}[1]{line~\ref{line:sip:#1}}
\newcommand{\siplinerangeref}[2]{lines~\ref{line:sip:#1} to~\ref{line:sip:#2}}

\newcommand{\citet}[1]{\citeauthor{#1} \shortcite{#1}}

% cref style
\crefname{figure}{Figure}{Figures}
\Crefname{figure}{Figure}{Figures}
\crefname{section}{Section}{Sections}
\Crefname{section}{Section}{Sections}
\crefname{proposition}{Proposition}{Propositions}
\Crefname{proposition}{Proposition}{Propositions}
\crefname{corollary}{Corollary}{Corollaries}
\Crefname{corollary}{Corollary}{Corollaries}

\crefname{algocf}{Algorithm}{Algorithms}
\Crefname{algocf}{Algorithm}{Algorithms}

\usetikzlibrary{decorations, decorations.pathreplacing, decorations.pathmorphing,
    calc, backgrounds, positioning, tikzmark, patterns, fit}

\definecolor{highlight}{rgb}{0.529412, 0.74902, 0.466667}

\title{Value Ordering, Discrepancies, and Restarts for Subgraph Algorithms}
\author{Anonymous \and Anonymous \and Anonymous \\ email address \\ address}

\begin{document}

\maketitle

\begin{abstract}
    Modern subgraph isomorphism solvers use degree-based value-ordering heuristics to direct
    backtracking search. This kind of search makes a heavy commitment to the first branching choice,
    which is often incorrect. We evaluate discrepancy-based searches as an alternative, but find
    them too expensive to be effective in this situation, even on satisfiable instances. We then
    demonstrate that a random value-ordering heuristic with restarts beats conventional search with
    the best-known tailored heuristic. This motivates the introduction of a novel slightly-random
    heuristic with similar behaviour to the tailored heuristic: when combined with restarts and
    nogood recording, this new search strategy is over a hundred times more effective on satisfiable
    instances, and does not affect performance on unsatisfiable instances.  Finally, we show that
    this strategy also improves two maximum common induced subgraph algorithms.
\end{abstract}

\section{Introduction}

The subgraph isomorphism problem is to decide whether a copy of a small ``pattern'' graph occurs
inside a larger ``target'' graph. Although \NP-complete, by combining design techniques from
artificial intelligence with careful algorithm engineering, modern subgraph isomorphism solvers can
often produce exact solutions quickly even on graphs with thousands of vertices. The current single
strongest subgraph isomorphism solver uses ``highest degree first'' as a value-ordering heuristic to
direct a constraint programming style search
\cite{DBLP:conf/cp/McCreeshP15,DBLP:conf/lion/KotthoffMS16,DBLP:journals/jair/McCreeshPST18}. This
heuristic is much better than branching randomly, but is still far from perfect. We therefore
investigate different techniques for offsetting mistakes in value-ordering decisions. We first try
discrepancy searches, which turn out to be much too expensive to be a practical solution. We then
look at using random value-ordering, restarts, and nogood recording, which together give better
aggregate performance at the expense of much worse behaviour on many instances. Next, we introduce a
new way of adapting the existing value-ordering heuristic to introduce just a small amount of
randomness.  This lets us make a state-of-the-art subgraph algorithm perform much better
on a large number of satisfiable instances, whilst performing worse only rarely on satisfiable
instances, and never on unsatisfiable instances. Finally, we show that this strategy is also
effective in an optimisation setting, producing benefits in two maximum common induced subgraph
algorithms. This combination of techniques gives us, for the first time, a practical alternative to
backtracking search for constraint programming style algorithms where we have a strong
value-ordering heuristic and where we care both about satisfiable and unsatisfiable instances.

\section{Definitions and Algorithms}

Let $G$ be a graph. We denote its set of vertices by $\vertexset(G)$, and write $v \sim_G w$ to mean
that vertices $v$ and $w$ are adjacent; we allow a vertex to be adjacent to itself (a \emph{loop}). The
\emph{neighbourhood} of a vertex, $\neighbourhood_G(v)$, is the set of vertices adjacent to $v$, and
the \emph{degree} of $v$, $\deg_G(v)$, is the cardinality of its neighbourhood. When the graph is clear from the context, we omit
the $G$ subscripts. As per \citet{DBLP:conf/aaai/HoffmannMR17}, $G^{n,\ell}$ denotes the graph with
vertex set $\vertexset(G)$, and with edges between vertices $v$ and $w$ if there are at least $n$
simple paths of length $\ell$ between vertices $v$ and $w$ in $G$.

The \emph{non-induced subgraph isomorphism problem} is to find an injective mapping from the
vertices of a pattern graph $\mathcal{P}$ to a target graph $\mathcal{T}$, such that adjacent
vertices in $\mathcal{P}$ are mapped to adjacent vertices in $\mathcal{T}$ (including that vertices
with loops in $\mathcal{P}$ may only be mapped to vertices with loops in $\mathcal{T}$). The
\emph{induced} problem additionally requires that non-adjacent vertices are mapped to non-adjacent
vertices. We write $v \mapsto v'$ to mean that pattern vertex $v$ is mapped to target $v'$ under
either kind of mapping. Finally, we define a function $\FuncSty{compatible}(v, w)$ to mean that
mapping $v \mapsto w$ is feasible based upon both the loop rule, and the neighbourhood degree
sequence relation of \citet{DBLP:journals/constraints/ZampelliDS10}.

\subsection{A Subgraph Isomorphism Algorithm}

We now briefly outline the non-shaded parts of \cref{algorithm:sip}, which is how we will solve the
subgraph isomorphism problem. This algorithm is very closely based upon the $k{\downarrow}$
algorithm of \citet{DBLP:conf/aaai/HoffmannMR17} with $k = 0$, and we refer the reader to that paper
for full technical details; that algorithm, in turn, is a simplification of the Glasgow2 algorithm
\cite{DBLP:conf/cp/McCreeshP15,DBLP:conf/lion/KotthoffMS16}. We show the non-induced problem; the
induced problem may be solved by including the loop-complements of $\mathcal{P}$ and $\mathcal{T}$
in $L$ on \siplineref{supplemental}.

\Cref{algorithm:sip} works by recursively building up a valid mapping of pattern vertices to target
vertices. For each pattern vertex $v$, we have a domain $D_v$ containing a set of feasible target
vertices. Each domain is initialised at the top of search (\siplinerangeref{initstart}{initend}),
using the invariants described by \citet{DBLP:journals/constraints/ZampelliDS10} and
\citet{DBLP:conf/aaai/HoffmannMR17} to restrict its initial values.

The $\FuncSty{search}$ procedure recursively selects an unassigned domain $D$
(\siplineref{variableordering}), and tries assigning
it each of its remaining values in turn (\siplineref{valueordering}). The effects of this assignment
are propagated (\siplineref{dopropagate}). If an
inconsistency is detected, we continue and try another value; otherwise, we recurse
(\siplineref{recurse}) until a solution is found. Finally, if we run out of values, we backtrack
(\siplineref{backtrack}).

Our $\FuncSty{propagate}$ routine is also close to that of \citet{DBLP:conf/aaai/HoffmannMR17},
differing only in that we carry out the filtering for injectivity
(\siplinerangeref{alldiffstart}{alldiffend}) after every assignment rather than once at the end of
the unit propagation loop. (We make this change because the all-different propagator used is not
idempotent, and we wish to avoid an odd interaction when we introduce nogood filtering.)

\begin{algorithm}[p]\DontPrintSemicolon\small\SetInd{0.1em}{1em}
    \begin{tikzpicture}[remember picture,overlay]
        \coordinate (restart1sc) at ($(pic cs:restart1s) + (0, 0.15)$);
        \coordinate (restart1ec) at ($(pic cs:restart1e) + (0, 0.00)$);
        \node [fill=highlight, rounded corners, fit=(restart1sc) (restart1ec)] { };
        \coordinate (restart2sc) at ($(pic cs:restart2s) + (0, 0.15)$);
        \coordinate (restart2ec) at ($(pic cs:restart2e) + (0, 0.00)$);
        \node [fill=highlight, rounded corners, fit=(restart2sc) (restart2ec)] { };
        \coordinate (restart3sc) at ($(pic cs:restart3s) + (0, 0.15)$);
        \coordinate (restart3ec) at ($(pic cs:restart3e) + (0, 0.00)$);
        \node [fill=highlight, rounded corners, fit=(restart3sc) (restart3ec)] { };
        \coordinate (restart4sc) at ($(pic cs:restart4s) + (0, 0.15)$);
        \coordinate (restart4ec) at ($(pic cs:restart4e) + (0, 0.00)$);
        \node [fill=highlight, rounded corners, fit=(restart4sc) (restart4ec)] { };
        \coordinate (restart5sc) at ($(pic cs:restart5s) + (0, 0.15)$);
        \coordinate (restart5ec) at ($(pic cs:restart5e) - (0, 0.10)$);
        \coordinate (restart5rc) at ($(pic cs:restart5r) + (0, 0.00)$);
        \node [fill=highlight, rounded corners, fit=(restart5sc) (restart5ec) (restart5rc)] { };
        \coordinate (restart6sc) at ($(pic cs:restart6s) + (0, 0.15)$);
        \coordinate (restart6ec) at ($(pic cs:restart6e) + (0, 0.00)$);
        \coordinate (restart6rc) at ($(pic cs:restart6r) + (0.8, 0.00)$);
        \node [fill=highlight, rounded corners, fit=(restart6sc) (restart6ec) (restart6rc)] { };
        \coordinate (restart7sc) at ($(pic cs:restart7s) + (0, 0.15)$);
        \coordinate (restart7ec) at ($(pic cs:restart7e) + (0, 0.00)$);
        \node [fill=highlight, rounded corners, fit=(restart7sc) (restart7ec)] { };
        \coordinate (restart8sc) at ($(pic cs:restart8s) + (0.05, 0.15)$);
        \coordinate (restart8ec) at ($(pic cs:restart8e) + (0, 0.00)$);
        \node [fill=highlight, rounded corners, fit=(restart8sc) (restart8ec)] { };
        \coordinate (restart9sc) at ($(pic cs:restart9s) + (0, 0.15)$);
        \coordinate (restart9ec) at ($(pic cs:restart9e) + (-0.15, 0.00)$);
        \node [fill=highlight, rounded corners, fit=(restart9sc) (restart9ec)] { };
    \end{tikzpicture}

    \nl $\FuncSty{subgraphIsomorphism}$ (Graph $\mathcal{P}$, Graph $\mathcal{T}$) $\rightarrow$ Bool \;
\nl \Begin{
    \nl \lIf{$\left|\vertexset(\mathcal{P})\right| >
    \left|\vertexset(\mathcal{T})\right|$}{$\KwSty{return}~\KwSty{false}$\siplabel{enough}}
    \nl Discard isolated vertices in $\mathcal{P}$\siplabel{isolated} \;
    \nl \KwSty{global} $L \gets \big[ (\mathcal{P},\ \mathcal{T}), \siplabel{supplemental}
        (\mathcal{P}^{1,2}, \mathcal{T}^{1,2}),
        (\mathcal{P}^{2,2},\ \mathcal{T}^{2,2}),$ \\
        $\hspace*{3em}(\mathcal{P}^{3,2},\ \mathcal{T}^{3,2}),
        (\mathcal{P}^{4,2},\ \mathcal{T}^{4,2}) \big]$ \;
    \nl \ForEach{\siplabel{initstart}$v \in \vertexset(\mathcal{P})$}{
        \nl $D_v \gets \vertexset(\mathcal{T})$\;
        \nl \ForEach{$(P,\,T) \in L$}{\nl$D_v \gets \{ w \in D_v : \FuncSty{compatible}(v, w)\}$\siplabel{initend}
        }
    }
    \nl \While{$\KwSty{true}$\siplabel{restartsloop}}{
        \nl \lIf{$\KwSty{not}~\FuncSty{propagate}(D)$}{$\KwSty{return}~\KwSty{false}$}
        \nl \tikzmark{restart1s}$\KwSty{global}~(R, R') \gets (0, \textnormal{the next Luby sequence value})$\tikzmark{restart1e} \;
        \nl $S \gets \FuncSty{search}(\{ E \in D : \left|E\right| > 1 \}, \top)$ \;
        \nl \tikzmark{restart9s}\lIf{$S \ne \KwSty{restart}$}{\tikzmark{restart9e}$\KwSty{return}~S$}
        \nl \tikzmark{restart2s}trigger nogoods containing only one clause\tikzmark{restart2e} \;
    }
}
\medskip
    \nl $\FuncSty{search}$ (Domains $D$, Trail $B$) $\rightarrow$ Bool
    \tikzmark{restart8s}\KwSty{or} \KwSty{restart}\tikzmark{restart8e} \;
\nl \Begin{
    \nl \lIf{$D = \emptyset$}{$\KwSty{return}~\KwSty{true}$}
    \nl $D_v \gets \textnormal{a domain from}~D~\textnormal{chosen by variable heuristic}$\siplabel{variableordering} \;
    \nl \tikzmark{restart3s}$C \gets \emptyset$\tikzmark{restart3e} \;
    \nl \ForEach {$v' \in D_v~\textnormal{ordered by value heuristic}$\siplabel{valueordering}}{
        \nl \tikzmark{restart4s}$C \gets C \cup \{ v' \}$\tikzmark{restart4e} \;
        \nl $D' \gets \FuncSty{clone}(D)$ \;
        \nl $D'_v \gets \{ v' \} $ \;
        \nl \If{\FuncSty{propagate}(D')\siplabel{dopropagate}}{
            \nl $S \gets \FuncSty{search}(\{ E \in D'\,{:}\,\left|E\right|{>}\,1 \}, B \wedge (v\,{\mapsto}\,v'))$\siplabel{recurse}\;
            \nl \lIf{$S = \KwSty{true}$}{$\KwSty{return}~\KwSty{true}$}
            \nl \tikzmark{restart5s}\ElseIf{$S = \KwSty{restart}$}{
                \nl \ForEach{$c \in C$}{\nl$\textnormal{post nogood}~B \wedge (v \mapsto c) \Rightarrow \bot$\tikzmark{restart5r}}
                \nl $\KwSty{return}~\KwSty{restart}$\tikzmark{restart5e} \;
            }
        }
    }
    \nl \tikzmark{restart6s}\If{$(R \gets R + 1) = R'$\tikzmark{restart6r}}{
        \nl post nogood $B$ \;
        \nl $\KwSty{return}~\KwSty{restart}\tikzmark{restart6e}$\siplabel{countrestarts}}
    \nl $\KwSty{return}~\KwSty{false}$\siplabel{backtrack}
}
\medskip
\nl $\FuncSty{propagate}$ (Domains $D$) $\rightarrow$ Bool \;
\nl \Begin{
    \nl \While{$D_v \gets $\textnormal{a unit domain from}~$D$}{
        \nl $v' \gets \textnormal{the single value in}~D_v$ \;
        \nl \tikzmark{restart7s}trigger nogoods with watch $(v \mapsto v')$\tikzmark{restart7e}\siplabel{2wl} \;
        \nl \ForEach{$D_w \in D \setminus \{ D_v$ \}}{
            \nl $D_w \gets D_w \setminus \{ v' \}$ \siplabel{removev} \;
            \nl \ForEach{$(P,\,T) \in L~\KwSty{where}~v \sim_P w$}{\nl$D_w \gets D_w \cap \neighbourhood_T(v')$}
            \nl \lIf{$D_w = \emptyset$}{$\KwSty{return}~\KwSty{false}$}
        }
\nl $(H,\,A,\,n) \gets (\emptyset,\,\emptyset,\,0)$\siplabel{alldiffstart} \;
\nl \ForEach{$D_v \in D$ \textnormal{in ascending cardinality order\siplabel{eachdomain}}}{
    \nl $(D_v, A,\,n) \gets (D_v \gets D_v \setminus H\siplabel{elimhall}, A \cup D_v,\,n + 1)$ \siplabel{acc} \;
    \nl \lIf{$D_v = \emptyset~\vee~|A| < n$}{$\KwSty{return}~\KwSty{false}$\siplabel{failhall}}
    \nl \lIf{$|A| = n$}{$(H,\,A,\,n) \gets (H \cup A,\,\emptyset,\,0)$\siplabel{hall}}\siplabel{alldiffend}
}
    }
\nl $\KwSty{return}~\KwSty{true}$ \;
}
\caption{Subgraph isomorphism. The shaded code is added for restarting.}
\label{algorithm:sip}
\end{algorithm}

For the variable-ordering heuristic on \siplineref{variableordering} of
\cref{algorithm:sip}, we use smallest domain first, tiebreaking on highest degree
\cite{DBLP:journals/ai/HaralickE80,DBLP:journals/jair/McCreeshPST18}. Variable ordering is not the
focus of this paper---as far as possible, we use this same dynamic ordering throughout the paper, so
as not to affect the performance of the algorithm on unsatisfiable instances.  For value ordering,
which occurs on \siplineref{valueordering} of \cref{algorithm:sip}, we will initially select
vertices from highest degree to lowest degree; value ordering is the major theme of the remainder of
this paper. Note that the variable-ordering heuristic affects both unsatisfiable and satisfiable
instances, whilst the value-ordering heuristic is relevant only for satisfiable instances.

\section{Empirical Evaluation}

Our experiments are performed on systems with dual Intel Xeon E5-2697A v4 CPUs and 512GBytes RAM,
running Ubuntu 17.04. We
implemented\footnote{DOI removed for anonymous review}
\cref{algorithm:sip} in C++ using the bit-parallel implementation of
\citet{DBLP:conf/aaai/HoffmannMR17} as a starting point, and compiled it using GCC 7.2.0. We use a
deterministic pseudo-random number generator for reproducibility.

We use the dataset introduced by \citet{DBLP:conf/lion/KotthoffMS16} for evaluation. This dataset
brings together a range of randomly-generated and application instance families from earlier papers:

\begin{description}
    \item[BVG(r), M4D(r), and Rand] are families of randomly generated graphs using different models (bounded
        degree, regular mesh, and uniform), where each pattern is a permuted random connected subgraph
        of the target (and so each instance is satisfiable) \cite{DBLP:journals/pami/CordellaFSV04}.
        These benchmark instances are widely used, but have unusual properties and so broad
        conclusions should not be drawn based solely upon behaviour on these instances
        \cite{DBLP:journals/jair/McCreeshPST18}.
    \item[SF] contains randomly generated scale-free graphs using a similar method
        \cite{DBLP:journals/constraints/ZampelliDS10}.
   \item[LV] consists of various kinds of graph gathered by Larrosa and Valiente
       \cite{DBLP:journals/mscs/LarrosaV02} from the Stanford Graph Database. We include both the
        50 small graphs, and the 50 larger graphs.
    \item[Phase] contains hard crafted instances that lie near the satisfiable / unsatisfiable phase
        transition \cite{DBLP:journals/jair/McCreeshPST18}.
    \item[PR] contains graphs generated from segmented images, corresponding to a computer vision
        problem \cite{DBLP:journals/pr/SolnonDHJ15}.
    \item[Images and Meshes] contain graphs representing 2D segmented images and 3D object models
        respectively, again representing a computer vision problem
        \cite{DBLP:journals/cviu/DamiandSHJS11}.
\end{description}

\noindent
Other studies use a random selection of 200 of each of the instances from the ``meshes'' and
``images'' families because some earlier solvers find many of these instances extremely hard. We
would like to have a larger number of satisfiable instances in our test set, and so we include all
pattern / target pairs. This gives a total of 14,621 instances (rather than the original 5,725). At
least \input{gen-n-sat.tex}\unskip\ of these instances are known to be satisfiable for the
non-induced problem, and at least \input{gen-n-unsat.tex}\unskip\ are unsatisfiable.

\subsection{Baseline Performance}

\begin{figure}[p]
    \centering
    \includegraphics*{gen-graph-others.pdf}

    \smallskip

    \centering
    \includegraphics*{gen-graph-others-zoom.pdf}

    \bigskip

    \centering
    \includegraphics*{gen-graph-others-induced.pdf}

    \smallskip

    \centering
    \includegraphics*{gen-graph-others-induced-zoom.pdf}

    \caption{On the top two rows, the cumulative number of instances solved over time, comparing our algorithm
    (both in its basic form, and with the improvements introduced in the remainder of the paper) to
    other solvers. The second considers only satisfiable instances. On the bottom two rows, the same,
    for the induced subgraph isomorphism problem.}
    \label{figure:others}
\end{figure}

We begin by verifying that our implementation of the basic \cref{algorithm:sip} is competitive. In
the first and third plots \cref{figure:others} we show the cumulative number of instances solved
over time for the non-induced and induced problems respectively. We compare our implementation (both
with the degree heuristic, and with the modifications described in the remainder of this paper), the
Glasgow2 and Glasgow3 algorithms from which \cref{algorithm:sip} is derived
\cite{DBLP:conf/cp/McCreeshP15,DBLP:conf/lion/KotthoffMS16}, the PathLAD variation of the LAD
algorithm \cite{DBLP:journals/ai/Solnon10,DBLP:conf/lion/KotthoffMS16}, VF2
\cite{DBLP:journals/pami/CordellaFSV04}, RI \cite{DBLP:journals/bmcbi/BonniciGPSF13}, and VF3
\cite{DBLP:conf/gbrpr/CarlettiFSV17} (which only supports the induced problem), in each case using
the original implementation provided by the algorithm's authors. For each algorithm, the $y$ value
gives the cumulative number of instances which (individually) can be solved in no more than $x$
milliseconds.  The vertical distance between two lines therefore shows how many more instances can
be solved by one solver than another, if every instance is run separately with the chosen $x$
timeout. The horizontal distance shows how many times longer the per-instance timeout would need to
be to allow the rightmost algorithm to succeed on $y$ out of the 14,621 instances (bearing in mind
that the two sets of $y$ instances could be different).

As expected, in both cases the performance of \cref{algorithm:sip} with the degree heuristic is very
close to that of Glasgow2, although we perform better at lower runtimes because we use simpler
preprocessing at the top of search. Our implementation also clearly beats PathLAD, VF2, VF3 and RI,
except for very low choices of timeout.

The dataset includes many instances which are extremely easy for a good solver, and so it can be
hard to see the differences between the stronger solvers at higher runtimes. This paper focusses
upon improving the performance on the remaining hard satisfiable instances, and so in the second and
fourth plots in \cref{figure:others} (and in subsequent cumulative plots) we show only satisfiable
instances, and use a reduced range on both axes.

\subsection{Value-Ordering Heuristics}

\Cref{figure:value-ordering-heuristics} shows the cumulative number of satisfiable instances
solved over time using different value-ordering heuristics. For the remainder of this section, we
show only the non-induced problem; results with the induced variant are similar.

\begin{description}
\item[Degree] We select vertices from highest degree to lowest degree
\cite{DBLP:conf/ijcai/McCreeshPT16}.

\item[Random] We select vertices uniformly at random.

\item[Anti] We select vertices from lowest degree to highest degree. This heuristic is used as
a sanity check: as expected, degree beats random, and random beats anti.

\item[Biased] We branch by selecting a vertex $v'$ from the chosen domain $D_v$ with
probability \[ p(v') = \frac{2^{\deg(v')}}{\sum_{w \in D_v}{2^{\deg(w)}}} \text{.} \]\end{description}

\noindent
The biased heuristic is new. It continues to prefer vertices of high degree, but will give the same
chance of being selected next to two vertices of equal degree.  It also introduces an element of
randomness, which will be needed to make restarts have an effect.  The performance of the degree and
biased heuristics are very similar: the biased heuristic has slightly worse performance below a 400s
timeout, and effectively equal performance above it. This shows that we can introduce an element of
randomness into the degree value-ordering heuristic without adversely affecting its performance
\emph{in aggregate} (and more generally, this scheme could be used for any value-ordering
heuristic which puts choices in some kind of rank with ties.)

\Cref{figure:scatter-heuristics} gives a detailed look at these results. This scatter plot shows
that despite the aggregate performance being similar, on a case by case basis, the two heuristics
can make a large difference to the performance of satisfiable instances.

\begin{figure}[p]
    \centering
    \includegraphics*{gen-graph-value-ordering-heuristics.pdf}

    \caption{The cumulative number of satisfiable instances solved over time, using four
    different value-ordering heuristics.}
    \label{figure:value-ordering-heuristics}
\end{figure}

\begin{figure}[p]
    \centering
    \includegraphics*{gen-graph-scatter-heuristics.pdf}

    \caption{An instance by instance comparison of the degree and biased heuristics on all
    instances. Points on the outer axes are timeouts, and point style shows instance family.}
    \label{figure:scatter-heuristics}
\end{figure}

\begin{figure}[p]
    \centering
    \includegraphics*{gen-graph-scatter-dds.pdf}

    \caption{Comparing depth-bounded discrepancy search to backtracking search, both with the degree
    heuristic.}\label{figure:scatter-dds}
\end{figure}

\subsection{Discrepancy Searches}

A \emph{discrepancy} is where search goes against a value-ordering heuristic.  Discrepancy searches
\cite{DBLP:conf/ijcai/HarveyG95,DBLP:conf/aaai/Korf96,DBLP:conf/ijcai/Walsh97,DBLP:conf/cpaior/KarouiHLN07}
are alternatives to backtracking search that initially search disallowing all discrepancies, and
then retry search allowing an increasing number of discrepancies at each iteration until either a
solution is found or unsatisfiability is proven. These schemes assume that value-ordering heuristics
are usually reliable, and that most solutions can be found with only a small number of
discrepancies; in such cases, the heavy commitment to early branching choices made by conventional
backtracking search can be extremely costly.

\Cref{figure:scatter-dds} shows the effects of adding Walsh's \cite{DBLP:conf/ijcai/Walsh97}
depth-bounded discrepancy search (DDS) to \cref{algorithm:sip} (results with other discrepancy
search variants are similar). Each point represents the solving
time for one instance---to avoid noise for easier instances, we measure only time spent during
search, and exclude time spent in preprocessing and initialisation.  Points below the $x-y$ diagonal
are speedups, whilst points on the top and right axes represent instances which timed out after one
thousand seconds with one algorithm, but not the other. For satisfiable instances, the different
point styles show the different families, whilst all unsatisfiable instances are shown as dark dots.

The points well below the diagonal line and along the right-hand axis show that DDS can sometimes be
extremely beneficial on satisfiable instances.  However, on both unsatisfiable and most satisfiable
instances, the overheads can be extremely large, and DDS is much worse in aggregate and is not a
viable approach (even when only considering satisfiable instances).  These large overheads are to be
expected: discrepancy searches are aimed primarily at getting better feasible solutions in
optimisation problems which are too large for a proof of optimality to be a realistic prospect, and
they are not well-suited for unsatisfiable decision problems. Despite this, the extremely large gains on
some satisfiable instances confirm our suspicions that we should find an alternative to
heavy-commitment backtracking search.

\subsection{Adding Restarts and Nogood Recording}

Another alternative to plain backtracking search is provided by \emph{restarts}. The general idea is
to perform a certain amount of search, and then if no solution has been found (and unsatisfiability
has not been proven), to abandon search and restart from the beginning. Obviously, such an approach
can only be beneficial if something changes after restarting---in a constraint programming setting,
this is usually the variable-ordering heuristic
\cite{DBLP:journals/jsat/LecoutreSTV07,DBLP:conf/cp/GayHLS15,DBLP:conf/aaai/LeeSZ16,DBLP:conf/cp/GlorianBLLM17}.
In this paper, we instead rely upon randomness in a special new \emph{value}-ordering heuristic, and continue
to use smallest domain first with static tiebreaking for variable-ordering.\footnote{It may be
possible to further improve \cref{algorithm:sip} by also introducing randomness or some form of
learning into its variable-ordering heuristic. However, simultaneously introducing a second change
would considerably complicate the empirical analysis.} Using restarts on value-ordering heuristics
appears to be uncommon---although \citet{DBLP:conf/flairs/RazgonOP07} look at learning
value-ordering heuristics from restarts, \citet{DBLP:conf/cp/ChuSS09} use a similar
scheme in the context of parallel search, and an early approach by \citet{DBLP:conf/aaai/GomesSK98} does so in an optimisation context.

The shaded code shows how to introduce restarts to \cref{algorithm:sip}. The recursive
$\FuncSty{search}$ procedure is modified to return either true (if the instance is satisfiable),
false (unsatisfiable), or a third value if some threshold was reached without the solution being
known.  The main function then calls $\FuncSty{search}$ in a loop (\siplineref{restartsloop}), until
a solution is found.

We use the Luby scheme \cite{DBLP:journals/ipl/LubySZ93} to determine when to restart. We count the
number of backtracks (that is, when we reach the end of the main $\KwSty{foreach}$ loop on
\siplineref{countrestarts}) to decide when to restart, rather than counting recursive calls.
Following established wisdom, we multiply each item in the Luby sequence by a magic constant---we
used the SMAC automatic parameter tuner \cite{DBLP:conf/lion/HutterHL11} to select the value 660.
(We also tried a geometric restart scheme, which gave less favourable results.)

The remainder of the changes are to introduce nogoods. To avoid exploring portions of the search
space that we have already visitied, every time we restart, we add new constraints to the problem
which eliminate already-explored subtrees---such a constraint is called a nogood. We generate simple
decision nogoods, as follows. The $B$ argument to $\FuncSty{search}$ tracks the branching choices
made so far, in the form of a CNF clause with each literal being a guessed $(v \mapsto v')$
assignment. Upon backtracking due to a decision to restart, we post a nogood of the form $(v \mapsto
v') \wedge (w \mapsto w') \wedge (x \mapsto x') \Rightarrow \bot$ for every branch to the left of
the current (incomplete) branch in the search tree, and when we first make a decision to restart
before backtracking, we post a similar nogood eliminating the entire subtree explored.

Finally, we use the two watched literals technique \cite{DBLP:conf/dac/MoskewiczMZZM01} to
propagate stored nogoods (\siplineref{2wl}). This has two benefits: the propagation complexity does
not particularly depend upon the number of stored nogoods, and it does not require any work upon
backtracking.  Other more sophisticated nogood generation and propagation schemes exist
\cite{DBLP:conf/aaai/LeeSZ16,DBLP:conf/cp/GlorianBLLM17}, but it is not clear whether these will be
beneficial in a setting where we are not maintaining arc consistency.

\subsection{The Effects of Restarts and Nogood Recording}

\begin{figure}[tb]
    \centering
    \includegraphics*{gen-graph-restarts.pdf}

    \caption{The cumulative number of satisfiable instances solved over time, with and without
    restarts and nogood recording.}
    \label{figure:restarts}
\end{figure}

\begin{figure*}[tb]
    \includegraphics*{gen-graph-scatter-random-goods.pdf}
    \hfill
    \includegraphics*{gen-graph-scatter-random.pdf}
    \hfill
    \includegraphics*{gen-graph-scatter-by-family.pdf}
    \caption{Comparing basic backtracking with the degree heuristic, versus the random and biased
    heuristics with restarts. The first plot does not use nogood recording, whilst the second and
    third do.}
    \label{figure:scatter-random}
\end{figure*}

In \cref{figure:restarts} we show the effects of adding restarts and nogood recording to the
algorithm. With restarts and nogood recording,
the random value-ordering heuristic comfortably beats the degree strategy with simple backtracking
search. In other
words, although having a good value-ordering heuristic is beneficial, introducing randomness into
the search is better, if it is done alongside a mechanism to avoid heavy commitment to any
particular random choice. \Cref{figure:scatter-random} looks at
this in more detail. On the first plot, without nogood recording, unsatisfiable instances
experience a slowdown of more than an order of magnitude, whilst on the second plot the
performance remains unchanged (which shows that the overheads of nogoods and restarting are
negligible). We therefore use nogood recording in all subsequent experiments with restarts. However,
although better in aggregate on satisfiable instances, on a case by case basis many instances
perform substantially worse with randomisation (either with or without restarts).

\Cref{figure:restarts} also shows that the biased heuristic together with restarts is better
still---that is, if we are introducing restarts, then it is better to add a small amount of
randomness to a tailored heuristic than it is to throw away the heuristic altogether.
Indeed, the original algorithm can solve 1,983 satisfiable instances by 993.0 seconds, whilst the
biased and random restarting algorithms require only 9.2 seconds and 33.8 seconds respectively to
solve the same number.

In the more detailed view in the final plot of \cref{figure:scatter-random}, comparing the
basic algorithm with the degree heuristic to the degree-biased algorithm with restarts, all of the
unsatisfiable instances are very close to the $x-y$ diagonal, showing that their performance is
nearly unchanged. On the other hand, there are large numbers of satisfiable instances well below the
diagonal line, indicating large speedups.  Better yet, there are only a handful of satisfiable
instances that are more than a factor of ten times worse. In other words, as well as improving
performance, we have made up most of the consistency we lost by introducing randomness. This is a
promising result: for the first time, we have introduced a viable alternative to backtracking search
in a setting where we both have a strong \emph{value}-ordering heuristic, \emph{and} care about both
satisfiable and unsatisfiable instances.

\section{Maximum Common Subgraph Algorithms}

Having looked at subgraph isomorphism in detail, we now briefly discuss the more general maximum
common induced subgraph problem.  Two recent algorithms for this problem also make use of
backtracking search with degree as a value-ordering heuristic. The k${\downarrow}$ algorithm
\cite{DBLP:conf/aaai/HoffmannMR17} attempts to solve the problem by first trying to solve the
induced subgraph isomorphism problem, and then if that fails, retries allowing a single unmatched
vertex (and thus using weaker invariants), and so on. Due to its similarity to \cref{algorithm:sip},
we can introduce the same bias and restart strategy, requiring only small changes
to handle wildcards in nogoods.

Meanwhile, the McSplit${\downarrow}$ algorithm
\cite{DBLP:conf/ijcai/McCreeshPT17} uses a constraint programming style search, but with special
propagators and backtrackable data structures that exploit special properties of the problem. The
unconventional domain store used by McSplit${\downarrow}$ precludes the use of arbitrary unit
propagation, and so when introducing restarts, we cannot propagate using nogoods.  Instead, we can
only detect when we are inside an already-visited branch.  We must therefore use the one watched
literal scheme instead, and we also introduce a basic subsumption scheme to prune redundant clauses.

Performance results from these two modified algorithms, using the same families of instances as in
the previous section, are shown in \cref{figure:mcs,figure:kdown,figure:mcsplit}. Although we have
moved from a decision problem to an optimisation problem, the same changes remain clearly
beneficial. For the $k{\downarrow}$ algorithm, the change has a minimal effect on many instances
(typically, where the $k = 0$ subproblem is unsatisfiable and hard, and the $k = 1$ subproblem is
satisfiable and easy), but gives large benefits on many more instances than it penalises: it is over
an order of magnitude better on nearly a hundred instances, whilst being an order of magnitude worse
on only seven.

\begin{figure}[p]
    \centering
    \includegraphics*{gen-graph-mcs.pdf}
    \caption{The cumulative number of maximum common induced subgraph instances solved by
    k${\downarrow}$ and McSplit${\downarrow}$ over time, with and without restarts and nogood
    recording.}\label{figure:mcs}
\end{figure}

\begin{figure}[p]
    \centering
    \includegraphics*{gen-graph-scatter-kdown.pdf}
    \caption{Comparing k${\downarrow}$ with and without restarts and nogood recording on an instance
    by instance basis.}\label{figure:kdown}
\end{figure}

\begin{figure}[p]
    \centering
    \includegraphics*{gen-graph-scatter-mcsplit.pdf}
    \caption{Comparing McSplit${\downarrow}$ with and without restarts and nogood recording on an instance
    by instance basis.}\label{figure:mcsplit}
\end{figure}


With McSplit${\downarrow}$, the inability to use two watched literals means that in many cases we
introduce a small slowdown. However, the overall pattern is the same: when introducing restarts and
a biased value ordering heuristic, it is much more common to see a large speedup than a large
slowdown.

\section{Conclusion and Future Work}

We have shown that it is both possible and beneficial to introduce a small amount of randomness into
backtracking search algorithms for hard subgraph problems, without compromising existing search
order heuristics, and without paying a penalty for unsatisfiable instances. In principle it is
well-known that introducing some form of randomness \cite{DBLP:conf/aaai/GomesSK98} or reduced
commitment \cite{DBLP:conf/ijcai/HarveyG95} into backtracking search can be beneficial, but until
now it has not been clear how to do so efficiently in practice, particularly when we already have
strong search ordering heuristics and when are expecting to encounter large numbers of unsatisfiable
instances. Our results show that the key to success is a combination of aggressive restarts, nogood
recording, and biasing an existing value-ordering heuristic with just a small amount of randomness.

The effectiveness of this approach raises a number of interesting questions and avenues for possible
future research. For example, will this same technique work for other problems where we have a good
value-ordering heuristic? Are there further benefits to be had from also modifying variable-ordering
heuristics? And how can such a search process be parallelised?

\section*{Acknowledgements}

% The authors would like to thank Ruth Hoffmann, Christophe Lecoutre, Christine Solnon and Craig
% Reilly for their comments.  This work was supported by the Engineering and Physical Sciences
% Research Council [grant numbers EP/026842/1 and EP/M508056/1].

The authors would like to thank Anonymous, Anonymous, Anonymous and Anonymous for their comments.
This work was supported by an anonymous funding agency.

\bibliographystyle{aaai}
\bibliography{dblp}

\end{document}

