% vim: set spell spelllang=en tw=100 et sw=4 sts=4 :

\def\year{2019}\relax
%File: formatting-instruction.tex
\documentclass[letterpaper]{article} %DO NOT CHANGE THIS
\usepackage{aaai18}  %Required
\usepackage{times}  %Required
\usepackage{helvet}  %Required
\usepackage{courier}  %Required
\usepackage{url}  %Required
\usepackage{graphicx}  %Required
\frenchspacing  %Required
\setlength{\pdfpagewidth}{8.5in}  %Required
\setlength{\pdfpageheight}{11in}  %Required
%PDF Info Is Required:
  \pdfinfo{
/Title (Value Ordering, Discrepancies, and Restarts for Subgraph Algorithms)
/Author (Anonymous and Anonymous and Anonymous)}
\setcounter{secnumdepth}{0}  

\usepackage{complexity}
\usepackage{amsmath}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{tikz}
\usepackage{cleveref}

\usepackage{showframe}

\newcommand{\neighbourhood}{\operatorname{N}}
\newcommand{\vertexset}{\operatorname{V}}

\newcommand{\siplabel}[1]{\label{line:sip:#1}}
\newcommand{\siplineref}[1]{line~\ref{line:sip:#1}}
\newcommand{\siplinerangeref}[2]{lines~\ref{line:sip:#1} to~\ref{line:sip:#2}}

\newcommand{\citet}[1]{\citeauthor{#1} \shortcite{#1}}

\usetikzlibrary{decorations, decorations.pathreplacing, decorations.pathmorphing,
    calc, backgrounds, positioning, tikzmark, patterns, fit}

\definecolor{highlight}{rgb}{0.819608, 0.733333, 0.34902}

\title{Value Ordering, Discrepancies, and Restarts for Subgraph Algorithms}
\author{Anonymous \and Anonymous \and Anonymous \\ email address \\ address}

\begin{document}

\maketitle

\begin{abstract}
    Modern subgraph isomorphism solvers use degree-based value-ordering heuristics to direct
    backtracking search. This kind of search makes a heavy commitment to the first branching choice,
    which is often incorrect. We evaluate discrepancy-based searches as an alternative, but find
    them too expensive to be effective in this situation, even on satisfiable instances. We then
    demonstrate that a random value-ordering heuristic with restarts beats conventional search with
    the best-known tailored heuristic. This motivates the introduction of a slightly-random
    heuristic with similar behaviour to the tailored heuristic: when combined with restarts and
    nogood recording, this new search strategy is over a hundred times more effective on satisfiable
    instances, and does not affect performance on unsatisfiable instances.  Finally, we show that
    this strategy also improves two maximum common induced subgraph algorithms.
\end{abstract}

\section{Introduction}

The subgraph isomorphism problem is to decide whether a copy of a small ``pattern'' graph inside a
larger ``target'' graph. Although \NP-complete, by combining design techniques from artificial
intelligence with careful algorithm engineering, modern subgraph isomorphism solvers can often
produce exact solutions quickly even on graphs with thousands of vertices. The current single
strongest subgraph isomorphism solver uses ``highest degree first'' as a value-ordering heuristic to
direct a constraint programming style search
\cite{DBLP:conf/cp/McCreeshP15,DBLP:conf/lion/KotthoffMS16,DBLP:conf/ijcai/McCreeshPT16}. This
heuristic is much better than branching randomly, but is still far from perfect.  We could try to
introduce tiebreaking mechanisms and similar refinements to strengthen the heuristic. However, given
the extreme cost of making an incorrect branching choice early in search, we believe it is more
fruitful to investigate alternatives to simple backtracking (depth-first) search. This paper
evaluates two such approaches: discrepancy search (which performs poorly), and a new form of search
(which is extremely beneficial) based around restarts and nogood recording. To explore different
parts of the search space when restarting, we introduce a novel value-ordering heuristic, which adds
a small amount of randomness to the ``highest degree first'' heuristic.  The effects of these
changes are shown in the final plot of \cref{figure:scatter-random}: we can make a state-of-the-art algorithm
perform much better on a large number of satisfiable instances, whilst performing worse only rarely
on satisfiable instances, and never on unsatisfiable instances.  Further, these properties are
\emph{not} shared by discrepancy searches (\cref{figure:scatter-dds}) or by random
value-ordering with restarts (the first two plots of \cref{figure:scatter-random}). Finally, we
show that this strategy is also effective in an optimisation setting, producing benefits in two
maximum common induced subgraph algorithms.

\section{Definitions and Algorithms}

Let $G$ be a graph. We denote its set of vertices by $\vertexset(G)$, and write $v \sim_G w$ to mean
that vertices $v$ and $w$ are adjacent; we allow a vertex to be adjacent to itself. The
\emph{neighbourhood} of a vertex, $\neighbourhood_G(v)$, is the set of vertices adjacent to $v$, and
the \emph{degree} of $v$, $\deg_G(v)$, is the cardinality of its neighbourhood. When the graph is clear from the context, we omit
the $G$ subscripts. As per \citet{DBLP:conf/aaai/HoffmannMR17}, $G^{n,\ell}$ denotes the graph with
vertex set $\vertexset(G)$, and with edges between vertices $v$ and $w$ if there are at least $n$
simple paths of length exactly $\ell$ between vertices $v$ and $w$ in $G$. Finally, we define a
function $\FuncSty{compatible}(v, w)$ to mean that mapping pattern vertex $v$ to target vertex $w$
is feasible based upon both loop constraints and the neighbourhood degree sequence relation of
\citet{DBLP:journals/constraints/ZampelliDS10}.

The non-induced subgraph isomorphism problem is to find an injective mapping from the vertices of a
pattern graph $\mathcal{P}$ to a target graph $\mathcal{T}$, such that adjacent vertices in
$\mathcal{P}$ are mapped to adjacent vertices in $\mathcal{T}$. We write $v \mapsto v'$ to mean that
pattern vertex $v$ is mapped to target $v'$ under such a mapping.

\subsection{A Subgraph Isomorphism Algorithm}

We now briefly outline the non-shaded parts of \cref{algorithm:sip}, which is how we will solve the
subgraph isomorphism problem. This algorithm is very closely based upon the $k{\downarrow}$
algorithm of \citet{DBLP:conf/aaai/HoffmannMR17} with $k = 0$, and we refer the reader to that paper
for full technical details; that algorithm, in turn, is a simplification of the Glasgow2 algorithm
\cite{DBLP:conf/cp/McCreeshP15,DBLP:conf/lion/KotthoffMS16}.

\Cref{algorithm:sip} works by recursively building up a valid mapping of pattern vertices to target
vertices. For each pattern vertex $v$, we have a domain $D_v$ containing a set of feasible target
vertices. Each domain is initialised at the top of search (\siplinerangeref{initstart}{initend}),
using the invariants described by \citet{DBLP:journals/constraints/ZampelliDS10} and
\citet{DBLP:conf/aaai/HoffmannMR17} to restrict its initial values.

The $\FuncSty{search}$ procedure recursively selects an unassigned domain $D$
(\siplineref{variableordering}), and tries assigning
it each of its remaining values in turn (\siplineref{valueordering}). The effects of this assignment
are propagated (\siplineref{dopropagate}). If an
inconsistency is detected, we continue and try another value; otherwise, we recurse
(\siplineref{recurse}) until a solution is found. Finally, if we run out of values, we backtrack
(\siplineref{backtrack}).

Our $\FuncSty{propagate}$ routine is also close to that of \citet{DBLP:conf/aaai/HoffmannMR17},
differing only in that we carry out the filtering for injectivity
(\siplinerangeref{alldiffstart}{alldiffend}) after every assignment rather than once at the end of
the unit propagation loop. (We make this change because the all-different propagator used is not
idempotent, and we wish to avoid an odd interaction when we introduce nogood filtering.)

\begin{algorithm}[p]\DontPrintSemicolon\small\SetInd{0.1em}{1em}
    \begin{tikzpicture}[remember picture,overlay]
        \coordinate (restart1sc) at ($(pic cs:restart1s) + (0, 0.15)$);
        \coordinate (restart1ec) at ($(pic cs:restart1e) + (0, 0.00)$);
        \node [fill=highlight, rounded corners, fit=(restart1sc) (restart1ec)] { };
        \coordinate (restart2sc) at ($(pic cs:restart2s) + (0, 0.15)$);
        \coordinate (restart2ec) at ($(pic cs:restart2e) + (0, 0.00)$);
        \node [fill=highlight, rounded corners, fit=(restart2sc) (restart2ec)] { };
        \coordinate (restart3sc) at ($(pic cs:restart3s) + (0, 0.15)$);
        \coordinate (restart3ec) at ($(pic cs:restart3e) + (0, 0.00)$);
        \node [fill=highlight, rounded corners, fit=(restart3sc) (restart3ec)] { };
        \coordinate (restart4sc) at ($(pic cs:restart4s) + (0, 0.15)$);
        \coordinate (restart4ec) at ($(pic cs:restart4e) + (0, 0.00)$);
        \node [fill=highlight, rounded corners, fit=(restart4sc) (restart4ec)] { };
        \coordinate (restart5sc) at ($(pic cs:restart5s) + (0, 0.15)$);
        \coordinate (restart5ec) at ($(pic cs:restart5e) - (0, 0.10)$);
        \coordinate (restart5rc) at ($(pic cs:restart5r) + (0, 0.00)$);
        \node [fill=highlight, rounded corners, fit=(restart5sc) (restart5ec) (restart5rc)] { };
        \coordinate (restart6sc) at ($(pic cs:restart6s) + (0, 0.15)$);
        \coordinate (restart6ec) at ($(pic cs:restart6e) + (0, 0.00)$);
        \coordinate (restart6rc) at ($(pic cs:restart6r) + (0.8, 0.00)$);
        \node [fill=highlight, rounded corners, fit=(restart6sc) (restart6ec) (restart6rc)] { };
        \coordinate (restart7sc) at ($(pic cs:restart7s) + (0, 0.15)$);
        \coordinate (restart7ec) at ($(pic cs:restart7e) + (0, 0.00)$);
        \node [fill=highlight, rounded corners, fit=(restart7sc) (restart7ec)] { };
        \coordinate (restart8sc) at ($(pic cs:restart8s) + (0.05, 0.15)$);
        \coordinate (restart8ec) at ($(pic cs:restart8e) + (0, 0.00)$);
        \node [fill=highlight, rounded corners, fit=(restart8sc) (restart8ec)] { };
        \coordinate (restart9sc) at ($(pic cs:restart9s) + (0, 0.15)$);
        \coordinate (restart9ec) at ($(pic cs:restart9e) + (-0.15, 0.00)$);
        \node [fill=highlight, rounded corners, fit=(restart9sc) (restart9ec)] { };
    \end{tikzpicture}

    \nl $\FuncSty{subgraphIsomorphism}$ (Graph $\mathcal{P}$, Graph $\mathcal{T}$) $\rightarrow$ Bool \;
\nl \Begin{
    \nl \lIf{$\left|\vertexset(\mathcal{P})\right| >
    \left|\vertexset(\mathcal{T})\right|$}{$\KwSty{return}~\KwSty{false}$\siplabel{enough}}
    \nl Discard isolated vertices in $\mathcal{P}$\siplabel{isolated} \;
    \nl \KwSty{global} $L \gets \big[ (\mathcal{P},\ \mathcal{T}), \siplabel{supplemental}
        (\mathcal{P}^{1,2}, \mathcal{T}^{1,2}),
        (\mathcal{P}^{2,2},\ \mathcal{T}^{2,2}),$ \\
        $\hspace*{3em}(\mathcal{P}^{3,2},\ \mathcal{T}^{3,2}),
        (\mathcal{P}^{4,2},\ \mathcal{T}^{4,2}) \big]$ \;
    \nl \ForEach{\siplabel{initstart}$v \in \vertexset(\mathcal{P})$}{
        \nl $D_v \gets \vertexset(\mathcal{T})$\;
        \nl \ForEach{$(P,\,T) \in L$}{\nl$D_v \gets \{ w \in D_v : \FuncSty{compatible}(v, w)\}$\siplabel{initend}
        }
    }
    \nl \While{$\KwSty{true}$\siplabel{restartsloop}}{
        \nl \lIf{$\KwSty{not}~\FuncSty{propagate}(D)$}{$\KwSty{return}~\KwSty{false}$}
        \nl \tikzmark{restart1s}$\KwSty{global}~(R, R') \gets (0, \textnormal{the next Luby sequence value})$\tikzmark{restart1e} \;
        \nl $S \gets \FuncSty{search}(\{ E \in D : \left|E\right| > 1 \}, \top)$ \;
        \nl \tikzmark{restart9s}\lIf{$S \ne \KwSty{restart}$}{\tikzmark{restart9e}$\KwSty{return}~S$}
        \nl \tikzmark{restart2s}trigger nogoods containing only one clause\tikzmark{restart2e} \;
    }
}
\medskip
    \nl $\FuncSty{search}$ (Domains $D$, Decisions $B$) $\rightarrow$ Bool
    \tikzmark{restart8s}\KwSty{or} \KwSty{restart}\tikzmark{restart8e} \;
\nl \Begin{
    \nl \lIf{$D = \emptyset$}{$\KwSty{return}~\KwSty{true}$}
    \nl $D_v \gets \textnormal{a domain from}~D~\textnormal{chosen by variable heuristic}$\siplabel{variableordering} \;
    \nl \tikzmark{restart3s}$C \gets \emptyset$\tikzmark{restart3e} \;
    \nl \ForEach {$v' \in D_v~\textnormal{ordered by value heuristic}$\siplabel{valueordering}}{
        \nl \tikzmark{restart4s}$C \gets C \cup \{ v' \}$\tikzmark{restart4e} \;
        \nl $D' \gets \FuncSty{clone}(D)$ \;
        \nl $D'_v \gets \{ v' \} $ \;
        \nl \If{\FuncSty{propagate}(D')\siplabel{dopropagate}}{
            \nl $S \gets \FuncSty{search}(\{ E \in D'\,{:}\,\left|E\right|{>}\,1 \}, B \wedge (v\,{\mapsto}\,v'))$\siplabel{recurse}\;
            \nl \lIf{$S = \KwSty{true}$}{$\KwSty{return}~\KwSty{true}$}
            \nl \tikzmark{restart5s}\ElseIf{$S = \KwSty{restart}$}{
                \nl \lForEach{$c \in C$}{$\textnormal{post nogood}~B \wedge (v \mapsto c) \Rightarrow \bot$\tikzmark{restart5r}}
                \nl $\KwSty{return}~\KwSty{restart}$\tikzmark{restart5e} \;
            }
        }
    }
    \nl \tikzmark{restart6s}\If{$(R \gets R + 1) = R'$\tikzmark{restart6r}}{
        \nl post nogood $B$ \;
        \nl $\KwSty{return}~\KwSty{restart}\tikzmark{restart6e}$\siplabel{countrestarts}}
    \nl $\KwSty{return}~\KwSty{false}$\siplabel{backtrack}
}
\medskip
\nl $\FuncSty{propagate}$ (Domains $D$) $\rightarrow$ Bool \;
\nl \Begin{
    \nl \While{$(D_v, v') \gets ($\textnormal{a unit domain from}~$D, \textnormal{the single value
    in that domain})$}{
        \nl \tikzmark{restart7s}trigger nogoods with watch $(v \mapsto v')$\tikzmark{restart7e}\siplabel{2wl} \;
        \nl \ForEach{$D_w \in D - D_v$}{
            \nl $D_w \gets D_w - v'$ \siplabel{removev} \;
            \nl \ForEach{$(P,\,T) \in L~\KwSty{where}~v \sim_P w$}{\nl$D_w \gets D_w \cap \neighbourhood_T(v')$}
            \nl \lIf{$D_w = \emptyset$}{$\KwSty{return}~\KwSty{false}$}
        }
\nl $(H,\,A,\,n) \gets (\emptyset,\,\emptyset,\,0)$\siplabel{alldiffstart} \;
\nl \ForEach{$D_v \in D$ \textnormal{in ascending cardinality order\siplabel{eachdomain}}}{
    \nl $(D_v, A,\,n) \gets (D_v \gets D_v \setminus H\siplabel{elimhall}, A \cup D_v,\,n + 1)$ \siplabel{acc} \;
    \nl \lIf{$D_v = \emptyset~\vee~|A| < n$}{$\KwSty{return}~\KwSty{false}$\siplabel{failhall}}
    \nl \lIf{$|A| = n$}{$(H,\,A,\,n) \gets (H \cup A,\,\emptyset,\,0)$\siplabel{hall}}\siplabel{alldiffend}
}
    }
\nl $\KwSty{return}~\KwSty{true}$ \;
}
\caption{Subgraph isomorphism. The shaded code is added for restarting.}
\label{algorithm:sip}
\end{algorithm}

For the variable-ordering heuristic on \siplineref{variableordering} of
\cref{algorithm:sip}, we use smallest domain first, tiebreaking on highest degree
\cite{DBLP:journals/ai/HaralickE80,DBLP:journals/jair/McCreeshPST18}. Variable ordering is not the
focus of this paper---as far as possible, we use this same dynamic ordering throughout the paper, so
as not to affect the performance of the algorithm on unsatisfiable instances.  For value ordering,
which occurs on \siplineref{valueordering} of \cref{algorithm:sip}, we will initially select
vertices from highest degree to lowest degree; we return to value ordering in the second half of
this paper. Note that the variable-ordering heuristic affects both unsatisfiable and satisfiable
instances, whilst the value-ordering heuristic is relevant only for satisfiable instances.

\section{Empirical Evaluation}

Our experiments are performed on systems with dual Intel Xeon E5-2697A v4 CPUs and 512GBytes RAM,
running Ubuntu 17.04. We
implemented\footnote{DOI removed for anonymous review}
\cref{algorithm:sip} in C++ using the bit-parallel implementation of
\citet{DBLP:conf/aaai/HoffmannMR17} as a starting point, and compiled it using GCC 6.3.0. We use a
deterministic pseudo-random number generator for reproducibility.

We use the dataset introduced by \citet{DBLP:conf/lion/KotthoffMS16} for evaluation. This dataset
brings together a range of randomly-generated and application instance families from earlier papers:

\begin{description}
    \item[BVG(r), M4D(r), and Rand] are randomly generated graphs using different models (bounded
        degree, regular mesh, and uniform), where each pattern is a permuted random connected subgraph
        of the target (and so each instance is satisfiable) \cite{DBLP:journals/pami/CordellaFSV04}.
        These benchmark instances are widely used, but have unusual properties and so broad
        conclusions should not be drawn based solely upon behaviour on these instances
        \cite{DBLP:journals/jair/McCreeshPST18}.
    \item[SF] contains randomly generated scale-free graphs using a similar method
        \cite{DBLP:journals/constraints/ZampelliDS10}.
   \item[LV] consists of various kinds of graph gathered by Larrosa and Valiente
       \cite{DBLP:journals/mscs/LarrosaV02} from the Stanford Graph Database. We include both the
        50 small graphs, and the 50 larger graphs.
    \item[Phase] contains hard crafted instances that lie near the satisfiable / unsatisfiable phase
        transition \cite{DBLP:journals/jair/McCreeshPST18}.
    \item[PR] contains graphs generated from segmented images, corresponding to a computer vision
        problem \cite{DBLP:journals/pr/SolnonDHJ15}.
    \item[Images and Meshes] contain graphs representing 2D segmented images and 3D object models
        respectively, again representing a computer vision problem
        \cite{DBLP:journals/cviu/DamiandSHJS11}.
\end{description}

Other studies use a random selection of 200 of each of the instances from the ``meshes'' and
``images'' families because some earlier solvers find many of these instances extremely hard. We
would like to have a larger number of satisfiable instances in our test set, and so we include all
pattern / target pairs. This gives a total of 14,621 instances (rather than the original 5,725). At
least \input{gen-n-sat.tex}\unskip\ of these instances are known to be satisfiable, and at least
\input{gen-n-unsat.tex}\unskip\ are unsatisfiable.

\subsection{Baseline Performance}

\begin{figure}[tb]
    \centering
    \includegraphics*{gen-graph-others.pdf}

    \bigskip

    \centering
    \includegraphics*{gen-graph-others-zoom.pdf}

    \caption{Above, the cumulative number of instances solved over time, comparing our algorithm
    (both in its basic form, and with the improvements introduced in the remainder of the paper) to
    other solvers. Below, the same, looking only at satisfiable instances.}
    \label{figure:others}
\end{figure}

We begin by verifying that our implementation of the basic \cref{algorithm:sip} is competitive. In
\cref{figure:others} we show the cumulative number of instances solved over time, using our
implementation (both with the degree heuristic, and with the modifications described in the
remainder of this paper), the Glasgow2 and Glasgow3 algorithms from which \cref{algorithm:sip} is derived
\cite{DBLP:conf/cp/McCreeshP15,DBLP:conf/lion/KotthoffMS16}, the PathLAD variation of the LAD
algorithm \cite{DBLP:journals/ai/Solnon10,DBLP:conf/lion/KotthoffMS16}, and VF2
\cite{DBLP:journals/pami/CordellaFSV04}, in each case using the original implementation provided
by the algorithm's authors. For each algorithm, the $y$ value
gives the cumulative number of instances which (individually) can be solved in no more than $x$
milliseconds.  The vertical distance between two lines therefore shows how many more instances can
be solved by one solver than another, if every instance is run separately with the chosen $x$
timeout. The horizontal distance shows how many times longer the per-instance timeout would need to
be to allow the rightmost algorithm to succeed on $y$ out of the 14,621 instances (bearing in mind
that the two sets of $y$ instances could be different).

As expected, the performance of \cref{algorithm:sip} with the degree heuristic is very close to that
of Glasgow2, although we perform better at lower runtimes because we use simpler preprocessing at
the top of search. Our implementation also clearly beats PathLAD and VF2, except for very low
choices of timeout.

The dataset includes many instances which are extremely easy for a good solver, and so it can be
hard to see the differences between the stronger solvers at higher runtimes. This paper focusses
upon improving the performance on the remaining hard satisfiable instances, and so in the second
plot in \cref{figure:others} (and in subsequent cumulative plots) we show only satisfiable
instances, and use a reduced range on both axes.

\subsection{Value-Ordering Heuristics}

\Cref{figure:value-ordering-heuristics} shows the cumulative number of satisfiable instances
solved over time using different value-ordering heuristics:

\begin{description}
\item[Degree] We select vertices from highest degree to lowest degree
\cite{DBLP:conf/ijcai/McCreeshPT16}.

\item[Random] We select vertices uniformly at random.

\item[Anti] We select vertices from lowest degree to highest degree. This heuristic is used as
a sanity check: as expected, degree beats random, and random beats anti.

\item[Biased] We branch by selecting a vertex $v'$ from the chosen domain $D_v$ with
probability \[ p(v') = \frac{2^{\deg(v')}}{\sum_{w \in D_v}{2^{\deg(w)}}} \text{.} \]\end{description}

The biased heuristic continues to prefer vertices of high degree, but will give the same chance of
being selected next to two vertices of equal degree.  It also introduces an element of randomness,
which is needed to make restarts have an effect.  We note that it resembles the well-known
\emph{softmax} weighting scheme applied to the degree heuristic, although usually \emph{softmax}
uses a base of $e$ rather than $2$.  Experiments showed that in this context, the choice of base is
not important to the search space size; however, a base of $2$ avoids the need to use floating point
arithmetic, which was a substantial overhead.

The performance of the degree and biased heuristics are very similar: the
biased heuristic has slightly worse performance below a 400s timeout, and effectively equal
performance above it. This shows that we can introduce an element of randomness into the degree
value-ordering heuristic without adversely affecting its performance \emph{in aggregate}.
\Cref{figure:scatter-heuristics} gives a more detailed look at these results. This scatter plot
shows that despite the aggregate performance being very similar, on a case by case basis, the two
heuristics can make a large difference to the performance of satisfiable instances.

\begin{figure}[p]
    \centering
    \includegraphics*{gen-graph-value-ordering-heuristics.pdf}

    \caption{The cumulative number of satisfiable instances solved over time, using four
    different value-ordering heuristics.}
    \label{figure:value-ordering-heuristics}
\end{figure}

\begin{figure}[p]
    \centering
    \includegraphics*{gen-graph-scatter-heuristics.pdf}

    \caption{An instance by instance comparison of the degree and biased heuristics on all
    instances. Points on the outer axes are timeouts, and point style shows instance family.}
    \label{figure:scatter-heuristics}
\end{figure}

\begin{figure}[p]
    \centering
    \includegraphics*{gen-graph-scatter-dds.pdf}

    \caption{Comparing depth-bounded discrepancy search to backtracking search, both with the degree
    heuristic.}\label{figure:scatter-dds}
\end{figure}

\subsection{Discrepancy Searches}

A \emph{discrepancy} is where search goes against a value-ordering heuristic.  Discrepancy searches
\cite{DBLP:conf/ijcai/HarveyG95,DBLP:conf/aaai/Korf96,DBLP:conf/ijcai/Walsh97} are alternatives to
backtracking search that initially search disallowing all discrepancies, and then retry search
allowing an increasing number of discrepancies at each iteration until either a solution is found or
unsatisfiability is proven. These schemes assume that value-ordering heuristics are usually
reliable, and that most solutions can be found with only a small number of discrepancies; in such
cases, the heavy commitment to early branching choices made by conventional backtracking search can
be extremely costly.

\Cref{figure:scatter-dds} shows the effects of adding Walsh's \cite{DBLP:conf/ijcai/Walsh97}
depth-bounded discrepancy search (DDS) to \cref{algorithm:sip}. Each point represents the solving
time for one instance---to avoid noise for easier instances, we measure only time spent during
search, and exclude time spent in preprocessing and initialisation.  Points below the $x-y$ diagonal
are speedups, whilst points on the top and right axes represent instances which timed out after one
thousand seconds with one algorithm, but not the other. For satisfiable instances, the different
point styles show the different families, whilst all unsatisfiable instances are shown as dark dots.

The points well below the diagonal line and along the right-hand axis show that DDS can sometimes be
extremely beneficial on satisfiable instances.  However, on both unsatisfiable and most satisfiable
instances, the overheads can be extremely large, and DDS is much worse in aggregate (even when only
considering satisfiable instances).

These large overheads are to be expected: discrepancy searches are aimed primarily at getting better
feasible solutions in optimisation problems which are too large for a proof of optimality to be a
realistic prospect, and they are not well-suited for unsatisfiable decision problems. However, the
extremely large gains on some satisfiable instances confirm our suspicions that we should find an
alternative to heavy-commitment backtracking search.

\subsection{Adding Restarts and Nogood Recording}

Another alternative to plain backtracking search is provided by \emph{restarts}. The general idea is
to perform a certain amount of search, and then if no solution has been found (and unsatisfiability
has not been proven), to abandon search and restart from the beginning. Obviously, such an approach
can only be beneficial if something changes after restarting---in a constraint programming setting,
this is usually the variable-ordering heuristic
\cite{DBLP:journals/jsat/LecoutreSTV07,DBLP:conf/cp/GayHLS15,DBLP:conf/aaai/LeeSZ16,DBLP:conf/cp/GlorianBLLM17}.
In this paper, we instead rely upon randomness in the \emph{value}-ordering heuristic, and continue
to use smallest domain first with static tiebreaking for variable-ordering.\footnote{It may be
possible to further improve \cref{algorithm:sip} by also introducing randomness or some form of
learning into its variable-ordering heuristic. However, simultaneously introducing a second change
would considerably complicate the empirical analysis.} Using restarts on value-ordering heuristics
appears to be uncommon---although \citet{DBLP:conf/flairs/RazgonOP07} look at learning
value-ordering heuristics from restarts, \citet{DBLP:conf/cp/ChuSS09} use a similar
scheme in the context of parallel search, and an early approach by \citet{DBLP:conf/aaai/GomesSK98} does so in an optimisation context.

The shaded code shows how to introduce restarts to \cref{algorithm:sip}. The recursive
$\FuncSty{search}$ procedure is modified to return either true (if the instance is satisfiable),
false (unsatisfiable), or a third value if some threshold was reached without the solution being
known.  The main function then calls $\FuncSty{search}$ in a loop (\siplineref{restartsloop}), until
a solution is found.

We use the Luby scheme \cite{DBLP:journals/ipl/LubySZ93} to determine when to restart. We count the
number of backtracks (that is, when we reach the end of the main $\KwSty{foreach}$ loop on
\siplineref{countrestarts}) to decide when to restart, rather than counting recursive calls.
Following established wisdom, we multiply each item in the Luby sequence by a magic constant---we
used the SMAC automatic parameter tuner \cite{DBLP:conf/lion/HutterHL11} to select the value 660.
(We also tried a geometric restart scheme, which gave less favourable results.)

The remainder of the changes are to introduce nogoods. To avoid exploring portions of the search
space that we have already visitied, every time we restart, we add new constraints to the problem
which eliminate already-explored subtrees---such a constraint is called a nogood. We generate simple
decision nogoods, as follows. The $B$ argument to $\FuncSty{search}$ tracks the branching choices
made so far, in the form of a CNF clause with each literal being a guessed $(v \mapsto v')$
assignment. Upon backtracking due to a decision to restart, we post a nogood of the form $(v \mapsto
v') \wedge (w \mapsto w') \wedge (x \mapsto x') \Rightarrow \bot$ for every branch to the left of
the current (incomplete) branch in the search tree, and when we first make a decision to restart
before backtracking, we post a similar nogood eliminating the entire subtree explored.

Finally, we use the two watched literals technique \cite{DBLP:conf/dac/MoskewiczMZZM01} to
propagate stored nogoods (\siplineref{2wl}). This has two benefits: the propagation complexity does
not particularly depend upon the number of stored nogoods, and it does not require any work upon
backtracking.  Other more sophisticated nogood generation and propagation schemes exist
\cite{DBLP:conf/aaai/LeeSZ16,DBLP:conf/cp/GlorianBLLM17}, but it is not clear whether these will be
beneficial in a setting where we are not maintaining arc consistency.

\subsection{The Effects of Restarts and Nogood Recording}

\begin{figure*}[tb]
    \includegraphics*{gen-graph-scatter-random-goods.pdf}
    \hfill
    \includegraphics*{gen-graph-scatter-random.pdf}
    \hfill
    \includegraphics*{gen-graph-scatter-by-family.pdf}
    \caption{Comparing basic backtracking with the degree heuristic, versus the random and biased
    heuristics with restarts. The first plot does not use nogood recording, whilst the second and
    third do.}
    \label{figure:scatter-random}
\end{figure*}

\begin{figure}[tb]
    \centering
    \includegraphics*{gen-graph-restarts.pdf}

    \caption{The cumulative number of satisfiable instances solved over time, with and without
    restarts and nogood recording.}
    \label{figure:restarts}
\end{figure}

In \cref{figure:restarts} we show the effects of adding restarts and nogood recording to the
algorithm. With restarts and nogood recording,
the random value-ordering heuristic comfortably beats the degree strategy with simple backtracking
search. In other
words, although having a good value-ordering heuristic is beneficial, introducing randomness into
the search is better, if it is done alongside a mechanism to avoid heavy commitment to any
particular random choice. \Cref{figure:scatter-random} looks at
this in more detail. On the first plot, without nogood recording, unsatisfiable instances
experience a slowdown of more than an order of magnitude, whilst on the second plot the
performance remains unchanged (which shows that the overheads of nogoods and restarting are
negligible). We therefore use nogood recording in all subsequent experiments with restarts. However,
although better in aggregate on satisfiable instances, on a case by case basis many instances
perform substantially worse with randomisation (either with or without restarts).

\Cref{figure:restarts} also shows that the biased heuristic together with restarts is better
still---that is, if we are introducing restarts, then it is better to add a small amount of
randomness to a tailored deterministic heuristic than it is to throw away the heuristic altogether.
Indeed, the original algorithm can solve 1,983 satisfiable instances by 993.0 seconds, whilst the
biased and random restarting algorithms require only 9.2 seconds and 33.8 seconds respectively to
solve the same number.

In the more detailed view in the final plot of \cref{figure:scatter-random}, comparing the
basic algorithm with the degree heuristic to the degree-biased algorithm with restarts, all of the
unsatisfiable instances are very close to the $x-y$ diagonal, showing that their performance is
nearly unchanged. On the other hand, there are large numbers of satisfiable instances well below the
diagonal line, indicating large speedups.  Better yet, there are only a handful of satisfiable
instances that are more than a factor of ten times worse. In other words, as well as improving
performance, we have made up most of the consistency we lost by introducing randomness.

\section{Maximum Common Subgraph Algorithms}

Having looked at subgraph isomorphism in detail, we now briefly discuss the more general maximum
common induced subgraph problem.  Two recent algorithms for this problem also make use of
backtracking search with degree as a value-ordering heuristic. The k${\downarrow}$ algorithm
\cite{DBLP:conf/aaai/HoffmannMR17} attempts to solve the problem by first trying to solve the
induced subgraph isomorphism problem, and then if that fails, retries allowing a single unmatched
vertex (and thus using weaker invariants), and so on. Due to its similarity to \cref{algorithm:sip},
we can introduce the same bias and restart strategy, requiring only small changes
to handle wildcards in nogoods.

Meanwhile, the McSplit${\downarrow}$ algorithm
\cite{DBLP:conf/ijcai/McCreeshPT17} uses a constraint programming style search, but with special
propagators and backtrackable data structures that exploit special properties of the problem. The
unconventional domain store used by McSplit${\downarrow}$ precludes the use of arbitrary unit
propagation, and so when introducing restarts, we cannot propagate using nogoods.  Instead, we can
only detect when we are inside an already-visited branch.  We must therefore use the one watched
literal scheme instead, and we also introduce a basic subsumption scheme to prune redundant clauses.

Performance results from these two modified algorithms, using the same families of instances as in
the previous section, are shown in \cref{figure:mcs,figure:kdown,figure:mcsplit}. Although we have
moved from a decision problem to an optimisation problem, the same changes remain clearly
beneficial. For the $k{\downarrow}$ algorithm, the change has a minimal effect on many instances
(typically, where the $k = 0$ subproblem is unsatisfiable and hard, and the $k = 1$ subproblem is
satisfiable and easy), but gives large benefits on many more instances than it penalises: it is over
an order of magnitude better on nearly a hundred instances, whilst being an order of magnitude worse
on only seven.

\begin{figure}[p]
    \centering
    \includegraphics*{gen-graph-mcs.pdf}
    \caption{The cumulative number of maximum common induced subgraph instances solved by
    k${\downarrow}$ and McSplit${\downarrow}$ over time, with and without restarts and nogood
    recording.}\label{figure:mcs}
\end{figure}

\begin{figure}[p]
    \centering
    \includegraphics*{gen-graph-scatter-kdown.pdf}
    \caption{Comparing k${\downarrow}$ with and without restarts and nogood recording on an instance
    by instance basis.}\label{figure:kdown}
\end{figure}

\begin{figure}[p]
    \centering
    \includegraphics*{gen-graph-scatter-mcsplit.pdf}
    \caption{Comparing McSplit${\downarrow}$ with and without restarts and nogood recording on an instance
    by instance basis.}\label{figure:mcsplit}
\end{figure}


With McSplit${\downarrow}$, the inability to use two watched literals means that in many cases we
introduce a small slowdown. However, the overall pattern is the same: when introducing restarts and
a biased value ordering heuristic, it is much more common to see a large speedup than a large
slowdown.

\section{Conclusion and Future Work}

We have shown that it is both possible and beneficial to introduce a small amount of randomness into
backtracking search algorithms for hard subgraph problems, without compromising existing search
order heuristics, and without paying a penalty for unsatisfiable instances. In principle it is
well-known that introducing some form of randomness \cite{DBLP:conf/aaai/GomesSK98} or reduced
commitment \cite{DBLP:conf/ijcai/HarveyG95} into backtracking search can be beneficial, but until
now it has not been clear how to do so efficiently in practice, particularly when we already have
strong search ordering heuristics and when are expecting to encounter large numbers of unsatisfiable
instances. Our results show that the key to success is a combination of aggressive restarts, nogood
recording, and biasing an existing value-ordering heuristic with just a small amount of randomness.

The effectiveness of this approach raises a number of interesting questions and avenues for possible
future research. For example, will this same technique work for other problems where we have a good
value-ordering heuristic? Are there further benefits to be had from also modifying variable-ordering
heuristics? Are there theoretical rather than just empirical reasons for using the
\emph{softmax}-like bias scheme? And how can such a search process be parallelised?

\section*{Acknowledgements}

% The authors would like to thank Ruth Hoffmann, Christophe Lecoutre, Christine Solnon and Craig
% Reilly for their comments.  This work was supported by the Engineering and Physical Sciences
% Research Council [grant numbers EP/026842/1 and EP/M508056/1].

The authors would like to thank Anonymous, Anonymous, Anonymous and Anonymous for their comments.
This work was supported by an anonymous funding agency.

\bibliographystyle{aaai}
\bibliography{dblp}

\end{document}

