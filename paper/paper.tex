% vim: set spell spelllang=en tw=100 et sw=4 sts=4 :

\documentclass{article}
\usepackage{ijcai17}
\usepackage{times}
\usepackage{microtype}
\usepackage{amsmath}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{tikz}

\usepackage{cleveref}

% \usepackage{showframe}

\newcommand{\neighbourhood}{\operatorname{N}}
\newcommand{\vertexset}{\operatorname{V}}
\newcommand{\nds}{\operatorname{S}}

\newcommand{\citet}[1]{\citeauthor{#1} \shortcite{#1}}
\newcommand{\citep}[1]{\cite{#1}}
\newcommand{\citepos}[1]{\citeauthor{#1}'s \shortcite{#1}}

\newcommand{\siplabel}[1]{\label{line:sip:#1}}
\newcommand{\siplineref}[1]{line~\ref{line:sip:#1}}
\newcommand{\siplinerangeref}[2]{lines~\ref{line:sip:#1} to~\ref{line:sip:#2}}

\usetikzlibrary{decorations, decorations.pathreplacing, decorations.pathmorphing,
    calc, backgrounds, positioning, tikzmark, patterns, fit}

\definecolor{highlight}{rgb}{0.819608, 0.733333, 0.34902}

% cref style
\crefname{algorithm}{Algorithm}{Algorithms}
\Crefname{algorithm}{Algorithm}{Algorithms}
\crefname{figure}{Figure}{Figures}
\Crefname{figure}{Figure}{Figures}
\crefname{table}{Table}{Tables}
\Crefname{table}{Table}{Tables}
\crefname{section}{Section}{Sections}
\Crefname{section}{Section}{Sections}

\title{Value Ordering, Discrepancies, and Restarts for Subgraph Algorithms\thanks{This work was
supported by the Engineering and Physical Sciences Research Council [grant numbers EP/026842/1 and EP/M508056/1]}}
\author{Ciaran McCreesh \and Patrick Prosser \and James Trimble\\ University of Glasgow, Glasgow, Scotland \\
    ciaran.mccreesh@glasgow.ac.uk}

\begin{document}

\maketitle

\begin{abstract}
    Modern subgraph isomorphism solvers use degree-based heuristics to direct a constraint
    programming style search. We show that discrepancy-based searches are too expensive to be
    effective in offsetting mistakes made by value-ordering heuristic in this setting.  We then
    demonstrate that a random heuristic with restarts beats conventional search with the best-known
    tailored heuristic. This motivates the introduction of a slightly-random heuristic with similar
    behaviour to the tailored heuristic: when combined with restarts, this new search strategy is
    over a hundred times more effective on satisfiable instances, and does not affect performance on
    unsatisfiable instances. Finally, we show that the same strategy also improves two maximum
    common induced subgraph algorithms.
\end{abstract}

\section{Introduction}

The subgraph isomorphism problem is to find a copy of a small ``pattern'' graph inside a larger
``target'' graph.  The current single strongest subgraph isomorphism solver uses ``highest degree
first'' as a value-ordering heuristic to direct a constraint programming style search
\citep{DBLP:conf/cp/McCreeshP15,DBLP:conf/lion/KotthoffMS16,DBLP:conf/ijcai/McCreeshPT16}. This
heuristic is much better than branching randomly, but is still far from perfect.  We could try to
introduce tiebreaking mechanisms and similar refinements to strengthen the heuristic. However, given
the extreme cost of making an incorrect branching choice early on in search, we believe it is more
fruitful to investigate alternatives to simple backtracking (depth-first) search. This paper
evaluates two such approaches: discrepancy search (which performs poorly), and restarts (which are
extremely beneficial). To explore different parts of the search space when restarting, we introduce
a new value-ordering heuristic, which adds a small amount of randomness to the ``highest degree
first'' heuristic.  The effects of these changes are shown in
\cref{figure:scatter-by-family}: we can make a state-of-the-art algorithm perform much better on a
large number of satisfiable instances, whilst performing worse only rarely on satisfiable instances,
and never on unsatisfiable instances.  Further, these properties are \emph{not} shared by
discrepancy searches (\cref{figure:scatter-dds}) or by random value-ordering with restarts
(\cref{figure:scatter-random}). Finally, we show that this strategy is also effective in an
optimisation setting, producing benefits in two maximum common induced subgraph algorithms.

\section{Definitions and Algorithms}

Let $G$ be a graph. We denote its set of vertices by $\vertexset(G)$, and write $v \sim_G w$ to mean
that vertices $v$ and $w$ are adjacent; we allow a vertex to be adjacent to itself. The
\emph{neighbourhood} of a vertex, $\neighbourhood_G(v)$, is the set of vertices adjacent to $v$, and
the \emph{degree} of $v$, $\deg_G(v)$, is the cardinality of its neighbourhood; when the graph is
clear from the context, we omit the $G$ subscript. The \emph{neighbourhood degree sequence} of $v$,
$\nds_G(v)$, is the sequence of degrees of the neighbours of $v$ in descending order, and we denote
by $\preceq$ the inclusion relation of \citet{DBLP:journals/constraints/ZampelliDS10}. As per
\citet{DBLP:conf/aaai/HoffmannMR17}, $G^{n,\ell}$ denotes the graph with vertex set $\vertexset(G)$,
and with edges between vertices $v$ and $v$ if there are at least $n$ simple paths of length exactly
$\ell$ between vertices $v$ and $w$ in $G$.

The non-induced subgraph isomorphism problem is to find an injective mapping from the vertices of a
pattern graph $\mathcal{P}$ to a target graph $\mathcal{T}$, such that adjacent vertices in
$\mathcal{P}$ are mapped to adjacent vertices in $\mathcal{T}$. We write $v \mapsto v'$ to mean that
pattern vertex $v$ is mapped to target $v'$ under such a mapping.

\subsection{A Subgraph Isomorphism Algorithm}

We now briefly outline the non-shaded parts of \cref{algorithm:sip}, which is how we will solve the
subgraph isomorphism problem. This algorithm is very closely based upon the $k{\downarrow}$
algorithm of \citet{DBLP:conf/aaai/HoffmannMR17} with $k = 0$, and we refer the reader to that paper
for full technical details; that algorithm, in turn, is a simplification of the Glasgow2 algorithm
\citep{DBLP:conf/cp/McCreeshP15,DBLP:conf/lion/KotthoffMS16}.

\Cref{algorithm:sip} works by recursively building up a valid mapping of pattern vertices to target
vertices. For each pattern vertex $v$, we have a domain $D_v$ containing a set of feasible target
vertices. Each domain is initialised at the top of search (\siplinerangeref{initstart}{initend}),
using the invariants described by \citet{DBLP:conf/aaai/HoffmannMR17} to restrict its initial
values.

The $\FuncSty{search}$ procedure recursively selects an unassigned domain $D$
(\siplineref{variableordering}), and tries assigning
it each of its remaining value in turn (\siplineref{valueordering}). The effects of this assignment
are propagated (\siplineref{dopropagate}). If an
inconsistency is detected, we continue and try another value; otherwise, we recurse
(\siplineref{recurse}) until a solution is found. Finally, if we run out of values, we backtrack
(\siplineref{backtrack}).

Our $\FuncSty{propagate}$ routine is similarly close to that of \citet{DBLP:conf/aaai/HoffmannMR17},
differing only in that we carry out the filtering for injectivity
(\siplinerangeref{alldiffstart}{alldiffend}) after every assignment rather than once at the end of
the unit propagation loop. (We make this change because the all-different propagator used is not
idempotent, and we wish to avoid an odd interaction when we introduce nogood filtering.)

\begin{algorithm}[p]\DontPrintSemicolon\small\SetInd{0.1em}{1em}
    \begin{tikzpicture}[remember picture,overlay]
        \coordinate (restart1sc) at ($(pic cs:restart1s) + (0, 0.15)$);
        \coordinate (restart1ec) at ($(pic cs:restart1e) + (0, 0.00)$);
        \node [fill=highlight, rounded corners, fit=(restart1sc) (restart1ec)] { };
        \coordinate (restart2sc) at ($(pic cs:restart2s) + (0, 0.15)$);
        \coordinate (restart2ec) at ($(pic cs:restart2e) + (0, 0.00)$);
        \node [fill=highlight, rounded corners, fit=(restart2sc) (restart2ec)] { };
        \coordinate (restart3sc) at ($(pic cs:restart3s) + (0, 0.15)$);
        \coordinate (restart3ec) at ($(pic cs:restart3e) + (0, 0.00)$);
        \node [fill=highlight, rounded corners, fit=(restart3sc) (restart3ec)] { };
        \coordinate (restart4sc) at ($(pic cs:restart4s) + (0, 0.15)$);
        \coordinate (restart4ec) at ($(pic cs:restart4e) + (0, 0.00)$);
        \node [fill=highlight, rounded corners, fit=(restart4sc) (restart4ec)] { };
        \coordinate (restart5sc) at ($(pic cs:restart5s) + (0, 0.15)$);
        \coordinate (restart5ec) at ($(pic cs:restart5e) - (0, 0.10)$);
        \coordinate (restart5rc) at ($(pic cs:restart5r) + (0, 0.00)$);
        \node [fill=highlight, rounded corners, fit=(restart5sc) (restart5ec) (restart5rc)] { };
        \coordinate (restart6sc) at ($(pic cs:restart6s) + (0, 0.15)$);
        \coordinate (restart6ec) at ($(pic cs:restart6e) + (0, 0.00)$);
        \node [fill=highlight, rounded corners, fit=(restart6sc) (restart6ec)] { };
        \coordinate (restart7sc) at ($(pic cs:restart7s) + (0, 0.15)$);
        \coordinate (restart7ec) at ($(pic cs:restart7e) + (0, 0.00)$);
        \node [fill=highlight, rounded corners, fit=(restart7sc) (restart7ec)] { };
        \coordinate (restart8sc) at ($(pic cs:restart8s) + (0.05, 0.15)$);
        \coordinate (restart8ec) at ($(pic cs:restart8e) + (0, 0.00)$);
        \node [fill=highlight, rounded corners, fit=(restart8sc) (restart8ec)] { };
        \coordinate (restart9sc) at ($(pic cs:restart9s) + (0, 0.15)$);
        \coordinate (restart9ec) at ($(pic cs:restart9e) + (-0.15, 0.00)$);
        \node [fill=highlight, rounded corners, fit=(restart9sc) (restart9ec)] { };
    \end{tikzpicture}

    \nl $\FuncSty{subgraphIsomorphism}$ (Graph $\mathcal{P}$, Graph $\mathcal{T}$) $\rightarrow$ Bool \;
\nl \Begin{
    \nl \lIf{$\left|\vertexset(\mathcal{P})\right| >
    \left|\vertexset(\mathcal{T})\right|$}{$\KwSty{return}~\KwSty{false}$\siplabel{enough}}
    \nl Discard isolated vertices in $\mathcal{P}$\siplabel{isolated} \;
    \nl \KwSty{global} $L \gets \big[ (\mathcal{P},\ \mathcal{T}), \siplabel{supplemental}
        (\mathcal{P}^{1,2}, \mathcal{T}^{1,2}),
        (\mathcal{P}^{2,2},\ \mathcal{T}^{2,2}), $\\\hspace*{6em}$ (\mathcal{P}^{3,2},\ \mathcal{T}^{3,2}),
        (\mathcal{P}^{4,2},\ \mathcal{T}^{4,2}) \big]$ \;
    \nl \ForEach{\siplabel{initstart}$v \in \vertexset(\mathcal{P})$}{
        \nl $D_v \gets \vertexset(\mathcal{T})$\;
        \nl \ForEach{$(P,\,T) \in L$}{
            \nl $D_v \gets \{ w \in D_v : $ \siplabel{degreefiltering} \\\hspace*{4em}$v \sim_P v \Rightarrow w \sim_T w~\wedge$
            \\\hspace*{4em}$\nds_P(v) \preceq \nds_T(w)\}$\siplabel{initend} \;
        }
    }
    \nl \While{$\KwSty{true}$\siplabel{restartsloop}}{
        \nl \lIf{$\KwSty{not}~\FuncSty{propagate}(D)$}{$\KwSty{return}~\KwSty{false}$}
        \nl \tikzmark{restart1s}$\KwSty{global}~(R, R') \gets (0, \textnormal{the next Luby sequence value})$\tikzmark{restart1e} \;
        \nl $S \gets \FuncSty{search}(\{ E \in D : \left|E\right| > 1 \}, \top)$ \;
        \nl \tikzmark{restart9s}\lIf{$S \ne \KwSty{restart}$}{\tikzmark{restart9e}$\KwSty{return}~S$}
        \nl \tikzmark{restart2s}trigger nogoods containing only one clause\tikzmark{restart2e} \;
    }
}
\medskip
    \nl $\FuncSty{search}$ (Domains $D$, Decisions $B$) $\rightarrow$ Bool
    \tikzmark{restart8s}\KwSty{or} \KwSty{restart}\tikzmark{restart8e} \;
\nl \Begin{
    \nl \lIf{$D = \emptyset$}{$\KwSty{return}~\KwSty{true}$}
    \nl $D_v \gets \textnormal{a domain from}~D~\textnormal{chosen by variable heuristic}$\siplabel{variableordering} \;
    \nl \tikzmark{restart3s}$C \gets \emptyset$\tikzmark{restart3e} \;
    \nl \ForEach {$v' \in D_v~\textnormal{ordered by value heuristic}$\siplabel{valueordering}}{
        \nl \tikzmark{restart4s}$C \gets C \cup \{ v' \}$\tikzmark{restart4e} \;
        \nl $D' \gets \FuncSty{clone}(D)$ \;
        \nl $D'_v \gets \{ v' \} $ \;
        \nl \If{\FuncSty{propagate}(D')\siplabel{dopropagate}}{
            \nl $S \gets \FuncSty{search}(\{ E \in D'\,{:}\,\left|E\right|{>}\,1 \}, B \wedge (v\,{\mapsto}\,v'))$\siplabel{recurse} \;
            \nl \lIf{$S = \KwSty{true}$}{$\KwSty{return}~\KwSty{true}$}
            \nl \tikzmark{restart5s}\ElseIf{$S = \KwSty{restart}$}{
                \nl post nogoods $\{B \wedge (v \mapsto c) \Rightarrow \bot : c \in C \}$\tikzmark{restart5r}\;
                \nl $\KwSty{return}~\KwSty{restart}$\tikzmark{restart5e} \;
            }
        }
    }
    \nl \tikzmark{restart6s}\lIf{$(R \gets R + 1) = R'$}{$\KwSty{return}~\KwSty{restart}\tikzmark{restart6e}$\siplabel{countrestarts}}
    \nl $\KwSty{return}~\KwSty{false}$\siplabel{backtrack}
}
\medskip
\nl $\FuncSty{propagate}$ (Domains $D$) $\rightarrow$ Bool \;
\nl \Begin{
    \nl \While{$D_v \gets$~\textnormal{a unit domain from}~$D$}{
        \nl $v' \gets $ the single value in $D_v$ \;
        \nl \tikzmark{restart7s}trigger nogoods with watch $(v \mapsto v')$\tikzmark{restart7e}\siplabel{2wl} \;
        \nl \ForEach{$D_w \in D - D_v$}{
            \nl $D_w \gets D_w - v'$ \siplabel{removev} \;
            \nl \ForEach{$(P,\,T) \in L$}{
                \nl \lIf{$v \sim_P w$}{$D_w \gets D_w \cap \neighbourhood_T(v')$
                }
            }
            \nl \lIf{$D_w = \emptyset$}{$\KwSty{return}~\KwSty{false}$}
        }
\nl $(H,\,A,\,n) \gets (\emptyset,\,\emptyset,\,0)$\siplabel{alldiffstart} \;
\nl \ForEach{$D_v \in D$ \textnormal{from smallest cardinality upwards\siplabel{eachdomain}}}{
    \nl $D_v \gets D_v \setminus H$ \siplabel{elimhall} \;
    \nl $(A,\,n) \gets (A \cup D_v,\,n + 1)$ \siplabel{acc} \;
    \nl \lIf{$D_v = \emptyset~\vee~|A| < n$}{$\KwSty{return}~\KwSty{false}$\siplabel{failhall}}
    \nl \lIf{$|A| = n$}{$(H,\,A,\,n) \gets (H \cup A,\,\emptyset,\,0)$\siplabel{hall}}\siplabel{alldiffend}
}
    }
\nl $\KwSty{return}~\KwSty{true}$ \;
}
\caption{Non-induced subgraph isomorphism. The shaded code is added for the restarting variant.}
\label{algorithm:sip}
\end{algorithm}

For the variable-ordering heuristic on \siplineref{variableordering} of \cref{algorithm:sip}, we use
smallest domain first, tiebreaking on highest degree. Variable ordering is not the focus of this
paper---as far as possible, we will leave it intact, so as not to affect the performance of the
algorithm on unsatisfiable instances.  For value ordering, which occurs on
\siplineref{valueordering} of \cref{algorithm:sip}, we will initially select vertices from highest
degree to lowest degree; we return to value ordering in the second half of this paper. Note that the
variable-ordering heuristic affects both unsatisfiable and satisfiable instances, whilst the
value-ordering heuristic is relevant only for satisfiable instances.

\section{Empirical Evaluation}

Our experiments are performed on systems with dual Intel Xeon E5-2697A v4 CPUs and 512GBytes RAM,
running Ubuntu 17.04. We implemented\footnote{Institutional DOI URL removed for anonymous review}
\cref{algorithm:sip} in C++ using the bit-parallel implementation of
\citet{DBLP:conf/aaai/HoffmannMR17} as a starting point, and compiled it using GCC 6.3.0. We use a
deterministic pseudo-random number generator for reproducibility.

We use the dataset introduced by \citet{DBLP:conf/lion/KotthoffMS16} for evaluation. This dataset
brings together a range of randomly-generated and application instance families from earlier papers.
Other studies use a random selection of 200 of each of the instances from the ``meshes'' and
``images'' families because some earlier solvers find many of these instances extremely hard. We
would like to have a larger number of satisfiable instances in our test set, and so we include all
pattern / target pairs. This gives a total of 14,621 instances (rather than the original 5,725). At
least \input{gen-n-sat.tex}\unskip\ of these instances are known to be satisfiable, and at least
\input{gen-n-unsat.tex}\unskip\ are unsatisfiable.

\subsection{Baseline Performance}

\begin{figure}[tb]
    \centering
    \includegraphics*{gen-graph-others.pdf}

    \bigskip

    \centering
    \includegraphics*{gen-graph-others-zoom.pdf}

    \caption{Above, the cumulative number of instances solved over time, comparing our algorithm
    (both in its basic form, and with the improvements introduced in the remainder of the paper) to
    other solvers. Below, the same, looking only at satisfiable instances.}
    \label{figure:others}
\end{figure}

We begin by verifying that our implementation of the basic \cref{algorithm:sip} is competitive. In
\cref{figure:others} we show the cumulative number of instances solved over time, using our
implementation (both with the degree heuristic, and with the modifications described in the
remainder of this paper), the Glasgow2 and Glasgow3 algorithms from which \cref{algorithm:sip} is derived
\citep{DBLP:conf/cp/McCreeshP15,DBLP:conf/lion/KotthoffMS16}, the PathLAD variation of the LAD
algorithm \citep{DBLP:journals/ai/Solnon10,DBLP:conf/lion/KotthoffMS16}, and VF2
\citep{DBLP:journals/pami/CordellaFSV04}, in each case using the original implementation provided
by the algorithm's authors. For each algorithm, the $y$ value
gives the cumulative number of instances which (individually) can be solved in no more than $x$
milliseconds.  The vertical distance between two lines therefore shows how many more instances can
be solved by one solver than another, if every instance is run separately with the chosen $x$
timeout; the horizontal distance shows how many times longer the per-instance timeout would need to
be to allow the rightmost algorithm to succeed on $y$ out of the 14,621 instances (bearing in mind
that the two sets of $y$ instances solved by the two algorithms could be completely different).

As expected, the performance of \cref{algorithm:sip} with the degree heuristic is very close to that
of Glasgow2, although we perform better at lower runtimes because we use simpler preprocessing at
the top of search. Our implementation also clearly beats PathLAD and VF2, except for very low
choices of timeout.

The dataset includes many instances which are extremely easy for a good solver, and so it can be
hard to see the differences between the stronger solvers at higher runtimes. This paper focusses
upon improving the performance on the remaining hard satisfiable instances, and so in the second
plot in \cref{figure:others} (and in subsequent cumulative plots) we show only satisfiable
instances, and use a reduced range on both axes.

\subsection{Discrepancy Searches}

A \emph{discrepancy} is where search goes against a value-ordering heuristic.  Discrepancy searches
\citep{DBLP:conf/ijcai/HarveyG95,DBLP:conf/aaai/Korf96,DBLP:conf/ijcai/Walsh97} are alternatives to
backtracking search that initially search disallowing all discrepancies, and then retry search
allowing an increasing number of discrepancies at each iteration until either a solution is found or
unsatisfiability is proven. These schemes assume that value-ordering heuristics are usually
reliable, and that most solutions can be found with only a small number of discrepancies; in such
cases, the heavy commitment to early branching choices made by conventional backtracking search can
be extremely costly.

\Cref{figure:scatter-dds} shows the effects of adding \citepos{DBLP:conf/ijcai/Walsh97}
depth-bounded discrepancy search (DDS) to \cref{algorithm:sip}. Each point represents the solving
time for one instance---to avoid noise for easier instances, we measure only time spent during
search, and exclude time spent in preprocessing and initialisation.  Points below the $x-y$ diagonal
are speedups, whilst points on the top and right axes represent instances which timed out after one
thousand seconds with one algorithm, but not the other. For satisfiable instances, the different
point styles show the different families, whilst all unsatisfiable instances are shown as dark dots.

The points well below the diagonal line and along the right-hand axis show that DDS can sometimes be
extremely beneficial on satisfiable instances.  However, on both unsatisfiable and most satisfiable
instances, the overheads can be extremely large, and DDS is much worse in aggregate (even when only
considering satisfiable instances).

These large overheads are to be expected: discrepancy searches are aimed primarily at getting better
feasible solutions in optimisation problems which are too large for a proof of optimality to be a
realistic prospect, and they are not well-suited for unsatisfiable decision problems. However, the
extremely large gains on some satisfiable instances confirm our suspicions that we should look at
value-ordering heuristics in more detail.

\subsection{Value-Ordering Heuristics}

\begin{figure}[p]
    \centering
    \includegraphics*{gen-graph-scatter-dds.pdf}

    \caption{Comparing depth-bounded discrepancy search to backtracking search, both with the degree
    heuristic.}\label{figure:scatter-dds}
\end{figure}

\begin{figure}[p]
    \centering
    \includegraphics*{gen-graph-value-ordering-heuristics.pdf}

    \caption{The cumulative number of satisfiable instances solved over time, using four
    different value-ordering heuristics. The degree and biased lines are close together.}
    \label{figure:value-ordering-heuristics}
\end{figure}

\begin{figure}[p]
    \centering
    \includegraphics*{gen-graph-scatter-heuristics.pdf}

    \caption{An instance by instance comparison of the degree and biased heuristics on all
    instances. Points on the outer axes are timeouts, and point style shows instance family.}
    \label{figure:scatter-heuristics}
\end{figure}

In \cref{figure:value-ordering-heuristics} we show the cumulative number of satisfiable instances
solved over time using four different value-ordering heuristics:

\paragraph{Degree} We select vertices from highest degree to lowest degree. This heuristic is the
current state of the art.

\paragraph{Random} We select vertices uniformly at random.

\paragraph{Anti} We select vertices from lowest degree to highest degree. This heuristic is used as
a sanity check: as expected, degree beats random, and random beats anti.

\paragraph{Biased} We branch by selecting a vertex $v'$ from the chosen domain $D_v$ with
probability \[ p(v') = \frac{2^{\deg(v')}}{\sum_{w \in D_v}{2^{\deg(w)}}} \text{.} \] This heuristic
continues to prefer vertices of high degree, but will give the same chance of being selected next to
two vertices of equal degree.  It also introduces an element of randomness, which is needed to make
restarts have an effect.

The biased heuristic resembles the well-known \emph{softmax} weighting scheme applied to the degree
heuristic, although usually \emph{softmax} uses a base of $e$ rather than $2$.  Experiments showed
that in this context, the choice of base is not important to the search space size; however, a base
of $2$ avoids the need to use floating point arithmetic, which was a substantial overhead.

The performance of the degree and biased heuristics are very similar: the
biased heuristic has slightly worse performance below a 400s timeout, and effectively equal
performance above it. This shows that we can introduce an element of randomness into the degree
value-ordering heuristic without adversely affecting its performance \emph{in aggregate}.
\Cref{figure:scatter-heuristics} gives a more detailed look at these results. This scatter plot
shows that despite the aggregate performance being very similar, on an case by case basis, the two
heuristics can make a large difference to the performance of satisfiable instances.

\subsection{Adding Restarts to the Algorithm}

Another alternative to plain backtracking search is provided by \emph{restarts}. The general idea is
to perform a certain amount of search, and then if no solution has been found (and unsatisfiability
has not been proven), to abandon search and restart from the beginning. Obviously, such an approach
can only be beneficial if something changes after restarting---in a constraint programming setting,
this is usually the variable-ordering heuristic
\citep{DBLP:journals/jsat/LecoutreSTV07,DBLP:conf/cp/GayHLS15,DBLP:conf/aaai/LeeSZ16,DBLP:conf/cp/GlorianBLLM17}.
In this paper, we instead rely upon randomness in the \emph{value}-ordering heuristic, and keep the
variable ordering fixed.\footnote{It may be possible to further improve \cref{algorithm:sip} by also
introducing randomness or some form of learning into its variable-ordering heuristic. However,
introducing a second change would considerably complicate the empirical analysis. We therefore leave
this change as future work.} Using restarts on value-ordering heuristics appears to be uncommon,
although \citet{DBLP:conf/flairs/RazgonOP07} look at learning value-ordering heuristics from
restarts, and \citet{DBLP:conf/cp/ChuSS09} use a similar scheme in the context of parallel search.

The shaded code shows how to introduce restarts to \cref{algorithm:sip}. The recursive
$\FuncSty{search}$ procedure is modified to return either true (if the instance is satisfiable),
false (unsatisfiable), or a third value if some threshold was reached without the solution being
known.  The main function then calls $\FuncSty{search}$ in a loop (\siplineref{restartsloop}), until
a solution is found.

We use the Luby scheme \citep{DBLP:journals/ipl/LubySZ93} to determine when to restart. We count the
number of backtracks (that is, when we reach the end of the main $\KwSty{foreach}$ loop on
\siplineref{countrestarts}) to decide when to restart, rather than counting recursive calls.
Following established wisdom, we multiply each item in the Luby sequence by a magic constant.
Preliminary experiments demonstrated that a constant 100 is slightly too small and 1,000 is slightly
too large, but lacking a more principled way of selecting the constant's value, we fell back on
divine revelation and use a multiplier of 666. (We also tried a geometric scheme. The results were
less favourable---it appears that value-ordering heuristics benefit from a very aggressive restart
strategy.)

The remainder of the changes are to introduce nogoods. To avoid exploring portions of the search
space that we have already visitied, every time we restart, we add new constraints to the problem
which eliminate already-explored subtrees. We generate simple decision nogoods, as follows. The $B$
argument to $\FuncSty{search}$ tracks the branching choices made so far, in the form of a CNF clause
with each literal being a guessed $(v \mapsto v')$ assignment. Upon backtracking due to a decision to
restart, we post a nogood of the form $(v \mapsto v') \wedge (w \mapsto w') \wedge (x \mapsto x')
\Rightarrow \bot$ for the current branch, and for every branch to its left in the search tree.

Finally, we use the two watched literals technique \citep{DBLP:conf/dac/MoskewiczMZZM01} to
propagate stored nogoods (\siplineref{2wl}). This has two benefits: the propagation complexity does
not particularly depend upon the number of stored nogoods, and it does not require any work upon
backtracking.  Other more sophisticated nogood generation and propagation schemes exist
\citep{DBLP:conf/aaai/LeeSZ16,DBLP:conf/cp/GlorianBLLM17}, but it is not clear whether these will be
beneficial in a setting where we are not maintaining arc consistency.

\subsection{The Effects of Restarts}

In \cref{figure:restarts} we show the effects of adding restarts to the algorithm. With restarts,
the random value-ordering heuristic comfortably beats the degree strategy without restarts. In other
words, although having a good variable-ordering heuristic is beneficial, introducing randomness into
the search is better, if it is done alongside a mechanism to avoid heavy commitment to any
particular random choice. \Cref{figure:scatter-random} looks at this in more detail. The performance
on unsatisfiable instances, drawn as dark dots, remains unchanged as expected (which shows that the
overheads of nogoods and restarting are negligible). However, although better in aggregate on
satisfiable instances, on a case by case basis many instances perform substantially worse with
randomisation and restarts. ?? Quantify this a bit

\begin{figure}[p]
    \centering
    \includegraphics*{gen-graph-restarts.pdf}

    \caption{The cumulative number of satisfiable instances solved over time, with and without
    restarts.}
    \label{figure:restarts}
\end{figure}

\begin{figure}[p]
    \centering
    \includegraphics*{gen-graph-scatter-random.pdf}
    \caption{Comparing basic backtracking with the degree heuristic, versus the Random heuristic
    with restarts.}
    \label{figure:scatter-random}
\end{figure}

\begin{figure}[p]
    \centering
    \includegraphics*{gen-graph-scatter-by-family.pdf}

    \caption{Comparing basic backtracking with the degree heuristic, versus the biased
    heuristic with restarts.}
    \label{figure:scatter-by-family}
\end{figure}

\Cref{figure:restarts} shows that the biased heuristic together with restarts is better still---that
is, if we are introducing restarts, then it is better to add a small amount of randomness to a
tailored deterministic heuristic than it is to throw away the heuristic altogether.

In the more detailed view in \cref{figure:scatter-by-family}, comparing the basic algorithm with the
degree heuristic to the degree-biased algorithm with restarts, all of the unsatisfiable instances
are very close to the $x-y$ diagonal, showing that their performance is nearly unchanged. On the
other hand, there are ?? large numbers of satisfiable instances well below the diagonal line,
indicating large speedups.  Better yet, there are only ?? satisfiable instances that are more than a
factor of ?? times worse.

?? By family

?? This has made up the consistency we lost from introducing randomness.

?? Explain why this is so amazingly awesome and the most important idea ever to happen in the entire
history of science.

?? Justify aggregate speedups on sat instances

\section{Maximum Common Subgraph Algorithms}

?? Two recent maximum common induced subgraph algorithms: k${\downarrow}$
\citep{DBLP:conf/aaai/HoffmannMR17}, and McSplit \citep{DBLP:conf/ijcai/McCreeshPT17}. The former
attempts to solve the problem by first trying to solve the induced subgraph isomorphism problem, and
then if that fails, retries allowing a single unmatched vertex (and thus using weaker invariants),
and so on. Due to its similarity to \cref{algorithm:sip}, we can introduce the same bias and restart
strategy, requiring only a small amount of jiggery-pokery to handle wildcards in nogoods.

The McSplit algorithm, meanwhile, uses a constraint programming style search, but with special
propagators and backtrackable data structures that exploit special properties of the problem. The
unconventional domain store used by McSplit precludes the use of arbitrary unit propagation, and so
when introducing restarts, we cannot propagate using nogoods. Instead, we can only detect when we
are inside an already-visited branch. We must therefore use the one watched literal scheme instead.

?? Benchmark results on the same families of instances are shown in
\cref{figure:kdown,figure:mcsplit,figure:mcs}. Clearly beneficial. Note that on McSplit we're doing
extra work and this shows, but it's still worthwhile.

\begin{figure}[p]
    \centering
    \includegraphics*{gen-graph-scatter-kdown.pdf}
    \caption{k${\downarrow}$ speedups}\label{figure:kdown}
\end{figure}

\begin{figure}[p]
    \centering
    \includegraphics*{gen-graph-scatter-mcsplit.pdf}
    \caption{McSplit speedups}\label{figure:mcsplit}
\end{figure}

\begin{figure}[p]
    \centering
    \includegraphics*{gen-graph-mcs.pdf}
    \caption{k${\downarrow}$ and McSplit cumulative curves}\label{figure:mcs}
\end{figure}

\section{Conclusion}

?? This stuff is quite good.

?? We can keep good value-ordering heuristics, and get large improvement. Importantly, we can do
this with almost no effect on unsatisfiable instances, and rare to make satisfiable much worse.

?? No theoretical justification for biased. Also tried polynomial weightings, exponential using
$\left|D_v\right|$ as the base. Using $e$ (or for that matter, $2$ or $\pi$) as a base seems to be
``just the right amount'' of randomness.

?? Variable ordering

?? Bigger CP picture.

?? Parallel future work

\bibliographystyle{named}
\bibliography{dblp}

\clearpage
\appendix

\section{Extra Plots not for the Final Paper}

\begin{figure}[h]
    \centering
    \includegraphics*{gen-graph-as.pdf}
    \caption{?? Aggregate speedups on satisfiable instances}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics*{gen-graph-as-mcs.pdf}
    \caption{?? Aggregate speedups on MCS instances}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics*{gen-graph-scatter-degree.pdf}

    \caption{When is the degree heuristic good?}
    \label{figure:scatter-degree}
\end{figure}

\end{document}

