% vim: set spell spelllang=en tw=100 et sw=4 sts=4 :

\documentclass{article}
\usepackage{ijcai17}
\usepackage{times}
\usepackage{microtype}
\usepackage{amsmath}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{tikz}

\usepackage{cleveref}

% \usepackage{showframe}

\newcommand{\neighbourhood}{\operatorname{N}}
\newcommand{\vertexset}{\operatorname{V}}
\newcommand{\nds}{\operatorname{S}}

\newcommand{\citet}[1]{\citeauthor{#1} \shortcite{#1}}
\newcommand{\citep}[1]{\cite{#1}}

\usetikzlibrary{decorations, decorations.pathreplacing, decorations.pathmorphing,
    calc, backgrounds, positioning, tikzmark, patterns, fit}

\definecolor{highlight}{rgb}{0.819608, 0.733333, 0.34902}

% cref style
\crefname{algorithm}{Algorithm}{Algorithms}
\Crefname{algorithm}{Algorithm}{Algorithms}
\crefname{figure}{Figure}{Figures}
\Crefname{figure}{Figure}{Figures}
\crefname{table}{Table}{Tables}
\Crefname{table}{Table}{Tables}
\crefname{section}{Section}{Sections}
\Crefname{section}{Section}{Sections}

\title{Value Ordering, Discrepancies, and Restarts for Subgraph Isomorphism}
\author{Ciaran McCreesh\thanks{This work was supported by the Engineering and Physical Sciences
    Research Council [grant number EP/026842/1]}\\ University of Glasgow, Glasgow, Scotland \\
    ciaran.mccreesh@glasgow.ac.uk}

\begin{document}

\maketitle

\begin{abstract}
    Modern subgraph isomorphism solvers use value-ordering heuristics to direct a constraint
    programming style search. These heuristics are far from perfect, and experiments show that using
    a random value-ordering heuristic with restarts beats conventional search with the best known
    tailored heuristic. This motivates the introduction of a slightly-random value-ordering
    heuristic with similar behaviour to the tailored heuristic: when combined with restarts, this
    new search strategy is over a hundred times more effective on satisfiable instances, and does
    not significantly decrease performance on unsatisfiable instances. In contrast,
    discrepancy-based searches are too expensive to be effective in this setting.
\end{abstract}

\section{Introduction}

The subgraph isomorphism problem is to find a copy of a small ``pattern'' graph inside a larger
``target'' graph.  The current single strongest subgraph isomorphism solver uses ``highest degree
first'' as a value-ordering heuristic to direct a constraint programming style search
\citep{DBLP:conf/cp/McCreeshP15,DBLP:conf/lion/KotthoffMS16,DBLP:conf/ijcai/McCreeshPT16}.  This is
much better than not having a value-ordering heuristic at all, but it is far from perfect,
particularly for early branching choices. There are two reasons to suspect a better option may
possible.  Firstly, even if degree were a perfect source of information, many target graphs do not
have a large degree distribution, and so we would still need to determine \emph{which} vertex of
highest degree would be best.  Secondly, experiments with a parallel search strategy which
explicitly steals early branching choices often gives strongly super-linear speedups on satisfiable
instances \citep{DBLP:conf/cp/McCreeshP15}.

We could try to introduce tiebreaking mechanisms to further refine this heuristic. However, given
the extreme cost of making an incorrect branching choice early on in search, we believe it is more
fruitful to investigate alternatives to simple backtracking (depth-first) search. This paper
evaluates two such approaches: discrepancy search (which performs poorly), and restarts (which are
extremely beneficial). To explore different parts of the search space when restarting, we introduce
a new value-ordering heuristic, which adds a small amount of randomness to the ``highest degree
first'' heuristic.  The effects of the changes we propose are shown in
\cref{figure:scatter-by-family}: we can make a state-of-the-art algorithm perform much better on a
large number of satisfiable instances, whilst performing worse only rarely on satisfiable instances,
and never on unsatisfiable instances.  Further, these properties are \emph{not} shared by
discrepancy searches (\cref{figure:scatter-dds}) or by random value-ordering with restarts
(\cref{figure:scatter-random}).

\section{The Basic Algorithm}

?? Non-induced!

?? Notation: $\vertexset(G)$, $\neighbourhood_G(v)$, $\nds_G(v)$, $\preceq$, $\sim_G$, $G^{a,b}$, $\mapsto$.

?? Started with the Glasgow algorithm, removed parts which did not contribute hugely to speedups,
and ended up with a simpler algorithm that performs as well in practice. In particular, because we
will be working with reasonably large target graphs, no ILF, and not doing supplemental graphs of
length 3. This is \cref{algorithm:sip}, ignoring the shaded parts, which are introduced in the
context of restarts.

?? Briefly describe this, probably referring elsewhere as far as possible to save space.

\newcommand{\siplabel}[1]{\label{line:sip:#1}}
\newcommand{\siplineref}[1]{line~\ref{line:sip:#1}}
\begin{algorithm}[p]\DontPrintSemicolon\small\SetInd{0.1em}{1em}
    \begin{tikzpicture}[remember picture,overlay]
        \coordinate (restart1sc) at ($(pic cs:restart1s) + (0, 0.15)$);
        \coordinate (restart1ec) at ($(pic cs:restart1e) + (0, 0.00)$);
        \node [fill=highlight, rounded corners, fit=(restart1sc) (restart1ec)] { };
        \coordinate (restart2sc) at ($(pic cs:restart2s) + (0, 0.15)$);
        \coordinate (restart2ec) at ($(pic cs:restart2e) + (0, 0.00)$);
        \node [fill=highlight, rounded corners, fit=(restart2sc) (restart2ec)] { };
        \coordinate (restart3sc) at ($(pic cs:restart3s) + (0, 0.15)$);
        \coordinate (restart3ec) at ($(pic cs:restart3e) + (0, 0.00)$);
        \node [fill=highlight, rounded corners, fit=(restart3sc) (restart3ec)] { };
        \coordinate (restart4sc) at ($(pic cs:restart4s) + (0, 0.15)$);
        \coordinate (restart4ec) at ($(pic cs:restart4e) + (0, 0.00)$);
        \node [fill=highlight, rounded corners, fit=(restart4sc) (restart4ec)] { };
        \coordinate (restart5sc) at ($(pic cs:restart5s) + (0, 0.15)$);
        \coordinate (restart5ec) at ($(pic cs:restart5e) - (0, 0.10)$);
        \coordinate (restart5rc) at ($(pic cs:restart5r) + (0, 0.00)$);
        \node [fill=highlight, rounded corners, fit=(restart5sc) (restart5ec) (restart5rc)] { };
        \coordinate (restart6sc) at ($(pic cs:restart6s) + (0, 0.15)$);
        \coordinate (restart6ec) at ($(pic cs:restart6e) + (0, 0.00)$);
        \node [fill=highlight, rounded corners, fit=(restart6sc) (restart6ec)] { };
        \coordinate (restart7sc) at ($(pic cs:restart7s) + (0, 0.15)$);
        \coordinate (restart7ec) at ($(pic cs:restart7e) + (0, 0.00)$);
        \node [fill=highlight, rounded corners, fit=(restart7sc) (restart7ec)] { };
        \coordinate (restart8sc) at ($(pic cs:restart8s) + (0.05, 0.15)$);
        \coordinate (restart8ec) at ($(pic cs:restart8e) + (0, 0.00)$);
        \node [fill=highlight, rounded corners, fit=(restart8sc) (restart8ec)] { };
        \coordinate (restart9sc) at ($(pic cs:restart9s) + (0, 0.15)$);
        \coordinate (restart9ec) at ($(pic cs:restart9e) + (-0.15, 0.00)$);
        \node [fill=highlight, rounded corners, fit=(restart9sc) (restart9ec)] { };
    \end{tikzpicture}

    \nl $\FuncSty{subgraphIsomorphism}$ (Graph $\mathcal{P}$, Graph $\mathcal{T}$) $\rightarrow$ Bool \;
\nl \Begin{
    \nl \lIf{$\left|\vertexset(\mathcal{P})\right| >
    \left|\vertexset(\mathcal{T})\right|$}{$\KwSty{return}~\KwSty{false}$\siplabel{enough}}
    \nl Discard isolated vertices in $\mathcal{P}$\siplabel{isolated} \;
    \nl \KwSty{global} $L \gets \big[ (\mathcal{P},\ \mathcal{T}), \siplabel{supplemental}
        (\mathcal{P}^{1,2}, \mathcal{T}^{1,2}),
        (\mathcal{P}^{2,2},\ \mathcal{T}^{2,2}), $\\\hspace*{6em}$ (\mathcal{P}^{3,2},\ \mathcal{T}^{3,2}),
        (\mathcal{P}^{4,2},\ \mathcal{T}^{4,2}) \big]$ \;
    \nl \ForEach{$v \in \vertexset(\mathcal{P})$}{
        \nl $D_v \gets \vertexset(\mathcal{T})$\;
        \nl \ForEach{$(P,\,T) \in L$}{
            \nl $D_v \gets \{ w \in D_v : $ \siplabel{degreefiltering} \\\hspace*{4em}$v \sim_P v \Rightarrow w \sim_T w~\wedge$
            \\\hspace*{4em}$\nds_P(v) \preceq \nds_T(w)\}$ \;
        }
    }
    \nl \While{$\KwSty{true}$}{
        \nl \lIf{$\KwSty{not}~\FuncSty{propagate}(D)$}{$\KwSty{return}~\KwSty{false}$}
        \nl \tikzmark{restart1s}$\KwSty{global}~(R, R') \gets (0, \textnormal{the next restart sequence value})$\tikzmark{restart1e} \;
        \nl $S \gets \FuncSty{search}(\{ E \in D : \left|E\right| > 1 \}, \top)$ \;
        \nl \tikzmark{restart9s}\lIf{$S \ne \KwSty{restart}$}{\tikzmark{restart9e}$\KwSty{return}~S$}
        \nl \tikzmark{restart2s}trigger nogoods containing only one clause\tikzmark{restart2e} \;
    }
}
\bigskip
    \nl $\FuncSty{search}$ (Domains $D$, Decisions $B$) $\rightarrow$ Bool
    \tikzmark{restart8s}\KwSty{or} \KwSty{restart}\tikzmark{restart8e} \;
\nl \Begin{
    \nl \lIf{$D = \emptyset$}{$\KwSty{return}~\KwSty{true}$}
    \nl $D_v \gets \textnormal{a domain from}~D~\textnormal{chosen by variable heuristic}$\siplabel{variableordering} \;
    \nl \tikzmark{restart3s}$C \gets \emptyset$\tikzmark{restart3e} \;
    \nl \ForEach {$v' \in D_v~\textnormal{ordered by value heuristic}$\siplabel{valueordering}}{
        \nl \tikzmark{restart4s}$C \gets C \cup \{ v' \}$\tikzmark{restart4e} \;
        \nl $D' \gets \FuncSty{clone}(D)$ \;
        \nl $D'_v \gets \{ v' \} $ \;
        \nl \If{\FuncSty{propagate}(D')}{
            \nl $S \gets \FuncSty{search}(\{ E \in D' : \left|E\right| > 1 \}, B \wedge (v \mapsto v'))$ \;
            \nl \lIf{$S = \KwSty{true}$}{$\KwSty{return}~\KwSty{true}$}
            \nl \tikzmark{restart5s}\ElseIf{$S = \KwSty{restart}$}{
                \nl post nogoods $\{B \wedge (v \mapsto c) \Rightarrow \bot : c \in C \}$\tikzmark{restart5r}\;
                \nl $\KwSty{return}~\KwSty{restart}$\tikzmark{restart5e} \;
            }
        }
    }
    \nl \tikzmark{restart6s}\lIf{$(R \gets R + 1) = R'$}{$\KwSty{return}~\KwSty{restart}\tikzmark{restart6e}$}
    \nl $\KwSty{return}~\KwSty{false}$
}
\bigskip
\nl $\FuncSty{propagate}$ (Domains $D$) $\rightarrow$ Bool \;
\nl \Begin{
    \nl \While{$D_v \gets$~\textnormal{a unit domain from}~$D$}{
        \nl $v' \gets $ the single value in $D_v$ \;
        \nl \tikzmark{restart7s}trigger nogoods with watch $(v \mapsto v')$\tikzmark{restart7e} \;
        \nl \ForEach{$D_w \in D - D_v$}{
            \nl $D_w \gets D_w - v'$ \siplabel{removev} \;
            \nl \ForEach{$(P,\,T) \in L$}{
                \nl \lIf{$v \sim_P w$}{$D_w \gets D_w \cap \neighbourhood_T(v')$
                }
            }
            \nl \lIf{$D_w = \emptyset$}{$\KwSty{return}~\KwSty{false}$}
        }
\nl $(H,\,A,\,n) \gets (\emptyset,\,\emptyset,\,0)$ \;
\nl \ForEach{$D_v \in D$ \textnormal{from smallest cardinality upwards\siplabel{eachdomain}}}{
    \nl $D_v \gets D_v \setminus H$ \siplabel{elimhall} \;
    \nl $(A,\,n) \gets (A \cup D_v,\,n + 1)$ \siplabel{acc} \;
    \nl \lIf{$D_v = \emptyset~\vee~|A| < n$}{$\KwSty{return}~\KwSty{false}$\siplabel{failhall}}
    \nl \lIf{$|A| = n$}{$(H,\,A,\,n) \gets (H \cup A,\,\emptyset,\,0)$\siplabel{hall}}
}
    }
\nl $\KwSty{return}~\KwSty{true}$ \;
}
\caption{Non-induced subgraph isomorphism. Shaded code is added for the restarting variant.}
\label{algorithm:sip}
\end{algorithm}

\subsection{Ordering Heuristics}

For the variable-ordering heuristic on \siplineref{variableordering} of \cref{algorithm:sip}, we use
smallest domain first, tiebreaking on highest degree. Variable ordering is not the focus of this
paper---as far as possible, we will leave it intact, so as not to affect the performance of the
algorithm on unsatisfiable instances.  For value ordering, which occurs on
\siplineref{valueordering} of \cref{algorithm:sip}, we will compare four strategies:

\paragraph{Random} We select vertices uniformly at random.

\paragraph{Degree} We select vertices from highest degree to lowest degree. This heuristic is the
current state of the art.

\paragraph{Anti} We select vertices from lowest degree to highest degree. This heuristic is used as
a sanity check: we expect degree to beat random, and random to beat anti.

\paragraph{Biased} We branch by selecting a vertex $v'$ from the chosen domain $D_v$ with
probability \[ p(v') = \frac{2^{\deg(v')}}{\sum_{w \in D_v}{2^{\deg(w)}}} \text{.} \] This heuristic
continues to prefer vertices of high degree, but will give the same chance of being selected next to
two vertices of equal degree.  It also introduces an element of randomness, which is needed to make
restarts have an effect.

The biased heuristic resembles the well-known \emph{softmax} weighting scheme applied to the degree
heuristic, although usually \emph{softmax} uses a base of $e$ rather than $2$.  Experiments showed
that in this context, the choice of base is not important to the search space size; however, a base
of $2$ avoids the need to use floating point arithmetic, which was a substantial overhead.

\section{Empirical Evaluation}

Our experiments are performed on systems with dual Intel Xeon E5-2697A v4 CPUs and 512GBytes RAM,
running Ubuntu 17.04. We implemented\footnote{Institutional DOI URL removed for anonymous review}
\cref{algorithm:sip} in C++ using the bit-parallel implementation of
\citet{DBLP:conf/aaai/HoffmannMR17} as a starting point, and compiled it using GCC 6.3.0. We use a
deterministic pseudo-random number generator for reproducibility.

We use the dataset introduced by \citet{DBLP:conf/lion/KotthoffMS16} for evaluation. This dataset
brings together a range of randomly-generated and application instance families from earlier papers.
Other studies use a random selection of 200 of each of the instances from the ``meshes'' and
``images'' families \citep{DBLP:journals/cviu/DamiandSHJS11} because some earlier solvers find many
of these instances extremely hard. We would like to have a larger number of satisfiable instances in
our test set, and so we include all pattern / target pairs. This gives a total of 14,621 instances
(rather than the original 5,725). At least ?? of these instances are known to be satisfiable, and at
least ?? are unsatisfiable.

\subsection{Our Solver is Good}

\begin{figure}[tb]
    \centering
    \includegraphics*{gen-graph-others.pdf}

    \bigskip

    \centering
    \includegraphics*{gen-graph-others-zoom.pdf}

    \caption{Above, the cumulative number of instances solved over time, comparing our algorithm
    (both in its basic form, and with the improvements introduced in the remainder of the paper) to
    other solvers. Below, the same, looking only at satisfiable instances.}
    \label{figure:others}
\end{figure}

We begin by verifying that our implementation of the basic \cref{algorithm:sip} is competitive. In
\cref{figure:others} we show the cumulative number of instances solved over time, using our
implementation (both with the degree heuristic, and with the modifications described in the
remainder of this paper), the Glasgow2 and Glasgow3 algorithms from which \cref{algorithm:sip} is derived
\citep{DBLP:conf/cp/McCreeshP15,DBLP:conf/lion/KotthoffMS16}, the PathLAD variation of the LAD
algorithm \citep{DBLP:journals/ai/Solnon10,DBLP:conf/lion/KotthoffMS16}, and VF2
\citep{DBLP:journals/pami/CordellaFSV04}. For each algorithm, the $y$ value
gives the cumulative number of instances which (individually) can be solved in no more than $x$
milliseconds.  The vertical distance between two lines therefore shows how many more instances can
be solved by one solver than another, if every instance is run separately with the chosen $x$
timeout; the horizontal distance shows how many times longer the per-instance timeout would need to
be to allow the rightmost algorithm to succeed on $y$ out of the 14,621 instances (bearing in mind
that the two sets of $y$ instances solved by the two algorithms could be completely different).

As expected, the performance of \cref{algorithm:sip} with the degree heuristic is very close to that
of Glasgow2, although we perform better at lower runtimes because we use simpler preprocessing at
the top of search. Our implementation also clearly beats PathLAD and VF2, except for very low
choices of timeout.

The dataset includes many instances which are extremely easy for a good solver, and so it can be
hard to see the differences between the stronger solvers at higher runtimes. This paper focusses
upon improving the performance on the remaining hard satisfiable instances, and so in the second
plot in \cref{figure:others} (and in subsequent cumulative plots) we show only satisfiable
instances, and use a reduced range on both axes.

\subsection{Discrepancy Searches}

?? Quick intro to discrepancy search
\citep{DBLP:conf/ijcai/HarveyG95,DBLP:conf/aaai/Korf96,DBLP:conf/ijcai/Walsh97,DBLP:conf/cpaior/KarouiHLN07,DBLP:journals/jea/ProsserU11}.
Discrepancy search is about value ordering, and it mostly looks at optimisation problems and at
problems where an optimality proof is not needed.

?? Talk about how we're not dealing with a binary search tree, so we could either make it binary, or
count one discrepancy each time the degree changes, or count every ``against'' branch as being just
one discrepancy regardless.

?? \cref{figure:scatter-dds}. Sometimes extremely beneficial on satisfiable instances. However, on both
unsatisfiable and satisfiable instances, the overheads are often large, and the algorithm is not
better in aggregate (even when only considering satisfiable instances).

\subsection{Value Ordering Heuristics}

\begin{figure}[p]
    \centering
    \includegraphics*{gen-graph-scatter-dds.pdf}

    \caption{Comparing depth-bounded discrepancy search to backtracking search, both with the degree
    heuristic.}\label{figure:scatter-dds}
\end{figure}

\begin{figure}[p]
    \centering
    \includegraphics*{gen-graph-value-ordering-heuristics.pdf}

    \caption{The cumulative number of satisfiable instances solved over time, using four
    different value-ordering heuristics. The degree and biased lines are close together.}
    \label{figure:value-ordering-heuristics}
\end{figure}

\begin{figure}[p]
    \centering
    \includegraphics*{gen-graph-scatter-heuristics.pdf}

    \caption{An instance by instance comparison of the degree and biased heuristics on all
    instances. Points on the outer axes are timeouts, and point style shows instance family.}
    \label{figure:scatter-heuristics}
\end{figure}

In \cref{figure:value-ordering-heuristics} we show the cumulative number of instances solved over
time using the four different value-ordering heuristics. As expected, the degree heuristic is much
better (?? give aggregate speedups for both all and just satisfiable instances) than random
ordering, which is in turn ?? times better than the anti-heuristic.

The performance of the two degree heuristics are very similar: the
biased heuristic has slightly worse performance below a 100s timeout, and effectively equal
performance above it. This shows that we can introduce an element of randomness into the degree
value-ordering heuristic without adversely affecting its performance \emph{in aggregate}.
\Cref{figure:scatter-heuristics} gives a more detailed look at these results. In this plot,
satisfiable and unsatisfiable instances are shown using coloured shapes and black dots respectively.
On the $x$-axis we measure the degree heuristic, whilst on the $y$-axis we measure the biased
heuristic. Points on the outer axes represent instances which timed out with one heuristic but not
the other.  This plot shows that despite the aggregate performance being very similar, on an case by
case basis, the two heuristics can make a large difference to the performance of satisfiable
instances.

\subsection{Restarts}

?? Quick intro to restarts, mentioning how it's important to change something when restarting.
Interestingly usually that thing is variable-ordering heuristics, not value-ordering
\citep{DBLP:conf/ijcai/LecoutreSTV07,DBLP:journals/jsat/LecoutreSTV07,DBLP:conf/cp/GayHLS15,DBLP:conf/aaai/LeeSZ16,DBLP:conf/cp/GlorianBLLM17}.
?? Does anyone at all do this on value ordering explicitly? And if not, why not? There's
\citet{DBLP:conf/flairs/RazgonOP07} at least. ?? There's a connection here to confidence-based work
stealing \citep{DBLP:conf/cp/ChuSS09} too. But what? They talk about confidence in a heuristic, but
not that different values could be ranked differently.

?? We use the Luby scheme \citep{DBLP:journals/ipl/LubySZ93}, because everyone else does. We count
the number of backtracks (that is, when we reach the end of the main $\KwSty{foreach}$ loop) to
decide when to restart. Following established wisdom, we multiply each item in the Luby sequence by
a magic constant. Preliminary experiments demonstrated that this is useful, but we failed to
determine a principled way of selecting the constant's value, so we fell back on divine revelation
and set it to 666. As with everyone else, we are not entirely clear why we are doing this: it could
be to allow the solver to spend more time deep in search, it could be to reduce the overheads from
repeatedly visiting inner nodes, or it could be to keep the nogoods small.

?? We just use two watched literals \citep{DBLP:conf/dac/MoskewiczMZZM01}. Could use increasing
nogoods \citep{DBLP:conf/aaai/LeeSZ16,DBLP:conf/cp/GlorianBLLM17} etc, but we are not confident enough
in our programming abilities to implement these correctly, and we are not maintaining arc
consistency anyway. We also really do not want to propagate on negative decisions, because we do not
want to iterate over every removed value.

?? Talk a bit about how it fits into \cref{algorithm:sip}.

\subsection{Restarts Make it Better}

In \cref{figure:restarts} we show the effects of adding restarts to the algorithm. With restarts,
the random value-ordering heuristic comfortably beats the degree strategy without restarts. In
other words, although having a good variable-ordering heuristic is beneficial, introducing
randomness into the search is better, if it is done alongside a mechanism to avoid heavy commitment
to any particular random choice. \Cref{figure:scatter-random} looks at this in more detail.  The
performance on unsatisfiable instances is the same. However, although better in aggregate, we see
many satisfiable instances performing substantially worse. ?? Quantify this a bit.

\begin{figure}[p]
    \centering
    \includegraphics*{gen-graph-restarts.pdf}

    \caption{The cumulative number of satisfiable instances solved over time, with and without
    restarts.}
    \label{figure:restarts}
\end{figure}

\begin{figure}[p]
    \centering
    \includegraphics*{gen-graph-scatter-random.pdf}
    \caption{Comparing basic backtracking with the degree heuristic, versus the Random heuristic
    with restarts.}
    \label{figure:scatter-random}
\end{figure}

\begin{figure}[p]
    \centering
    \includegraphics*{gen-graph-scatter-by-family.pdf}

    \caption{Comparing basic backtracking with the degree heuristic, versus the biased
    heuristic with restarts.}
    \label{figure:scatter-by-family}
\end{figure}

\Cref{figure:restarts} shows that either biased heuristic together with restarts is better
still---that is, if we are introducing restarts, then it is better to add a small amount of
randomness to a tailored deterministic heuristic than it is to throw away the heuristic altogether.
The degree-biased heuristic is also stronger than position-biased, showing that exploiting ties and
relative sizes in the heuristic ordering is also worthwhile. Interestingly, the gap between degree-
and position-based is much larger when using restarts than it is when not using them.

In the more detailed view in \cref{figure:scatter-by-family}, comparing the basic algorithm with the
degree heuristic to the degree-biased algorithm with restarts, all of the unsatisfiable instances
are very close to the $x-y$ diagonal, showing that their performance is nearly unchanged. On the
other hand, there are ?? large numbers of satisfiable instances well below the diagonal line,
indicating large speedups.  Better yet, there are only ?? satisfiable instances that are more than a
factor of ?? times worse.

?? By family

?? This has made up the consistency we lost from introducing randomness.

?? Explain why this is so amazingly awesome and the most important idea ever to happen in the entire
history of science.

?? Justify aggregate speedups on sat instances

\subsection{Further Experiments}

\begin{itemize}
    \item The choice of Luby multiplier is not hugely important, although 100 is too small and 1,000
        is too big.
    \item The base of the exponential does not have to be $e$, and it may be easier on implementers
        to use 2. Using a polynomial weighting is bad, and using a base of $|D_v|$ is bad.
    \item Using geometric restarts instead of Luby does not give much of a benefit. A constant
        restart policy is also fine (and terminates on unsatisfiable instances because we count
        backtracks).
    \item Using rank with restarts is better than random with restarts, but not as good as using
        biased with restarts.
    \item Nogood recording is important.
    \item Nogood recording is cheap.
    \item Discarding subsumed nogoods may be done cheaply, if memory space is a concern.
\end{itemize}

\section{Conclusion}

?? This stuff is quite good.

?? We can keep good value-ordering heuristics, and get large improvement. Importantly, we can do
this with almost no effect on unsatisfiable instances, and rare to make satisfiable much worse.

?? No theoretical justification for biased. Also tried polynomial weightings, exponential using
$\left|D_v\right|$ as the base. Using $e$ (or for that matter, $2$ or $\pi$) as a base seems to be
``just the right amount'' of randomness.

?? Variable ordering

?? Bigger CP picture.

?? Parallel future work

\begin{figure}[tb]
    \centering
    \includegraphics*{gen-graph-as.pdf}
    \caption{?? Aggregate speedups on satisfiable instances}
\end{figure}

\begin{figure}[tb]
    \centering
    \includegraphics*{gen-graph-scatter-degree.pdf}

    \caption{When is the degree heuristic good?}
    \label{figure:scatter-degree}
\end{figure}

\bibliographystyle{named}
\bibliography{dblp}

\end{document}

