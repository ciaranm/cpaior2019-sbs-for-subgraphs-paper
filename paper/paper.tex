% vim: set spell spelllang=en tw=100 et sw=4 sts=4 :

\documentclass{article}
\usepackage{ijcai17}
\usepackage{times}
\usepackage{microtype}
\usepackage{amsmath}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{tikz}

\usepackage{cleveref}

% \usepackage{showframe}

\newcommand{\neighbourhood}{\operatorname{N}}
\newcommand{\vertexset}{\operatorname{V}}
\newcommand{\nds}{\operatorname{S}}

\newcommand{\citet}[1]{\citeauthor{#1} \shortcite{#1}}
\newcommand{\citep}[1]{\cite{#1}}

\usetikzlibrary{decorations, decorations.pathreplacing, decorations.pathmorphing,
    calc, backgrounds, positioning, tikzmark, patterns, fit}

\definecolor{inferno7}{rgb}{0.976471, 0.584314, 0.039216}

% cref style
\crefname{algorithm}{Algorithm}{Algorithms}
\Crefname{algorithm}{Algorithm}{Algorithms}
\crefname{figure}{Figure}{Figures}
\Crefname{figure}{Figure}{Figures}
\crefname{table}{Table}{Tables}
\Crefname{table}{Table}{Tables}
\crefname{section}{Section}{Sections}
\Crefname{section}{Section}{Sections}

\title{Value Ordering, Discrepancies, and Restarts in Subgraph Solvers: \\ Some Half-Baked Mucking
Around}
\author{Ciaran McCreesh\thanks{This work was supported by the Engineering and Physical Sciences
    Research Council [grant number EP/026842/1]}\\ University of Glasgow, Glasgow, Scotland \\
    ciaran.mccreesh@glasgow.ac.uk}

\begin{document}

\maketitle

\begin{abstract}
    Modern subgraph isomorphism solvers use value-ordering heuristics to direct search, but these
    heuristics are not perfect, particularly for early choices. We investigate discrepancy searches
    and restarts to offset this weakness. Our results suggest that discrepancy-based searches are
    too expensive to be effective.  However, restarts with a random value-ordering heuristic beats
    conventional search with the best tailored heuristic. This motivates the introduction of a
    slightly-random value-ordering heuristic with similar behaviour to the tailored heuristic: when
    combined with restarts, this new heuristic is even more effective on satisfiable instances,
    whilst not significantly decreasing performance on unsatisfiable instances.
\end{abstract}

\section{Introduction}

?? Subgraph isomorphism is useful.

The current single strongest subgraph isomorphism solver uses ``highest degree first'' as a
value-ordering heuristic \citep{dblp:conf/cp/McCreeshP15,dblp:conf/ijcai/McCreeshPT16}.  This is
much better than not having a value-ordering heuristic at all, but it is far from perfect,
particularly for early branching choices. There are two reasons to suspect we could do better.
Firstly, even if degree were a perfect source of information, many graphs do not have a large degree
distribution, and so we would still need to determine \emph{which} vertex of highest degree would be
best.  Secondly, experiments with a parallel search strategy which explicitly steals early branching
choices often gives strongly super-linear speedups on satisfiable instances
\citep{dblp:conf/cp/McCreeshP15}.

We could try to introduce tiebreaking mechanisms to further refine this heuristic. However, given
the extreme cost of making an incorrect branching choice early on in search, we believe it is more
fruitful to investigate alternatives to simple backtracking (depth-first) search. This paper
evaluates two such approaches: discrepancy search (which perform poorly), and restarts (which are
extremely beneficial). To make restarts work, we introduce a new value-ordering heuristic, which is
nearly the same as ``highest degree first'' except with small amounts of heavily biased randomness.
The effects of the changes we propose are shown in \cref{figure:scatter-by-family}: we can make a
state-of-the-art algorithm perform much better on a large number of satisfiable instances, whilst
performing worse only rarely on satisfiable instances, and never on unsatisfiable instances.
Further, these properties are \emph{not} shared by discrepancy searches (\cref{figure:scatter-dds})
or by random value-ordering with restarts (\cref{figure:scatter-random}).

\section{The Basic Algorithm}

?? Notation: $\vertexset(G)$, $\neighbourhood_G(v)$, $\nds_G(v)$, $\preceq$, $\sim_G$, $G^{a,b}$, $\mapsto$.

?? Started with the Glasgow algorithm, removed parts which did not contribute hugely to speedups,
and ended up with a simpler algorithm that performs as well in practice. In particular, because we
will be working with reasonably large target graphs, no ILF, and not doing supplemental graphs of
length 3. This is \cref{algorithm:sip}, ignoring the shaded parts, which are introduced in the
context of restarts.

?? Briefly describe this, probably referring elsewhere as far as possible to save space.

\newcommand{\siplabel}[1]{\label{line:sip:#1}}
\newcommand{\siplineref}[1]{line~\ref{line:sip:#1}}
\begin{algorithm}[p]\DontPrintSemicolon\small\SetInd{0.1em}{1em}
    \begin{tikzpicture}[remember picture,overlay]
        \coordinate (restart1sc) at ($(pic cs:restart1s) + (0, 0.15)$);
        \coordinate (restart1ec) at ($(pic cs:restart1e) + (0, 0.00)$);
        \node [fill=inferno7, rounded corners, fit=(restart1sc) (restart1ec)] { };
        \coordinate (restart2sc) at ($(pic cs:restart2s) + (0, 0.15)$);
        \coordinate (restart2ec) at ($(pic cs:restart2e) + (0, 0.00)$);
        \node [fill=inferno7, rounded corners, fit=(restart2sc) (restart2ec)] { };
        \coordinate (restart3sc) at ($(pic cs:restart3s) + (0, 0.15)$);
        \coordinate (restart3ec) at ($(pic cs:restart3e) + (0, 0.00)$);
        \node [fill=inferno7, rounded corners, fit=(restart3sc) (restart3ec)] { };
        \coordinate (restart4sc) at ($(pic cs:restart4s) + (0, 0.15)$);
        \coordinate (restart4ec) at ($(pic cs:restart4e) + (0, 0.00)$);
        \node [fill=inferno7, rounded corners, fit=(restart4sc) (restart4ec)] { };
        \coordinate (restart5sc) at ($(pic cs:restart5s) + (0, 0.15)$);
        \coordinate (restart5ec) at ($(pic cs:restart5e) - (0, 0.10)$);
        \coordinate (restart5rc) at ($(pic cs:restart5r) + (0, 0.00)$);
        \node [fill=inferno7, rounded corners, fit=(restart5sc) (restart5ec) (restart5rc)] { };
        \coordinate (restart6sc) at ($(pic cs:restart6s) + (0, 0.15)$);
        \coordinate (restart6ec) at ($(pic cs:restart6e) + (0, 0.00)$);
        \node [fill=inferno7, rounded corners, fit=(restart6sc) (restart6ec)] { };
        \coordinate (restart7sc) at ($(pic cs:restart7s) + (0, 0.15)$);
        \coordinate (restart7ec) at ($(pic cs:restart7e) + (0, 0.00)$);
        \node [fill=inferno7, rounded corners, fit=(restart7sc) (restart7ec)] { };
        \coordinate (restart8sc) at ($(pic cs:restart8s) + (0.05, 0.15)$);
        \coordinate (restart8ec) at ($(pic cs:restart8e) + (0, 0.00)$);
        \node [fill=inferno7, rounded corners, fit=(restart8sc) (restart8ec)] { };
        \coordinate (restart9sc) at ($(pic cs:restart9s) + (0, 0.15)$);
        \coordinate (restart9ec) at ($(pic cs:restart9e) + (-0.15, 0.00)$);
        \node [fill=inferno7, rounded corners, fit=(restart9sc) (restart9ec)] { };
    \end{tikzpicture}

    \nl $\FuncSty{subgraphIsomorphism}$ (Graph $\mathcal{P}$, Graph $\mathcal{T}$) $\rightarrow$ Bool \;
\nl \Begin{
    \nl \lIf{$\left|\vertexset(\mathcal{P})\right| >
    \left|\vertexset(\mathcal{T})\right|$}{$\KwSty{return}~\KwSty{false}$\siplabel{enough}}
    \nl Discard isolated vertices in $\mathcal{P}$\siplabel{isolated} \;
    \nl \KwSty{global} $L \gets \big[ (\mathcal{P},\ \mathcal{T}), \siplabel{supplemental}
        (\mathcal{P}^{1,2}, \mathcal{T}^{1,2}),
        (\mathcal{P}^{2,2},\ \mathcal{T}^{2,2}), $\\\hspace*{6em}$ (\mathcal{P}^{3,2},\ \mathcal{T}^{3,2}),
        (\mathcal{P}^{4,2},\ \mathcal{T}^{4,2}) \big]$ \;
    \nl \ForEach{$v \in \vertexset(\mathcal{P})$}{
        \nl $D_v \gets \vertexset(\mathcal{T})$\;
        \nl \ForEach{$(P,\,T) \in L$}{
            \nl $D_v \gets \{ w \in D_v : $ \siplabel{degreefiltering} \\\hspace*{4em}$v \sim_P v \Rightarrow w \sim_T w~\wedge$
            \\\hspace*{4em}$\nds_P(v) \preceq \nds_T(w)\}$ \;
        }
    }
    \nl \While{$\KwSty{true}$}{
        \nl \lIf{$\KwSty{not}~\FuncSty{propagate}(D)$}{$\KwSty{return}~\KwSty{false}$}
        \nl \tikzmark{restart1s}$\KwSty{global}~(R, R') \gets (0, 666 \times \textnormal{the next Luby value})$\tikzmark{restart1e} \;
        \nl $S \gets \FuncSty{search}(\{ E \in D : \left|E\right| > 1 \}, \top)$ \;
        \nl \tikzmark{restart9s}\lIf{$S \ne \KwSty{restart}$}{\tikzmark{restart9e}$\KwSty{return}~S$}
        \nl \tikzmark{restart2s}trigger nogoods containing only one clause\tikzmark{restart2e} \;
    }
}
\bigskip
    \nl $\FuncSty{search}$ (Domains $D$, Decisions $B$) $\rightarrow$ Bool
    \tikzmark{restart8s}\KwSty{or} \KwSty{restart}\tikzmark{restart8e} \;
\nl \Begin{
    \nl \lIf{$D = \emptyset$}{$\KwSty{return}~\KwSty{true}$}
    \nl $D_v \gets \textnormal{a domain from}~D~\textnormal{chosen by variable heuristic}$\siplabel{variableordering} \;
    \nl \tikzmark{restart3s}$C \gets \emptyset$\tikzmark{restart3e} \;
    \nl \ForEach {$v' \in D_v~\textnormal{ordered by value heuristic}$\siplabel{valueordering}}{
        \nl \tikzmark{restart4s}$C \gets C \cup \{ v' \}$\tikzmark{restart4e} \;
        \nl $D' \gets \FuncSty{clone}(D)$ \;
        \nl $D'_v \gets \{ v' \} $ \;
        \nl \If{\FuncSty{propagate}(D')}{
            \nl $S \gets \FuncSty{search}(\{ E \in D' : \left|E\right| > 1 \}, B \wedge (v \mapsto v'))$ \;
            \nl \lIf{$S = \KwSty{true}$}{$\KwSty{return}~\KwSty{true}$}
            \nl \tikzmark{restart5s}\ElseIf{$S = \KwSty{restart}$}{
                \nl post nogoods $\{B \wedge (v \mapsto c) \Rightarrow \bot : c \in C \}$\tikzmark{restart5r}\;
                \nl $\KwSty{return}~\KwSty{restart}$\tikzmark{restart5e} \;
            }
        }
    }
    \nl \tikzmark{restart6s}\lIf{$(R \gets R + 1) = R'$}{$\KwSty{return}~\KwSty{restart}\tikzmark{restart6e}$}
    \nl $\KwSty{return}~\KwSty{false}$
}
\bigskip
\nl $\FuncSty{propagate}$ (Domains $D$) $\rightarrow$ Bool \;
\nl \Begin{
    \nl \While{$D_v \gets$~\textnormal{a unit domain from}~$D$}{
        \nl $v' \gets $ the single value in $D_v$ \;
        \nl \tikzmark{restart7s}trigger nogoods with watch $(v \mapsto v')$\tikzmark{restart7e} \;
        \nl \ForEach{$D_w \in D - D_v$}{
            \nl $D_w \gets D_w - v'$ \siplabel{removev} \;
            \nl \ForEach{$(P,\,T) \in L$}{
                \nl \lIf{$v \sim_P w$}{$D_w \gets D_w \cap \neighbourhood_T(v')$
                }
            }
            \nl \lIf{$D_w = \emptyset$}{$\KwSty{return}~\KwSty{false}$}
        }
\nl $(H,\,A,\,n) \gets (\emptyset,\,\emptyset,\,0)$ \;
\nl \ForEach{$D_v \in D$ \textnormal{from smallest cardinality upwards\siplabel{eachdomain}}}{
    \nl $D_v \gets D_v \setminus H$ \siplabel{elimhall} \;
    \nl $(A,\,n) \gets (A \cup D_v,\,n + 1)$ \siplabel{acc} \;
    \nl \lIf{$D_v = \emptyset~\vee~|A| < n$}{$\KwSty{return}~\KwSty{false}$\siplabel{failhall}}
    \nl \lIf{$|A| = n$}{$(H,\,A,\,n) \gets (H \cup A,\,\emptyset,\,0)$\siplabel{hall}}
}
    }
\nl $\KwSty{return}~\KwSty{true}$ \;
}
\caption{Non-induced subgraph isomorphism. Shaded code is added for the restarting variant.}
\label{algorithm:sip}
\end{algorithm}

\subsection{Ordering Heuristics}

For the variable-ordering heuristic on \siplineref{variableordering} of \cref{algorithm:sip}, we use
smallest domain first, tiebreaking on highest degree. Variable ordering is not the focus of this
paper---as far as possible, we will leave it intact, so as not to affect the performance of the
algorithm on unsatisfiable instances.

For value ordering, which occurs on \siplineref{valueordering} of \cref{algorithm:sip}, we will
compare five strategies:

\paragraph{Random} We select a vertex dynamically at random (that is, we shuffle the vertices before
entering the $\KwSty{foreach}$ loop on \siplineref{valueordering} of \cref{algorithm:sip}). This is
\emph{usually} effectively equivalent to picking vertices in the order in which they appear in the
input files, although some instances from some benchmark suites are naturally in a special order
(which may or may not be a good order for searching).

\paragraph{Degree} We select vertices from highest degree to lowest degree. This is done by
permuting the target graph at the top of search. This heuristic is the current state of the art.

\paragraph{Anti} We select vertices from lowest degree to highest degree. This heuristic is used as
a sanity check: we expect degree to beat random, and random to beat anti.

\paragraph{Position-biased} We permute the graph at the top of search, as in the degree heuristic.
Then, as with the random heuristic, we shuffle the vertices before we enter the $\KwSty{foreach}$
loop. Instead of picking a vertex uniformly at random, we select select the first (that is, highest
degree) vertex with probability 0.5, the second vertex with probability 0.25, and the $n$th with
probability $2^{-n}$.  This heuristic heavily prefers vertices of high degree, similar to the
degree heuristic, but also introduces an element of randomness, and so can be used with restarts.

\paragraph{Degree-biased} Refining position-biased, we shuffle the vertices, using a weight of
$2^{\left|\neighbourhood(v)\right|}$ when deciding whether to branch on vertex $v$ next. (We cheat
slightly to avoid range issues with random number generation: we give the vertex with highest degree
in the entire graph weight $2^{50}$, and then give lower degree vertices smaller powers, giving
every vertex weight at least 1.  This is a bit of hack but programming is hard.) This heuristic
continues to prefer vertices of high degree, but will give the same chance of being selected next to
two vertices of equal degree.

\section{Empirical Evaluation}

?? Dual Xeon E5-2697A v4 CPUs, 512GBytes RAM, running Ubuntu 17.04, implemented in C++ and compiled
using GCC 6.3.0. Our code is heavily based upon \citet{dblp:conf/cp/McCreeshP15}.

?? Deterministic RNG.

?? Bitsets.

?? The 5,725 instances \citep{dblp:conf/lion/KotthoffMS16}.

?? Other studies use a random selection of 200 of each of the mesh and images instances
\citep{DBLP:journals/cviu/DamiandSHJS11}, partly because some earlier solvers find many of these
instances extremely hard, and partly to avoid skewing a portfolio. We want larger numbers of
satisfiable instances, so we include all pattern / target pairs. This gives a total of 14,621
instances, at least 2,110 of which are known to be satisfiable, and at least 12,323 are
unsatisfiable.

?? Note that an awful lot of these instances are very easy. We are trying to tackle the remaining
hard satisfiable instances.

\subsection{Our Solver is Good}

\begin{figure}[tb]
    \centering
    \includegraphics*{gen-graph-others.pdf}

    \bigskip

    \centering
    \includegraphics*{gen-graph-others-zoom.pdf}

    \caption{Cumulative number of instances solved over time, comparing our algorithm to other
    solvers. Below, zoomed in a bit ?? make this pretty.}
    \label{figure:others}
\end{figure}

?? In \cref{figure:others} we show that our solver is the bestest. For each algorithm, the $y$ value
gives the cumulative number of instances which (individually) can be solved in no more than $x$
milliseconds.  The vertical distance between two lines therefore shows how many more instances can
be solved by one solver than another, if every instance is run separately with the chosen $x$
timeout; the horizontal distance shows how many times longer the per-instance timeout would need to
be to allow the rightmost algorithm to succeed on $y$ out of the 14,621 instances (bearing in mind
that the two sets of $y$ instances solved by the two algorithms could be completely different).

?? These results show our algorithm clearly beating PathLAD and VF2, except for very low choices of
timeout. Our algorithm also beats Glasgow2 and Glasgow3, despite being slightly simpler---on an
instance by instance basis, sometimes the additional inference performed by these algorithms pays
off, but in aggregate, there is no difference for longer choices of timeout, and a clear gain for
shorter choices.

?? Add in SND if we can get it to run on the larger set of instances, maybe SAT just to have
something to cite showing how bad SAT is at this.

?? Possibly also do induced, compare to Glasgow, LAD, PathLAD, VF2, VF3, McSplit.

\subsection{Value Ordering Heuristics}

\begin{figure}[p]
    \centering
    \includegraphics*{gen-graph-value-ordering-heuristics.pdf}

    \caption{The cumulative number of instances solved over time, using the four different
    value-ordering heuristics.}
    \label{figure:value-ordering-heuristics}
\end{figure}

\begin{figure}[p]
    \centering
    \includegraphics*{gen-graph-scatter-heuristics.pdf}

    \caption{An instance by instance comparison of the degree and degree-biased heuristics.}
    \label{figure:scatter-heuristics}
\end{figure}

\begin{figure}[p]
    \centering
    \includegraphics*{gen-graph-scatter-dds.pdf}

    \caption{The effects of using depth-bounded discrepancy search.}\label{figure:scatter-dds}
\end{figure}

In \cref{figure:value-ordering-heuristics} we show the cumulative number of instances solved over
time using the four different value-ordering heuristics. As expected, the degree heuristic is much
better (?? give aggregate speedups for both all and just satisfiable instances) than random
ordering, which is in turn ?? times better than the anti-heuristic.

The performance of both of the biased heuristics and the degree heuristic are very similar: the
degree-biased heuristic has slightly worse performance below a 100s timeout, and effectively equal
performance above it. This shows that we can introduce an element of randomness into the degree
value-ordering heuristic without adversely affecting its performance \emph{in aggregate}.
\Cref{figure:scatter-heuristics} gives a more detailed look at these results. In this plot,
satisfiable and unsatisfiable instances are shown using coloured shapes and black dots respectively.
On the $x$-axis we measure the degree heuristic, whilst on the $y$-axis we measure the degree-biased
heuristic. Points on the outer axes represent instances which timed out with one heuristic but not
the other.  This plot shows that despite the aggregate performance being very similar, on an case by
case basis, the two heuristics can make a large difference to the performance of satisfiable
instances.

\subsection{Discrepancy Searches}

?? Quick intro to discrepancy search
\citep{DBLP:conf/ijcai/HarveyG95,DBLP:conf/aaai/Korf96,DBLP:conf/ijcai/Walsh97,DBLP:conf/cpaior/KarouiHLN07,DBLP:journals/jea/ProsserU11}.
Discrepancy search is about value ordering.

?? Talk about how we're not dealing with a binary search tree, so we could either make it binary, or
count one discrepancy each time the degree changes, or count every ``against'' branch as being just
one discrepancy regardless.

?? \cref{figure:scatter-dds}. Sometimes extremely beneficial on satisfiable instances. However, on both
unsatisfiable and satisfiable instances, the overheads are often large, and the algorithm is not
better in aggregate (even when only considering satisfiable instances).

\subsection{Restarts}

?? Quick intro to restarts, mentioning how it's important to change something when restarting.
Interestingly usually that thing is variable-ordering heuristics, not value-ordering
\citep{DBLP:conf/ijcai/LecoutreSTV07,DBLP:journals/jsat/LecoutreSTV07,DBLP:conf/cp/GayHLS15,DBLP:conf/aaai/LeeSZ16,DBLP:conf/cp/GlorianBLLM17}.
?? Does anyone at all do this on value ordering explicitly? And if not, why not? There's
\citet{DBLP:conf/flairs/RazgonOP07} at least. ?? There's a connection here to confidence-based work
stealing \citep{DBLP:conf/cp/ChuSS09} too. But what? They talk about confidence in a heuristic, but
not that different values could be ranked differently.

?? We use the Luby scheme \citep{DBLP:journals/ipl/LubySZ93}, because everyone else does. We count
the number of backtracks (that is, when we reach the end of the main $\KwSty{foreach}$ loop) to
decide when to restart. Following established wisdom, we multiply each item in the Luby sequence by
a magic constant. Preliminary experiments demonstrated that this is useful, but we failed to
determine a principled way of selecting the constant's value, so we fell back on divine revelation
and set it to 666. As with everyone else, we are not entirely clear why we are doing this: it could
be to allow the solver to spend more time deep in search, it could be to reduce the overheads from
repeatedly visiting inner nodes, or it could be to keep the nogoods small.

?? We just use two watched literals \citep{DBLP:conf/dac/MoskewiczMZZM01}. Could use increasing
nogoods \citep{DBLP:conf/aaai/LeeSZ16,DBLP:conf/cp/GlorianBLLM17} etc, but we are not confident enough
in our programming abilities to implement these correctly, and we are not maintaining arc
consistency anyway. We also really do not want to propagate on negative decisions, because we do not
want to iterate over every removed value.

?? Talk a bit about how it fits into \cref{algorithm:sip}.

\subsection{Restarts Make it Better}

In \cref{figure:restarts} we show the effects of adding restarts to the algorithm. With restarts,
the random value-ordering heuristic comfortably beats the degree strategy without restarts. In
other words, although having a good variable-ordering heuristic is beneficial, introducing
randomness into the search is better, if it is done alongside a mechanism to avoid heavy commitment
to any particular random choice. \Cref{figure:scatter-random} looks at this in more detail.  The
performance on unsatisfiable instances is the same. However, although better in aggregate, we see
many satisfiable instances performing substantially worse. ?? Quantify this a bit.

\begin{figure}[p]
    \centering
    \includegraphics*{gen-graph-restarts.pdf}

    \caption{Cumulative number of instances solved over time, with (dotted lines) and without (solid
    lines) restarts, and with depth-bounded discrepancy search (dashed line).}
    \label{figure:restarts}
\end{figure}

\begin{figure}[p]
    \centering
    \includegraphics*{gen-graph-scatter-random.pdf}
    \caption{Instance by instance comparison of the basic algorithm with the degree heuristic,
    versus random value-ordering with restarts.}
    \label{figure:scatter-random}
\end{figure}

\begin{figure}[p]
    \centering
    \includegraphics*{gen-graph-scatter-by-family.pdf}

    \caption{Instance by instance comparison of the basic algorithm with the degree heuristic,
    versus degree-biased value ordering with restarts.}
    \label{figure:scatter-by-family}
\end{figure}

\Cref{figure:restarts} shows that either biased heuristic together with restarts is better
still---that is, if we are introducing restarts, then it is better to add a small amount of
randomness to a tailored deterministic heuristic than it is to throw away the heuristic altogether.
The degree-biased heuristic is also stronger than position-biased, showing that exploiting ties and
relative sizes in the heuristic ordering is also worthwhile. Interestingly, the gap between degree-
and position-based is much larger when using restarts than it is when not using them.

In the more detailed view in \cref{figure:scatter-by-family}, comparing the basic algorithm with the
degree heuristic to the degree-biased algorithm with restarts, all of the unsatisfiable instances
are very close to the $x-y$ diagonal, showing that their performance is nearly unchanged. On the
other hand, there are ?? large numbers of satisfiable instances well below the diagonal line,
indicating large speedups.  Better yet, there are only ?? satisfiable instances that are more than a
factor of ?? times worse.

?? By family

?? This has made up the consistency we lost from introducing randomness.

?? Explain why this is so amazingly awesome and the most important idea ever to happen in the entire
history of science.

\section{Conclusion}

?? This stuff is quite good.

?? We can keep good value-ordering heuristics, and get large improvement. Importantly, we can do
this with almost no effect on unsatisfiable instances, and rare to make satisfiable much worse.

?? Variable ordering

?? Bigger CP picture.

?? Parallel

\bibliographystyle{named}
\bibliography{dblp}

\appendix
\clearpage
\section{Further Experiments}

?? engineering aspects: look at what happens without nogoods in \cref{figure:scatter-nogoods}, and
at performance with restarts on degree in \cref{figure:scatter-overheads}.

\begin{figure}[h]
    \centering
    \includegraphics*{gen-graph-scatter-nogoods.pdf}

    \caption{Is nogood recording important?}
    \label{figure:scatter-nogoods}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics*{gen-graph-scatter-overheads.pdf}

    \caption{Is nogood recording expensive?}
    \label{figure:scatter-overheads}
\end{figure}

?? effects of restart policies, \cref{figure:restart-policies}.

\begin{figure}[h]
    \centering
    \includegraphics*{gen-graph-restart-policies.pdf}

    \caption{Different restart policies.}
    \label{figure:restart-policies}
\end{figure}

\end{document}

